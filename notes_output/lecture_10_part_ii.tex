% Chapter 18
\section{Fuzzy Inference Systems: Rule Composition and Output Calculation}\label{chap:fuzzyinference}
\markboth{Fuzzy inference}{}

\begin{tcolorbox}[summarybox,title={Learning Outcomes}]
\begin{itemize}
    \item Execute full Mamdani/Larsen style inference: antecedent aggregation, implication, aggregation, and defuzzification.
    \item Compare implication/aggregation choices (product vs.\ min, max vs.\ sum) and their impact on the running thermostat/autofocus example.
    \item Contrast Mamdani systems with Sugeno/Takagi--Sugeno systems to know when weighted-average consequents are preferable.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox,title={Running example checkpoint}]
For the thermostat, each rule combines the temperature error (\textit{Cold}, \textit{Slightly Warm}, \ldots) and rate-of-change to set heater power. As you work through antecedent aggregation, implication, and defuzzification, keep one concrete rule base (e.g., ``IF error is Cold AND rate is Falling THEN heater power is High'') in mind; the formulas below map directly onto that setup.
\end{tcolorbox}

Throughout this chapter we keep the trilogy defaults from \Crefrange{chap:fuzzysets}{chap:fuzzyrelations}: \(\wedge=\min\), \(\vee=\max\), and the standard complement \(1-\mu\). Aggregation over rule consequents defaults to the max s\hyp{}norm unless stated otherwise; alternatives live in \Cref{tab:tnorms}.

Building on \Cref{chap:fuzzyrelations}, where we developed transfer operators (projection, composition, and the extension principle) to move fuzzy information between related universes, we now assemble complete fuzzy inference systems (FIS): rule composition and output calculation. The roadmap in \Cref{fig:roadmap} shows this as the inference step that turns fuzzy sets into decisions.

\begin{tcolorbox}[summarybox,title={Design motif}]
Local linguistic rules become a global behavior only after you commit to concrete operators (t\hyp{}norm, implication, aggregation, defuzzifier) and then sanity-check the resulting surface.
\end{tcolorbox}

\subsection{Context and Motivation}
\label{sec:fuzzyinference_context_and_motivation}

Recall that a fuzzy inference system maps crisp inputs to fuzzy outputs by applying a set of fuzzy rules. Each rule typically has the form:

\begin{quote}
\textit{If} $x_1$ is $A_1$ \textit{and} $x_2$ is $A_2$ \textit{and} $\cdots$ \textit{then} $y$ is $B$,
\end{quote}

where $A_i$ and $B$ are fuzzy sets defined on the respective universes of discourse. The antecedent (premise) combines multiple fuzzy conditions on inputs, and the consequent (conclusion) specifies the fuzzy output.

\begin{tcolorbox}[summarybox,title={Author's note: rules as lived experience}]
These rules are not immutable physical laws; they are codified experience. We record facts such as ``if it is morning then the sun is in the east,'' yet real observations may arrive at noon. Fuzzy inference exists to bridge that gap: observed memberships are composed with stored rules so that slight deviations in the antecedent produce softened consequents instead of brittle yes/no responses. When you carry out the algebra below, keep that picture of ``experience vs.\ observation'' in mind.
\end{tcolorbox}

The key challenge is to systematically combine the antecedent fuzzy sets and then infer the output fuzzy set for each rule, before aggregating all rules to produce a final output.

\subsection{Rule Antecedent Composition}
\label{sec:fuzzyinference_rule_antecedent_composition}

Given a rule with $n$ antecedents, each associated with a fuzzy set $A_i$ and an input value $x_i$, the degree to which the rule is activated (also called the \emph{firing strength}) is computed by combining the membership values of each antecedent condition.

\paragraph{Membership values of antecedents:} For each input $x_i$, the membership degree in fuzzy set $A_i$ is

\begin{equation}
\mu_{A_i}(x_i) \in [0,1]
\label{eq:auto_fuzzyinference_57eb8c12c9}
\end{equation}

\paragraph{Aggregation operator:} The combined antecedent membership is obtained by applying a fuzzy logical operator, typically the \emph{minimum} (intersection) or the \emph{product} operator:

\begin{align}
\mu_{\text{antecedent}}(x_1, \ldots, x_n) &= \min_{i=1}^n \mu_{A_i}(x_i), \quad \text{(min operator)} \label{eq:antecedent_min} \\
\text{or} \quad \mu_{\text{antecedent}}(x_1, \ldots, x_n) &= \prod_{i=1}^n \mu_{A_i}(x_i). \quad \text{(product operator)} \label{eq:antecedent_prod}
\end{align}

This value quantifies the degree to which the entire antecedent condition is satisfied by the input vector $\mathbf{x} = (x_1, \ldots, x_n)$. More generally, any t\hyp{}norm $T$ can be used in place of the min or product, provided it satisfies the standard properties (commutativity, associativity, monotonicity, and $T(a,1)=a$); the chosen t\hyp{}norm shapes how strictly the rule demands simultaneous satisfaction of all antecedents.

\subsection{Rule Consequent and Output Fuzzy Set}
\label{sec:fuzzyinference_rule_consequent_and_output_fuzzy_set}

Once the antecedent firing strength $\alpha$ is computed, it is used to modify the consequent fuzzy set $B$. The consequent fuzzy set is typically defined by its membership function $\mu_B(y)$ over the output universe.

\paragraph{Implication operator:} The implication step adjusts the consequent membership function based on the firing strength $\alpha$. Commonly used implication methods include:

\begin{itemize}
    \item \textbf{Minimum implication:} Truncate the consequent membership function at level $\alpha$,
    \begin{equation}
    \mu_{B'}(y) = \min \big( \alpha, \mu_B(y) \big).
    \label{eq:min_implication}
    \end{equation}
    \item \textbf{Product implication:} Scale the consequent membership function by $\alpha$,
    \begin{equation}
    \mu_{B'}(y) = \alpha \cdot \mu_B(y).
    \label{eq:prod_implication}
    \end{equation}
\end{itemize}

The resulting fuzzy set $B'$ represents the \emph{output fuzzy set} contributed by this particular rule.
\begin{tcolorbox}[summarybox,title={Implication choices}]
Mamdani uses min-implication (clipping), Larsen uses product-implication (scaling). Other options include residuated and axiomatic implicators paired with their t\hyp{}norms (e.g., G\"odel, Product/Goguen, \L{}ukasiewicz; see \citealp{Klement2000,Dubois1988}). Pick by desired smoothness: clipping preserves shape and interpretability; scaling yields smoother surfaces and is friendlier to gradient-based tuning.
\end{tcolorbox}

\subsection{Aggregation of Multiple Rules}
\label{sec:fuzzyinference_aggregation_of_multiple_rules}

When multiple rules are present, each produces an output fuzzy set $B'_j$ with membership function $\mu_{B'_j}(y)$, where $j$ indexes the rules. These are aggregated to form a combined output fuzzy set:

\begin{equation}
\mu_{B_{\text{agg}}}(y) = \max_j \mu_{B'_j}(y).
\label{eq:output_aggregation}
\end{equation}

The \emph{max} operator corresponds to the fuzzy union of the individual rule outputs, capturing the overall inference result.
\paragraph{Other aggregations} Algebraic sum or bounded sum (\Cref{tab:tnorms}) are used when max is too brittle; they can over-saturate when many rules fire, so start with max unless smooth blending is required.

\subsection{Summary of the Fuzzy Inference Process}
\label{sec:fuzzyinference_summary_of_the_fuzzy_inference_process}

To summarize, the fuzzy inference process for a given input vector $\mathbf{x}$ proceeds as follows:

\begin{enumerate}
    \item For each rule $j$, compute the antecedent membership degree $\alpha_j$ using \eqref{eq:antecedent_min} or \eqref{eq:antecedent_prod}.
\item Modify the consequent fuzzy set $B_j$ by applying the implication operator \eqref{eq:min_implication} or \eqref{eq:prod_implication} to obtain $B'_j$.
\item Aggregate all $B'_j$ using \eqref{eq:output_aggregation} to obtain the overall output fuzzy set $B_{\text{agg}}$.
\end{enumerate}

In sup-$T$ form this is the same compositional rule of inference used in \Cref{chap:fuzzyrelations}; here \(T\) defaults to \(\min\) (or product) and \(\sup\) reduces to a max on discrete grids.

The final step, defuzzification, converts $B_{\text{agg}}$ into a crisp output value. One widely used approach is the centroid (center-of-gravity) method, which computes

\begin{equation}
y^* = \frac{\int_Y y \, \mu_{B_{\text{agg}}}(y) \, dy}{\int_Y \mu_{B_{\text{agg}}}(y) \, dy}.
\label{eq:centroid_defuzz}
\end{equation}

This expression balances all candidate output values $y$ by weighting them according to their membership grade in the aggregated fuzzy set. In discrete implementations, the integral is replaced with a sum over sampled output points.
\paragraph{Other defuzzifiers} Common alternatives are mean/center of maxima (robust to multi-modal sets), smallest/largest of maxima (conservative tie-breaks), and center of sums (less sensitive to overlap than max aggregation). Choose centroid for smoothness, a max-based rule for fast or safety-critical switches, and always handle the zero-mass case explicitly.
\paragraph{Zero-mass fallback} If the denominator in \eqref{eq:centroid_defuzz} is zero (e.g., all consequents clipped to zero), fall back to a max-membership or rule-based tie-breaker to avoid NaNs; log the condition for debugging.
\paragraph{Computation note} With uniform sampling over \(m\) output points, centroid costs \(O(mR)\) per evaluation for \(R\) rules. Non-singleton inputs add a convolution step but reuse the same aggregation/defuzz pipeline; refine the grid near peaks to reduce bias.

\begin{tcolorbox}[pitfallbox,title={Centroid stability and tie-breaking}]
\begin{itemize}
    \item \textbf{Multi-modal sets.} When $B_{\text{agg}}$ has multiple peaks, the centroid may fall between modes. Log numerator and denominator separately and check that $\int \mu_{B_{\text{agg}}}(y)\,dy$ is non-zero; otherwise fall back to max-membership or a rule-based tie-break.
    \item \textbf{Discretisation.} Sampling the universe with too few points biases the centroid. Use uniform grids for smooth consequents and adaptive refinement near peaks for multi-modal sets. Report the step size (e.g., $0.5^\circ$C) to show numeric fidelity.
\end{itemize}
\end{tcolorbox}

\vspace*{1.2\baselineskip}
\noindent
\begin{tcolorbox}[summarybox,title={Pipeline at a glance (Mamdani/Larsen)}]
\begin{verbatim}
for each rule j:
    alpha_j = T( mu_A1(x1), ..., mu_An(xn) )
        # firing strength
    mu_Bj_prime(y) = implication(alpha_j, mu_Bj(y))
        # clip or scale
mu_Bagg(y) = S( mu_B1_prime(y), ..., mu_BR_prime(y) )
    # aggregate
y_star = centroid(mu_Bagg(y))
    # or another defuzzifier
\end{verbatim}
Defaults: \(T=\min\) or product; \(S=\max\); centroid defuzzification. Non-singleton fuzzification (convolving input uncertainty with \(\mu_{A_i}\)) uses the same pipeline once the input blend is computed.
\end{tcolorbox}

\begin{tcolorbox}[summarybox,title={Design checklist}]
\begin{itemize}
    \item Define universes/labels; ensure coverage and reasonable overlap.
    \item Pick \(T/S/\Rightarrow\) via \Cref{tab:tnorms} to get the smoothness/interpretability you need.
    \item Verify rule-base coverage; avoid contradictions/holes.
    \item Choose defuzzifier and sampling resolution; set a minimum-mass fallback.
    \item Test monotonicity/saturation; refine membership widths or rule weights.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[pitfallbox,title={Common pitfalls}]
\begin{itemize}
    \item Max aggregation can mask contributions from several moderate rules; algebraic sum can over-saturate.
    \item Memberships that are too narrow yield sparse firing; too wide produce mushy outputs.
    \item Coarse grids bias centroids; inconsistent units across labels break interpretability.
    \item Neglecting non-singleton inputs: if sensor noise matters, blur inputs before fuzzifying.
\end{itemize}
\end{tcolorbox}

\subsection{Mamdani vs.\ Sugeno/Takagi--Sugeno systems}
\label{sec:fuzzyinference_mamdani_vs_sugeno_takagi_sugeno_systems}

Mamdani-style inference (scaled fuzzy consequents, centroid defuzzification) excels when linguistic interpretability is a priority and when rule consequents must remain human-readable.\\
Sugeno/Takagi--Sugeno (TSK) systems replace fuzzy consequents with crisp functions such as affine models,
\[
\text{IF } e \text{ is } A_i \text{ AND } \dot{e} \text{ is } B_i \text{ THEN } u_i = p_i e + q_i \dot{e} + r_i.
\]
Each rule still produces a firing strength $\lambda_i$ via a t\hyp{}norm, but the final output becomes the weighted average
\[
u^\ast = \frac{\sum_i \lambda_i u_i}{\sum_i \lambda_i},
\]
eliminating the defuzzification integral. The trade-offs are:
{\raggedright
\begin{itemize}
    \item \textbf{Mamdani:} transparent consequents, straightforward incorporation of expert knowledge, but higher computational cost due to aggregation and centroid evaluation.
    \item \textbf{Sugeno/TSK:} faster evaluation (weighted averages), amenable to gradient-based tuning of the consequent parameters, yet less interpretable because consequents are numerical functions rather than linguistic labels.
\end{itemize}
}
For the thermostat example, Mamdani rules (``IF error is Cold AND rate is Falling THEN heater power is High'') are ideal when operators must audit decisions, whereas a Sugeno/TSK variant is preferable when embedding the controller into a high-speed or automatically tuned loop.
\footnote{Classic sources: \citet{Mamdani1975} for clipping implication; \citet{TakagiSugeno1985} for TSK; \citet{Jang1993} for ANFIS; see also \citep{Klement2000,Dubois1988} for operator/implicator theory.}

\begin{tcolorbox}[summarybox,title={Key takeaways}]
\begin{itemize}
    \item Fuzzy inference composes rule antecedents (via a t\hyp{}norm) and modifies consequents by implication.
    \item Aggregation and defuzzification (e.g., centroid) produce crisp outputs from fuzzy rule bases.
    \item Design choices (operators, shapes) trade interpretability and control smoothness.
\end{itemize}

\medskip
\noindent\textbf{Minimum viable mastery.}
\begin{itemize}
    \item For a rule base, compute firing strengths, apply implication, aggregate consequents, and compute a centroid output.
    \item Explain the Mamdani vs.\ Sugeno/TSK tradeoff (interpretability vs.\ efficiency/tunability).
    \item Treat operator choice as part of the specification and keep it consistent across examples and implementations.
\end{itemize}

\noindent\textbf{Common pitfalls.}
\begin{itemize}
    \item Defining memberships on incompatible scales (inputs never trigger, or everything triggers).
    \item Writing many redundant rules and then tuning symptoms rather than consolidating the rule base.
    \item Comparing controllers without stating operators, defuzzification, and evaluation protocol.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox,title={Exercises and lab ideas}]
\begin{itemize}
    \item Implement a Mamdani thermostat with three error labels and two rate labels; experiment with min/product t\hyp{}norms and report the resulting control surfaces.
    \item Build a Sugeno/TSK variant of the same controller and compare outputs to the Mamdani version under identical test trajectories.
    \item Evaluate different defuzzification methods (centroid, weighted average, max membership) on a toy rule base and quantify the steady-state error they induce.
\end{itemize}

\medskip
\noindent\textbf{If you are skipping ahead.} When you reach \Cref{chap:evo}, keep this chapter's audit instinct: log your design choices. Evolutionary and fuzzy systems both invite silent degrees of freedom that only become visible with disciplined reporting.
\end{tcolorbox}

\paragraph{Where we head next.} \Cref{chap:evo} moves from fuzzy controllers to evolutionary computing, where population-based search and optimization heuristics provide another pillar of soft computing.

\paragraph{References.} Full citations for works mentioned in this chapter appear in the book-wide bibliography.

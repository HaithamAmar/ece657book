% Chapter 6
\section{Multi-Layer Perceptrons: Challenges and Foundations}\label{chap:mlp}

\Cref{chap:perceptron} introduced the perceptron and Adaline: single units that learn by updating weights from data, with Adaline giving our first explicit glimpse of gradient descent on a smooth performance function.

We now take the smallest step beyond a single unit. We connect two neurons in series, treat the whole diagram as one trainable model, and choose a simple performance function that tells us when the output is moving in the right direction. This is the step where the simple unit becomes itself a building block. The central question is practical: \emph{if performance improves when the output moves one way, how should each weight move?}

As soon as we try to compute those derivatives, a key obstacle appears: hard thresholds do not carry useful gradient information. Replacing them with smooth activations fixes the learning signal and gives us a clean update story. We keep the network small enough that every intermediate quantity is visible and derive the update rules once in full; scaling the same logic to deeper models is then a matter of organization.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
By the end of this chapter, you should be able to:
\begin{itemize}
    \item Write a complete forward pass for a two\hyp{}neuron chain, including intermediate quantities \((p_1,y_1,p_2,y_2)\).
    \item Choose a simple scalar performance function \(P\) and explain what ``improving performance'' means in that setting.
    \item Use the chain rule to derive the gradients for \(w_2,b_2,\mathbf{w}_1,b_1\) and interpret the \(\delta\) notation as reusable local error signals.
    \item Explain (concretely) why hard thresholds break derivative-based updates and why smooth activations fix the learning signal.
    \item Sanity-check an update with one tiny numerical example before you scale the same logic to many layers.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Start small. Keep every intermediate quantity visible. Let the chain rule explain how the learning signal moves, and use quick numerical checks to catch sign and accounting mistakes early.
\end{tcolorbox}

\subsection{From a single unit to the smallest network}
\label{sec:mlp-limitations}

\paragraph{A short map: building a trainable network.}
The chapter follows one tight loop:
\begin{itemize}
    \item \textbf{Build:} write the forward computation (a two\hyp{}neuron chain).
    \item \textbf{Judge:} define a scalar performance function \(P\) (we use squared error).
    \item \textbf{Move:} update parameters using the direction given by derivatives (gradient descent).
    \item \textbf{Fix:} Take out the threshold and make the unit differentiable so the learning signal can pass through it.
\end{itemize}
Once you can execute this loop for two neurons, scaling to many neurons is mostly accounting; we return to the organization when we scale up.
\paragraph{How this chapter fits the workflow.}
The objective-and-audit loop still applies; what changes is the \emph{representation}. Once a model has internal computations, you have to keep track of intermediate variables and make sure the learning signal can actually pass through them. Before you scale up, do a few small sanity checks: confirm the forward pass, confirm the gradient signs, and check one tiny numerical update.
\begin{itemize}
    \item From \Cref{chap:supervised}: diagnostics (learning curves, bias--variance) tell you \emph{what} is going wrong.
    \item From \Cref{chap:logistic}: a linear probabilistic baseline tells you \emph{how far} you can go without nonlinear features.
    \item Here: we build the smallest multi-layer network and derive its update rules, so the mechanics feel concrete.
\end{itemize}

\paragraph{Function estimation as the unifying view.}
Learning is about building a usable input\(\rightarrow\)output mapping from examples. The difference from linear models is not the goal, but the \emph{representation}: a neural network builds the mapping by composing simple units, so intermediate signals become part of the model. Each unit implements its own local mapping (from its input to its pre-activation and activation), and these local maps accumulate into a successful nonlinear mapping \(f: X \to Y\). In this chapter we keep the accounting minimal---one tiny network and a simple squared-error objective---so we can focus on how the update rules emerge.

\paragraph{From one unit to a chain of units.}
A single unit (the perceptron) computes a weighted sum and then applies an activation:
\begin{equation}
 y = f(p), \qquad p = \mathbf{w}^\top\mathbf{x} + b,
 \label{eq:perceptron_forward}
\end{equation}
where $\mathbf{x}\in\mathbb{R}^n$, $\mathbf{w}\in\mathbb{R}^n$, and $b$ is a bias. With one unit, the decision boundary $\mathbf{w}^\top\mathbf{x}+b=0$ is a hyperplane, so a single unit can only produce linear separations; XOR is the canonical reminder of that limitation.

The smallest step beyond a single unit is a \emph{two\hyp{}neuron chain}: one unit feeds another. Write
\begin{align}
 p_1 &= \mathbf{w}_1^\top \mathbf{x} + b_1, & y_1 &= f(p_1), \\
 p_2 &= w_2 y_1 + b_2, & y_2 &= f(p_2). \label{eq:two_neuron_forward}
\end{align}
This is the point where the unit becomes a building block: an intermediate signal $y_1$ is computed, reused, and then transformed again. Complexity now comes not only from what each block does in isolation, but from how the blocks connect, and how a change in one propagates to the other. We will reuse \Cref{fig:mlp_minimal_chain} as our tracking diagram when we apply the chain rule.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
        node distance=2.4cm,
        >=Stealth,
        box/.style={draw, rounded corners, minimum width=1.6cm, minimum height=0.9cm, align=center, fill=cbBlue!6},
        neuron/.style={draw, circle, minimum size=1.0cm, align=center, fill=cbGreen!8},
        outbox/.style={draw, rounded corners, minimum width=1.2cm, minimum height=0.9cm, align=center, fill=cbOrange!8},
        lab/.style={font=\footnotesize, inner sep=1pt, align=center},
    ]
        \node[box] (x) {$\mathbf{x}$};
        \node[neuron, right=2.6cm of x] (n1) {$f$};
        \node[neuron, right=2.4cm of n1] (n2) {$f$};
        \node[outbox, right=3.0cm of n2] (y) {$y_2$};

        \draw[->] (x) -- node[lab, above, pos=0.55, yshift=6pt] {$p_1=\mathbf{w}_1^\top\mathbf{x}+b_1$} (n1);
        \draw[->] (n1) -- node[lab, below, pos=0.5, yshift=-2pt] {$y_1=f(p_1)$} (n2);
        \draw[->] (n2) -- node[lab, above, pos=0.5, xshift=-6pt, yshift=8pt] {$p_2=w_2 y_1+b_2$} (y);
        \node[lab, below=4pt of y] {$y_2=f(p_2)$};
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{The minimal neural network used in this chapter is a two-neuron chain. The first unit produces an intermediate signal, and the second unit maps that signal to the final output.}
    \label{fig:mlp_minimal_chain}
\end{figure}

\paragraph{Author's note: why a network changes the story.}
With one perceptron, every example pushes on the same single separator. But if you add a second unit, you are no longer forcing the entire problem through one shared knife-edge: you create intermediate signals that can specialize. Some units become sensitive to one pattern, others to another, and a later unit learns how to combine them.

I like to read those intermediate signals as learned transformations whose job is simple: make the final separation easier than it looked in the raw input space.

\paragraph{A checklist of what we must settle (and why).}
A diagram becomes a learning machine only after we make four choices explicit:
\begin{itemize}
    \item \textbf{Parameters:} which numbers we are allowed to change (weights and biases).
    \item \textbf{Performance:} a single scalar \(P\) that measures how wrong the current output is.
    \item \textbf{Update:} a rule that changes parameters to reduce \(P\).
    \item \textbf{Differentiability:} if the update is derivative-based, the path from parameters to \(P\) must be differentiable so the chain rule can assign credit (and blame).
\end{itemize}
We keep the network tiny so you can see all four ingredients at the same time.

\paragraph{Bias as a learned threshold.}
A threshold can be written as a bias. If \(p=\mathbf{w}^\top\mathbf{x}-\theta\), define \(b=-\theta\) and write \(p=\mathbf{w}^\top\mathbf{x}+b\). In practice we append \(x_0=1\) and treat \(b\) as an extra weight \(w_0\); the algebra is identical. Intuitively, the bias controls \emph{where} the unit switches, while the weights control \emph{which directions} in input space push the unit toward switching. This simple trick lets us treat the bias exactly like another weight in both notation and updates.

\subsection{Performance: what are we trying to improve?}
\label{sec:mlp_performance_what_are_we_trying_to_improve}

Once we have a forward computation, we need a performance function that tells us whether the output is good. For one training example with target $t$, a simple choice is the squared error
\begin{equation}
 P = \frac{1}{2}(y_2 - t)^2.
 \label{eq:performance}
\end{equation}
The factor $\tfrac{1}{2}$ makes derivatives cleaner. If you prefer to \emph{maximize} a score rather than minimize an error, you could take $-\tfrac{1}{2}(y_2-t)^2$ instead. The math below is identical up to a sign. We will minimize $P$.

\paragraph{Why a square?}
The signed error \(e=y_2-t\) can be positive or negative. Squaring removes the sign, penalizes large mistakes more than small ones, and keeps \(P\) smooth. That smoothness matters: if \(P\) changes continuously with the weights, derivatives can give a reliable direction for improvement. It is also easy to interpret and explain.

\begin{tcolorbox}[summarybox, title={Author's note: one objective is enough for the first derivation}]
I start with squared error for one reason: it keeps the algebra short, so the chain rule is the only moving part you have to watch. The lesson here is not ``squared error is always right.'' The lesson is learning how to take a forward computation and turn it into correct weight updates.

Once that story is clear, you can swap the objective and redo only the small set of derivatives that touch the output layer.
\end{tcolorbox}

\paragraph{A geometric intuition.}
For a fixed input, the performance becomes a surface over the weights. If the surface is bowl-shaped, the best parameters sit near the bottom. Learning is then ``navigation'': move the weights in a direction that reduces performance. This is why it is preferable to use an algorithm that follows the local slope rather than guessing directions coordinate by coordinate. \Cref{fig:mlp_gd_surface} visualizes why moving in one vector direction is typically more efficient than changing one coordinate at a time.

\subsection{Gradient descent: how do weights move?}
\label{sec:mlp_gradient_descent_how_do_weights_move}

We now ask: how should $\mathbf{w}_1, w_2, b_1, b_2$ change to reduce $P$? The standard answer is gradient descent:
\begin{equation}
\theta \leftarrow \theta - \eta \nabla_{\theta} P,
\label{eq:gd_update}
\end{equation}
where $\theta$ stands for any parameter and $\eta>0$ is the step size. Geometrically, you can picture the performance surface as a landscape: the gradient points uphill, so we step in the opposite direction to descend toward a minimum. The step size controls how far we move; too large can overshoot, too small can crawl.

For a weight vector, the update is a vector step:
\begin{equation}
\Delta \mathbf{w} = -\eta \nabla_{\mathbf{w}} P.
\label{eq:vectorized_update}
\end{equation}
This is the ``move the weights in the right direction'' story made precise: we do not guess the direction; we compute it from the derivative of performance. Importantly, we update \emph{all} weights at once (a vector step), not one coordinate at a time.

\paragraph{Step size is a design choice.}
The gradient gives a direction; the step size \(\eta\) sets how far you move. Too large and you bounce around (or diverge); too small and learning crawls. Many practical tricks are really ways of managing this tradeoff while keeping updates stable. In more uneven landscapes with multiple valleys, step size also affects \emph{which} valley you settle into: aggressive steps can jump across good regions, while timid steps can get you stuck early.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[background rectangle/.style={fill=white}, show background rectangle]
        \begin{axis}[
            width=0.72\linewidth,
            height=0.46\linewidth,
            xmin=-2.5, xmax=2.5,
            ymin=-2.0, ymax=2.2,
            axis lines=middle,
            xlabel={$w_1$},
            ylabel={$w_2$},
            grid=both,
            minor grid style={gray!10},
            major grid style={gray!20},
            tick label style={font=\scriptsize},
            label style={font=\scriptsize},
            axis background/.style={fill=white},
            clip=false,
        ]
            % Elliptical contours to suggest a convex bowl.
            \addplot[domain=0:360, samples=200, gray!55] ({1.6*cos(x)},{0.7*sin(x)});
            \addplot[domain=0:360, samples=200, gray!55] ({2.1*cos(x)},{0.95*sin(x)});
            \addplot[domain=0:360, samples=200, gray!55] ({2.6*cos(x)},{1.2*sin(x)});

            % Coordinate-wise updates (zig-zag).
            \addplot[cbOrange, thick, -{Stealth[length=2mm]}] coordinates {(-2.0,1.3) (-0.6,1.3)};
            \addplot[cbOrange, thick, -{Stealth[length=2mm]}] coordinates {(-0.6,1.3) (-0.6,0.2)};
            \addplot[cbOrange, thick, -{Stealth[length=2mm]}] coordinates {(-0.6,0.2) (0.0,0.2)};

            % Gradient step (single vector move).
            \addplot[cbBlue, very thick, -{Stealth[length=2.2mm]}] coordinates {(-2.0,1.3) (-0.8,0.35)};

            \node[font=\scriptsize, cbBlue!70!black] at (axis cs:-1.25,0.55) {$-\nabla P$};
            \node[font=\scriptsize, cbOrange!80!black, align=center] at (axis cs:-0.1,1.45) {one weight\\at a time};
        \end{axis}
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Think of performance as a surface over the weights. Gradient descent moves in one vector step (blue), whereas coordinate-wise updates can zig-zag (orange).}
    \label{fig:mlp_gd_surface}
\end{figure}

\subsection{Why hard thresholds block learning}
\label{sec:mlp_why_hard_thresholds_block_learning}

At this point the story is simple: define $P$, compute $\nabla P$, and update weights. The catch is that computing $\nabla P$ requires derivatives through the activation. When we apply the chain rule to the forward-pass equations in \eqref{eq:two_neuron_forward}, factors like $f'(p_1)$ and $f'(p_2)$ appear immediately. \Cref{fig:mlp_step_vs_sigmoid} makes this derivative contrast concrete.

If $f$ is a hard threshold (a step function), it is discontinuous and non\hyp{}differentiable at the threshold. That breaks the gradient story: $f'(p)$ either does not exist or is zero almost everywhere, so derivatives cannot guide learning. This is the core reason we replace thresholds with \emph{smooth, differentiable activations}.

\paragraph{Absorbing the threshold.}
Folding the threshold into a bias simplifies notation, but it does not remove the real issue: a hard step is still discontinuous. When you try to compute how a weight changes performance, the chain rule immediately introduces a derivative of the activation---terms of the form \(f'(p)\) appear. With a step function, that derivative is either undefined (at the threshold) or zero almost everywhere, so the learning signal cannot pass through the unit in a useful way. This is why we replace the step with a smooth activation.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[background rectangle/.style={fill=white}, show background rectangle]
        \begin{groupplot}[
            group style={group size=2 by 1, horizontal sep=1.1cm},
            width=0.44\linewidth,
            height=0.32\linewidth,
            axis lines=middle,
            xmin=-4, xmax=4,
            samples=200,
            tick label style={font=\scriptsize},
            label style={font=\scriptsize},
            title style={font=\scriptsize},
            axis background/.style={fill=white},
        ]
        \nextgroupplot[
            title={Hard threshold (step)},
            ymin=-0.15, ymax=1.15,
            ytick={0,1},
            xlabel={$p$},
        ]
            \addplot[cbBlue, thick, domain=-4:0] {0};
            \addplot[cbBlue, thick, domain=0:4] {1};
            % Derivative is zero almost everywhere (and undefined at 0).
            \addplot[cbBlue, dashed, domain=-4:4] {0};
        \nextgroupplot[
            title={Smooth activation (sigmoid)},
            ymin=-0.15, ymax=1.15,
            ytick={0,0.5,1},
            xlabel={$p$},
        ]
            \addplot[cbOrange, thick, domain=-4:4] {1/(1+exp(-x))};
            \addplot[cbOrange, dashed, domain=-4:4] {1/(1+exp(-x))*(1-1/(1+exp(-x)))};
        \end{groupplot}
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Hard thresholds block gradient-based learning because the derivative is zero almost everywhere. A smooth activation like the sigmoid provides informative derivatives across a wide range of inputs.}
    \label{fig:mlp_step_vs_sigmoid}
\end{figure}


\subsection{Differentiable activations and the sigmoid trick}
\label{sec:mlp_differentiable_activations_and_the_sigmoid_trick}

A classic choice is the logistic (sigmoid) function
\begin{equation}
\sigma(p) = \frac{1}{1+e^{-p}}.
\label{eq:sigmoid}
\end{equation}
It maps real inputs to $(0,1)$ and is differentiable everywhere. The key identity is
\begin{equation}
\sigma'(p) = \sigma(p)\,[1-\sigma(p)].
\label{eq:sigmoid_derivative}
\end{equation}
This is a useful trick: the derivative is a function of the \emph{output} itself. If $y=\sigma(p)$ is already computed in the forward pass, then $\sigma'(p)=y(1-y)$ is immediately available in the backward pass. No extra exponentials are needed.

\begin{tcolorbox}[summarybox, title={Author's note: the derivative is already in the forward pass}]
In practice you do not want to recompute expensive expressions during learning. With a sigmoid, once you have the forward output \(y=\sigma(p)\), you also have its slope for free: \(\sigma'(p)=y(1-y)\).

That tiny identity hints at the bigger pattern: the forward pass is not only for predictions. It is also where you produce (and store) the intermediate values the backward pass will reuse. This idea, as intuitive as it might feel, is one of the reasons multi-layer perceptrons became trainable at scale and why modern deep learning can exist as an engineering discipline.
\end{tcolorbox}

\paragraph{Derivation sketch.}
Let \(\beta=\sigma(\alpha)=(1+e^{-\alpha})^{-1}\). Differentiate and rewrite the result in terms of \(\beta\):
\[
\frac{d\beta}{d\alpha} = \frac{e^{-\alpha}}{(1+e^{-\alpha})^2}
= \left(\frac{1}{1+e^{-\alpha}}\right)\left(1-\frac{1}{1+e^{-\alpha}}\right)
 = \beta(1-\beta).
\]
The key point is that once you have computed \(\beta\) on the forward pass, you can compute the slope from \(\beta\) alone, without recomputing exponentials.

\subsection{Deriving weight updates for the two\hyp{}neuron network}
\label{sec:mlp_deriving_weight_updates_for_the_two_neuron_network}

The diagram in \Cref{fig:mlp_minimal_chain} is also a derivative map: to update a weight, follow how a small change in that weight would flow forward to the output and then back to the performance. The chain rule turns that story into algebra.

We now compute the gradients from the forward-pass equations in \eqref{eq:two_neuron_forward} using the chain rule. First note the easy derivatives:
\begin{itemize}
    \item $\displaystyle \frac{\partial P}{\partial y_2} = y_2 - t$.
    \item $\displaystyle \frac{\partial y_i}{\partial p_i} = f'(p_i)$.
    \item $\displaystyle \frac{\partial p_2}{\partial w_2} = y_1$ and $\displaystyle \frac{\partial p_2}{\partial y_1} = w_2$.
    \item $\displaystyle \frac{\partial p_1}{\partial \mathbf{w}_1} = \mathbf{x}$.
\end{itemize}

For the second-layer parameters, the chain rule gives
\begin{align}
\frac{\partial P}{\partial w_2}
&= \frac{\partial P}{\partial y_2}\frac{\partial y_2}{\partial p_2}\frac{\partial p_2}{\partial w_2}
= (y_2 - t)\, f'(p_2)\, y_1,
\label{eq:grad_w2}
\end{align}
and similarly
\begin{equation}
\frac{\partial P}{\partial b_2} = (y_2 - t) f'(p_2).
\label{eq:grad_b2}
\end{equation}

For the first-layer parameters, the effect of the second layer appears explicitly through the chain rule:
\begin{align}
\frac{\partial P}{\partial \mathbf{w}_1}
&= \frac{\partial P}{\partial y_2}\frac{\partial y_2}{\partial p_2}\frac{\partial p_2}{\partial y_1}\frac{\partial y_1}{\partial p_1}\frac{\partial p_1}{\partial \mathbf{w}_1} \\
&= (y_2 - t)\, f'(p_2)\, w_2\, f'(p_1)\, \mathbf{x},
\label{eq:grad_w1}
\end{align}
with bias derivative
\begin{equation}
\frac{\partial P}{\partial b_1} = (y_2 - t) f'(p_2) w_2 f'(p_1).
\label{eq:grad_b1}
\end{equation}

\smallskip
\noindent To make the reuse explicit, define local error terms:
\begin{equation}
\delta_2:= \frac{\partial P}{\partial p_2} = (y_2 - t) f'(p_2).
\label{eq:delta2}
\end{equation}
Then $\partial P/\partial w_2 = \delta_2 y_1$ and $\partial P/\partial b_2=\delta_2$. The first layer receives an error signal propagated backward:
\begin{equation}
\delta_1:= \frac{\partial P}{\partial p_1} = \delta_2 w_2 f'(p_1),
\label{eq:delta1}
\end{equation}
so $\partial P/\partial \mathbf{w}_1 = \delta_1 \mathbf{x}$ and $\partial P/\partial b_1 = \delta_1$.

This is the central lesson: once we compute a local error term, it can be reused across many gradients. That reuse is what makes multi-layer training efficient and is why deeper networks remain tractable.

\begin{tcolorbox}[summarybox, title={Worked example: one numerical gradient step (sanity check)}]
Take a single input $\mathbf{x} = [1,-1]^\top$, target $t=1$, sigmoid activation $f=\sigma$, and parameters
$\mathbf{w}_1=[0.8,\,0.2]^\top$, $b_1=0$, $w_2=1$, $b_2=0$.
\medskip

\noindent\textbf{Forward:} $p_1=0.6$, $y_1=\sigma(p_1)\approx 0.646$; $p_2=y_1$, $y_2=\sigma(p_2)\approx 0.656$; $P=\tfrac12(y_2-t)^2\approx 0.059$.
\medskip

\noindent\textbf{Backward:} $\sigma'(p)=y(1-y)$, so $\delta_2=(y_2-t)\sigma'(p_2)\approx -0.078$ and $\delta_1=\delta_2 w_2 \sigma'(p_1)\approx -0.018$.
Thus $\nabla_{\mathbf{w}_1}P=\delta_1 \mathbf{x} \approx [-0.018,\,+0.018]^\top$ and $\nabla_{w_2}P=\delta_2 y_1\approx -0.050$.
\medskip

\noindent\textbf{Update:} with $\eta=0.5$, gradient descent increases $w_2$ slightly (since the gradient is negative) and nudges $\mathbf{w}_1$ in a direction that increases $y_2$ toward the target.
\end{tcolorbox}

\subsection{From two neurons to multi-layer networks}
\label{sec:mlp_from_two_neurons_to_multi_layer_networks}

Nothing essential changes for deeper networks; we simply apply the same chain rule repeatedly. For a layer $l$ with pre\hyp{}activations $\mathbf{p}^{(l)}$ and weights $\mathbf{W}^{(l+1)}$, the error signal satisfies
\begin{equation}
\boldsymbol{\delta}^{(l)} = \bigl(\boldsymbol{\delta}^{(l+1)} (\mathbf{W}^{(l+1)})^\top\bigr) \circ f'(\mathbf{p}^{(l)}),
\label{eq:delta_recursion}
\end{equation}
where $\circ$ denotes element\hyp{}wise multiplication. This recursion is the basic backward update pattern in multi-layer networks; the only real challenge is organizing the computation so you can evaluate it efficiently.

\begin{tcolorbox}[summarybox, title={Author's note: what changes when you scale up}]
Going from two neurons to many layers does not change the idea. It is still the chain rule and the same local derivatives. What changes is the \emph{organization}.

You run a forward pass that stores what you will need, then a backward pass that reuses those stored values to compute every gradient efficiently over a whole batch. Later we turn that into an implementable algorithm and show the accounting that keeps it reliable.
\end{tcolorbox}

\subsection{Summary}
\label{sec:mlp_summary}
\begin{itemize}
    \item A two\hyp{}neuron chain is the smallest network that goes beyond a single perceptron.
    \item Learning starts by defining a performance function and asking how weights change it.
    \item Gradient descent uses derivatives to choose the correct update direction.
    \item Hard thresholds obstruct gradients; smooth activations fix the problem.
    \item The sigmoid derivative $\sigma'(p)=\sigma(p)(1-\sigma(p))$ is a convenient identity because it reuses the output.
    \item The two\hyp{}neuron derivation already contains the backward-recursion pattern used in deep networks.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Derivation closure: implement, cache, fail-fast}]
\begin{itemize}
    \item \textbf{Implement:} treat the chapter equations as a forward function plus a scalar loss; write them once in vector form before batching.
    \item \textbf{Cache:} keep \((\mathbf{x}, p_1, y_1, p_2, y_2)\) from the forward pass so each gradient term is a local reuse, not a re-derivation.
    \item \textbf{Fail-fast checks:} run finite-difference gradient checks on a tiny example, then track train/validation divergence to catch update-sign or step-size mistakes early.
    \item \textbf{Handoff:} the same cache-then-backward discipline scales directly; only the index/shape accounting grows.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Key takeaways}]
\textbf{If you remember only a few things.} The summary in \Cref{sec:mlp_summary} is the core loop: a tiny computation graph, a scalar loss, and gradients that tell you how to move weights. The pitfalls below are the places people usually lose a sign, a factor, or a cached value.
\medskip
\textbf{Common pitfalls}
\begin{itemize}
    \item Treating a step function like a differentiable activation and then trying to ``repair'' gradients by hand.
    \item Dropping a cached intermediate (or reusing the wrong one) and silently breaking the chain rule.
    \item Mixing up pre\hyp{}activations and activations; when you batch, also keep shapes explicit so you do not rely on accidental broadcasting.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\noindent\textbf{Try these while the network is still small.} These reinforce the two\hyp{}neuron derivation and prepare you for multi-layer accounting.
\begin{itemize}
    \item \textbf{Numerical gradient check:} Implement finite differences for the two\hyp{}neuron chain and compare to your analytic gradients; report relative error and track down the first mismatch.
    \item \textbf{Step vs.\ sigmoid:} Replace the smooth activation with a hard threshold and observe what breaks when you try to compute updates via derivatives.
    \item \textbf{XOR with two hidden units:} Train a tiny MLP on XOR and plot its decision regions; note sensitivity to initialization and step size.
\end{itemize}
\medskip
\noindent\textbf{If you are reading for implementation.} Be able to read a forward graph and a backward recursion: local derivatives, cached activations, and a scalar loss driving parameter updates. That is exactly what gets scaled up in deeper networks.
\end{tcolorbox}

\medskip
\paragraph{Where we head next.}
\Cref{chap:backprop} answers the practical scaling question: how to go from the smallest network in this chapter to networks with many layers and many units per layer. It uses the same ideas, but organizes them into a general recipe---a forward pass that caches intermediate quantities, and a backward pass that reuses those caches to compute gradients efficiently for deep, batched models.

# Chapter 14 (Transformers) -- Author Dictation / Raw Material (Reference)

This file preserves the author's raw dictation used to shape Chapter 14.
It is intentionally **not** fully copy-edited: the goal is to keep voice and intent
so we can refer back to it during editorial passes.

Canonical chapter source (edited/book-voice): `notes_output/lecture_transformers.tex`.

---

## Batch 1: Intro -> Seq2Seq -> Attention -> Self-Attention -> Multi-Head (Dictation)

We now move to attention mechanisms that replaced RNN, NLP, and processing potentially some CNNs applications. The genesis was looking at attention in RNN models with the intent of solving the context challenge where instead of worrying about the past few words as seen in LSTM and GRUs, we apply attention models to find the important words or tokens. But quickly, it turns out that attention is all we need. Sequence to sequence. Let's take translation as an example and try to solve it. A simple framework for translation would be an encoder to decoder setup. Maintain the knowledge from chapter 12 and 13 for the conversation moving forward. The encoder would understand and observe the latent features of the sequence or the sentence that's coming in as an input. The decoder would be able to decode this representation in a different language as desired. So the latent representation of a word of English can be decoded to be in French for the sequence. This, the simplest way we build the sequence to sequence network is RNN. So our RNN, RNN as an encoder and RNN as decoder. RNNs discussed in chapter 12 can map vectors to the different words into, of the different words into output vectors. So the encoder is essentially a sequence to vector. The output vector of a decoder, we can consider it as a context, becomes an input to the output RNN of what we call a decoder, where it gets decoded into another sequence of vectors for the new language, giving us the sequence to sequence implementation. We can call this setup a language model. The sequence to sequence model can be viewed as a probability, the probability of yi, that's the word that we're trying to predict, giving all previous words. This probability is produced by the decoder giving the previous word yi minus 1, the current information of the current layer or the hidden state, as well as the context that we get from the encoder, where the probability is a function of previously predicted words, the current states of the layer, and the context from the encoder. However, regardless of the RNN variant that you choose, the long-range dependency remains to be a problem. And that takes us to attention. The issue with dependency is that while we solve for memory, not long memory, but we do solve for it, we don't solve for relevance of importance. When you read a book, certain words or certain paragraphs draw your attention more than other words. So the objective is therefore, instead of producing one context for the input sequence through the process of encoding, we produce multiple contexts. The decoder therefore does not have to focus on the entire compressed source vector. The decoder can decide which source parts are more important than other source parts. The pipeline is fairly simple. The encoder will have all of its states or hidden states and one decoder state at a time. We calculate the attention score for each encoder state given the decoder state. So one decoder state, all encoder states, and then we pair them up. The output of this pairwise calculation is a scalar. Once we have the attention scores, or these scalars, we call them attention scores, we can calculate an attention weight for each input state. So now we're calculating attention states for input states through a softmax function. Think of this as a weighted average process. This leads to a source context for the decoder step of our way that we essentially calculate the summation of. This weighted average across and the hidden states, summation of multiple states, a case, and attention weight. The probability function now references the context at the L for YL. So for every word we're predicting, we have a context relevant to it. It is notable that the pipeline is differentiable. This is very important, which means that the attention weights are learnable either indirectly, if they are calculated using a simple means like, you know, a dot product, or directly if we choose a more sophisticated way for this core calculation. Think of multi-layer perceptron. Note, the attention score can be thought of as a similarity measure between the current hidden state of the decoder and each hidden state of the encoder, creating this notion of attention. Self-attention. Attention models are still limited by the encoder setup and the way the words are embedded, as seen in chapter 13, as well as the restriction of the memory challenge that we already have observed in the recurrent neural networks or RNNs. Therefore, compressing everything in a context or a group of contexts does little to solve these issues, the issue of the long-range dependency as well as the other things. An interesting idea would be to instead of the decoder attending... to the encoder, how about will the encoder attend to itself? This is self-attention or the self-attention context. Important concepts for us to understand are self-attention is a retrieval mechanism as well as a contextualized representation. The mechanism is having, so the retrieval mechanism, we can think of it as having query Q, and that attempts to find value V using key K. If you have a database that has two values representing some box capacity, and the two values are 1,000 and 1,500, based on the keys associated with the boxes, think of them as 70 and 80 consequently. If the query Q wants to retrieve the value V for the key 70, then the retrieved value will be 1,000. However, if the query was something different, let's say that the query wants to retrieve key 75, then we have a problem because 75 doesn't exist. So what do you do? The solution is to try to figure out what is the proper key for query 75 and therefore what's the proper value. So we take a look at the keys and we calculate the similarity measure for these keys according to the query. So we have keys 70 and 80, so the similarity weight or measure for 70, if the query is 75, the similarity measure for key 70 is 1 divided by the absolute value of 70 minus 75. For key 80, The coefficient or similarity coefficient is one divided by the absolute value of 80 minus 75. In both cases, we get 0.5. We apply this as a weighted average across the values 1,000 and 1,500, and that gives us a new value of 1,250 that did not exist in the original data. So now, using this retrieval mechanism, we can retrieve values for queries where the keys don't originally exist. So how about we apply this on the input tokens? The formula for this is an attention formula that's a function of Q, K, and V, and it looks at the similarity between Q, K, and V. Now, we have to step back, and we still have to think of the translation example of the language model. For encoder tokens, we can take a look at each token, and we think of it as a sequence of itself. The token could be a key, could be a query, could be a value. So every token is represented as a key, as a query, as a value. And now we take a look at the sequence, the entire sequence, and we construct the database structure.

Now every token has a key, a value, a query, and what we do is essentially calculate the similarity for each Q. And you could think of it as now, each token can attend to the other tokens. We can take this Q or this key or this value and we calculate those similarities with every other token. And the process is fairly intuitive. We take each of the token, and the key, the value, and the query associated with the token, say token X, will be converted through a matrix multiplication. So the key will be multiplied by the vector representation of the key of the token. So the token has a vector multiplication. So how do we create Q, K, MV? The vector representation of the token will be multiplied by a Wk, and that will give me the key K. It will be multiplied by WV, and that will give me V. And it will be multiplied by WQ, and that will give me Q. These Ws are initialized and hopefully will be optimized later through backpropagation. So now, for every token, we have a Q, and we have a Q, a V, and a K. Okay? So we take these things, and we calculate the similarity measure between those values. For example, token X, we look at the query for token X, and we have the other keys for all the other tokens. So we take a softmax function, and the softmax function looks at the Q. of the query of token X, multiply it by the key of all other tokens, and that's divided by the dimension of the key, essentially. Let's assume all of them are by square root of that dimension. This soft math, this division will be multiplied by V for the key. And that will give me some similarity value. We take these similarity, so each of these similarity values or similarity measures, will produce, so the output is an attention weight. We'll take all these attention weights for all the other keys and calculate with it the average value for, say, for say V. The input X, therefore, is now represented by calculated V, Q, and K, with the output of the self-attention layer being something called Z. So X mapped into Q, K, and V. The output of this attention layer is Z. We take X, we feed it back into Z as a residual, and that gives me, we can always, we take it, we normalize it, and so multiply it by some weighted function, and that's the output. If I take this process that has one set of weights, WQ, WK, WV, and similar to what we did with convolutional neural network or CNN, we increase the dimension of this attention layer to multiple cases, we'll end up with a multi... headed attention. So now I have multiple attentions. I take all these sets that come out from those different threads or headers and concatenate them and normalize them, and I input them into a feedforward neural network. This process is differentiable. We can map it out. Backpropagation will be used to calculate all these W's. And also, it's very important that I give an example of token X, but this is happening for all tokens in parallel at the same time. The sequence doesn't matter anymore because we're attending to the token with respect to all the other words anyways. And, you know, let's say that we have an example of a red flag. We'll say this operation, their statement was a red flag. And then later on in the sentence, the word red comes out, like red coat, and the word engineering flag comes out. Red flag in this long paragraph has different meaning than red or flag in the same paragraph. The process of self-attention will preserve that interesting relationship between red flag, even though the word red and the word flag individually have their own representation. So that's essentially self-attention in a multi-threaded manner

---

## Batch 2: Blocks (FFN/Residual/LN) + Cross-Attention + Decoding + Positional Encoding (Dictation)

A few concepts to note is the feedforward neural network and the attention layer, or self-attention layer. What the attention layer learns is the local information of the tokens that is attending to this processing, you know, a few tokens here, a few tokens there, even with the multi-head attention. What the feedforward neural network does, it's operating as a teacher who evaluates the behavior and the performance of the students and aggregates this knowledge and moves it forward. So there is a teacher-student dynamic between the feedforward neural network and the attention layer, or the self-attention layer. In the decoder, and this is in the transformer architecture, in the decoder, we have something called cross-attention. The cross-attention operates similarly to the self-attention when it comes to the math. Logically, it operates similar to the sequence-to-sequence in that some of the information comes from the encoder, some of the information comes from the decoder. Here the information that comes from the encoder is the key and the values. The query comes from the decoder. So we take the encoder information, the key, the memory key, and the value, and we take the query from the previous layer from the encoder and put them together in the attention layer. We do what we do there and then we move the residual and then we apply the linear transformation and so on and so forth. We apply the linear, we add and normalize, we add to feedforward neural network, then we add the residual and we normalize. And this happens multiple times as we're capturing multiple information, multiple intricacies of the different tokens. In the decoder, obviously, so we're looking at the entire sequence, obviously, so we're learning multiple different things as we are having multiple blocks. All of these learnings move to a linear layer. What the role of the linear layer is, what the role of the linear layer is that it takes these calculations and expand them, it normalizes them, then expands them, so it takes the output of the normalization, right? and expands it to the size of the dictionary. You can see the dictionary is 100,000 words. It expands this to the size of the dictionary. And then on top of that, we apply a softmax layer. What the softmax layer does, it just calculates the probability of the next predicted token, given the size of the dictionary. And the token with the highest probability is the one that gets chosen. Now, that means every token in the dictionary probably have probability assigned to them. Some of them have higher probability. So we have an opportunity of choosing top probability. We have an opportunity of also using something called temperature. What temperature does, it allows us to manipulate the way the probability works. A very low temperature, and again, do the mathematical computation of it, a very low temperature reinforces the token with the highest probability. So when we choose the token with the highest probability, if we want to choose the next token that's almost always with the highest probability, we'll have a very small temperature. A temperature as zero is not mathematically computable, but it just tells you to take the top, the token with the top probability. Alternatively, if we take the temperature and increase it, the more we increase it, you know, at the extreme to the infinity, every token has equal probability, and then we randomize and we pick the one, we apply a roulette choice, we choose it through rolling roulette, whatever it is, some of the other methodologies that chooses the probability, and then the word with the highest probability gets chosen. So the idea is that if we have a roulette, for example, and the top probability is 0.5, and then we run the roulette and it chooses 0.4, which is a random number. So 0.4 is less than 0.5, so we choose the top word. If the second one has a probability of 0.2, so 0.5 plus 0.2 is 0.7, if the number that gets chosen is 0.6, then we choose the second word, and so on and so forth. This adds a randomness in the sampling, and it creates what we call creativity when it comes to the next layer. But anyways, regardless of these techniques, the softmax concludes the encoder-decoder journey and allows you to make a choice of the next word. For the positional encoding, which we've talked about before, the positional encoding has an interesting issue. So there are ways of encoding the position. The simple way is just a one-hot encoding. You know, if it is the first word of the sequence, just give it one to the first position and zeros everywhere. There's a problem with that. It just encodes position, it doesn't encode sequence, it doesn't encode distances, geometric relationship, encoders and decoders deal usually with dot products, so there's a lot of geometry. dot products is the way we use similarity, so there's a lot of geometric calculation that goes into working within transformers. So we don't have a notion of distance, we cannot generalize to a long sequence, you know, if we train up to 512 tokens or vectors, what happens if you have 513? We cannot generalize. It doesn't, we cannot recover positions. Attention is fundamentally used dot products. We cannot with one-hot encoding recover the position. The sine and cosine calculations we have allow us to have a continuous structure. Nearby positions produce nearby vectors, distant positions produce distant vectors. We can, there's a relative position, so we can recover distance between the different tokens, which is interesting with different words. So they are positionally, so we can move. Low frequency capture long-range structures, high frequency captures fine-grained local differences, so it's the multi-frequency means multi-scale awareness. And we can extrapolate to a longer sequence. If we start, if the positioning, if we're training for 512 only, we can extrapolate the position encoding to 10,000 words. So the sine and cosine way of position encoding helps us create some harmonic coordinate systems. And because transformers operate via dot product, it lends itself very well to this geometric system. It's great, it's beautiful, and it allows for us to capture the distance, the relative offset, the extrapolations, and it's continuous. So in simple terms, we don't use one-hot encoding positionally because attention operates geometrically via dot products and one-hot encoding. have no metric structure. Sinusoidal encodings embed positions into a continuous Fourier space where relative distances and offsets become linearly recoverable.

---

## Batch 3: BERT and GPT (Dictation)

The encoder and the decoder, as two models, separately have inspired different areas of research. The encoder itself, with its self-attention and the way that the different tokens tend to each other, has been tried to build what we call BERT, B-E-R-T. BERT is a stack of encoders where you have the same encoders stacked multiple times, and the softmax is the end of the encoder, and that's where the prediction happens. And the way BERT works is that we take sentences or sequences that get entered, and we mask a word in the middle, for example, and then we try to run a bidirectional attention to it, essentially. You go to the left and you go to the right, and you figure out, okay, how can I have a bidirectional representation of the language, essentially. So the idea of masking and trying to attempt to predict the masking is similar in a sense to word2vec in sampling in particular. And the way it goes is that we are trying to learn about a word from its context. So there's a degree of of sampling when it comes to BERT, or the degree of similarity between BERT and Word2Vec. It's a skip gram in particular, because it has that kind of semantic similarity, synthetic behavior, and co-occurrence statistics that exist in Word2Vec. But in BERT, it's more of a contextual embedding. So in Word2Vec, the word is paired, the vector is per word. In BERT, it's one vector pair, a whole sentence. So that's why BERT, at least analogously, seems similar to Word2Vec, but an execution is much more sophisticated because it provides us with semantic clustering, an analogy, meaningful geometry, and it is optimized for contextual prediction. It has multiple, it's stacked in a sense that it has multiple layers, because one layer will provide you with shadow representation, but if you add multiple encoders, you end up to have an iterative refinement. So maybe one layer will remix the information, the other layer will find some local agreement, some class level structure, coreference, global semantics, as you keep going within the layers. Similar to convolutional neural networks, where the multiple layers apply different feature extractions, we have those stack of encoders in BERT. To provide that kind of a global contextualized representation between the tokens, adding a non-linearity from each layer, adding, having additional transformations, as well as reapplying the residual refinement. It's a more of a deep compositional function that you get from the stack of encoders. In BERT, there's a certain token that's called CLS. The goal of this token is to attend to the whole sentence. So every token attends to every other token in the encoders. CLS is a token at the beginning of the sequence that attends to the entire sequence and gets refined each layer at a time. It persists across and evolves across the coding layers. In a sense, it is a summary of the learnings from the sequence over the different layers as it persists and updates through every transformation layer. If you come to think of it as a learned aggregation of the entire sequence. The second model in what will become large language models is GPT, Generative Pre-trained. Pretrained transformers. GPT, opposite to BERT, is a stack of decoders. Decoders with nothing else. Oh, BERT gets trained for the masked word, so that's how BERT gets trained and backpropagation solves for different trainings and decoders. GPT is a stack of decoders, and those decoders have no input from the encoders, therefore the cross-attention step is gone. What you have is a different form of attention at the encoder layer because you still have to mask, you still have to mask the words right to the predicted word. So, similar to what we talked about masking in transformers on decoders, this one applies more. And the process of those stack of decoders is basically you train on a sequence of tokens, and first, you know, you start, like, think of it, I am a happy person. You start with I, and you train it to predict that the next one is am, then you add am back, and you go and you try to train it on the next word being and, and you progress in that sense. That basically gives you GPT, which is a stack of decoders that has a that has those interesting differences from a transformer. GPT-1 has 12 layers, 117 million parameters, so it created what we should think of as a large language model. So you have a masked multi-head attention that goes to normalization. You have the residual at the normalization. Then it goes. And as we go through the training, the outputs of the model is shifted right and becomes input and so on and so forth. And that gives you essentially a GPT model.


% Chapter 19
\section{Introduction to Evolutionary Computing}\label{chap:evo}
\graphicspath{{assets/lec19/}}

Parts I--III built three complementary toolkits: optimize models against data (ERM), learn representations with gradients, and encode auditable heuristics with fuzzy rules. Each is powerful when you can write a smooth objective or commit to a compact rule base. Many engineering choices, however, live outside that comfort zone: the knobs are discrete or tightly constrained, evaluations are noisy or expensive, and the landscape is rugged enough that local improvement is unreliable.

Evolutionary computing treats that situation as search. Instead of following gradients or hand\hyp{}tuning rules, we maintain a population of candidate designs, score them with a fitness function, and use selection plus variation to explore and refine solutions within a fixed budget. After the fuzzy trilogy (\Crefrange{chap:softcomp}{chap:fuzzyinference}), this chapter develops the evolutionary strand introduced in \Cref{chap:intro} and focuses on population\hyp{}based optimization under constraints, budgets, and stochastic noise.

\paragraph{A distinct paradigm (and what ``nature-inspired'' means here).}
Fuzzy inference systems are a behavioral modeling tool: we write down the reasoning logic directly so a human can audit why a particular decision was made. Evolutionary computing aims at a different target. It encodes \emph{candidate designs} and uses a simplified selection--variation loop---an algorithmic echo of natural evolution---to search for designs that perform well under a fixed evaluation budget (the Karl Sims\hyp{}style intuition: emergence via selection and variation, not hand-coded reasoning). The point is not to be faithful to evolutionary biology; the point is that this process can produce designs that respond well to situations under constraints and noise even when gradients are unavailable and the knobs are mixed discrete/continuous.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
\begin{itemize}
    \item Explain the evolutionary-computation toolbox (GAs, GP, CMA-ES, DE) and when each is well-suited.
    \item Implement population-based optimization loops with selection, crossover/mutation, and constraint handling.
    \item Diagnose convergence, premature stagnation, and feasibility trade-offs on examples such as controller tuning.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
When gradients are unavailable or the landscape is rugged, treat design as search: maintain a diverse population, score candidates with a fitness function, and let selection plus variation drive improvement under constraints.
\end{tcolorbox}

\subsection{Context and Motivation}
\label{sec:evo_context_and_motivation}

Evolutionary computing is the strand you reach for when the objective is a black box (simulation, experiment, or expensive audit) or when the design space is a mix of continuous and discrete choices. The modeling contract stays simple: pick an encoding, define a fitness function, state the constraints, and commit to an evaluation budget. The algorithm then trades exploration (diversity) against exploitation (selecting what works) as it searches a rugged landscape.

In the fuzzy trilogy, once you commit to membership shapes and a rule base, a practical question remains: how do you tune those choices (and their trade-offs) when hand tweaks do not scale? Evolutionary search provides a disciplined answer: score candidate controllers, select the better ones, perturb/recombine them, and keep iterating until the improvements flatten or the budget runs out.

\begin{tcolorbox}[summarybox, title={Vignette: budgeted search in the wild (locomotion, ML/LLMs, and physical experiments)}]
\textbf{Robot gait / locomotion.} Suppose we want a legged robot to move forward robustly across slightly different terrains. The design has mixed knobs: continuous parameters (gains, trajectory coefficients, penalty weights) and discrete choices (controller mode, safety logic, constraint-handling policy). A single trial is expensive and noisy (contact dynamics, initial conditions, sensor noise), so it is natural to treat performance as a black-box fitness, average over multiple seeds, and search under a strict evaluation budget.\par
\textbf{Hyperparameter and pipeline tuning (including LLM systems).} Many modern learning pipelines have the same structure: discrete design choices (model family, prompt/toolchain variant, data filters) plus continuous knobs (learning rates, thresholds, temperatures), measured by an empirical score with variance across runs. Evolutionary search is attractive when gradients are unavailable, the evaluation surface is rugged, or the objective invites silent failure modes (metric gaming, constraint violations).\par
\textbf{Physical-world analogy (jet nozzle).} Classic evolution-strategy examples describe optimizing a jet-nozzle shape via random mutations plus selection: you do not differentiate the wind tunnel; you spend a limited trial budget to evolve better designs. The point of the analogy is not biology, but budgeted optimization under constraints.
\end{tcolorbox}

\subsection{Philosophical and Historical Background}
\label{sec:evo_philosophical_and_historical_background}

Evolutionary computing traces its roots back to the 1950s and 1960s, contemporaneous with early developments in neural networks. It is important to recognize that evolutionary algorithms are not direct scientific models of biological evolution; rather, they are inspired by a simplified, abstracted view of evolutionary principles such as selection, mutation, and reproduction.

\paragraph{Key Insight:} These algorithms are \emph{heuristics}; they provide practical methods to find \emph{good enough} solutions to problems that are otherwise computationally intractable, rather than guaranteed optimal solutions. Consequently, convergence proofs typically ensure improvement in expectation or under restrictive assumptions, but not attainment of the true global optimum.

\begin{tcolorbox}[summarybox, title={Author's note: a pragmatic take on evolution}]
Evolutionary algorithms borrow the language of biology to provide a disciplined way to search rugged landscapes, not to recreate population genetics. The design mandate is pragmatic: deliver a respectable solution within the computational budget, even if it is only approximately optimal. Keep that lens in mind when evaluating selection, mutation, or recombination operators; they are tuned because they help optimization, not because they are biologically faithful.
\end{tcolorbox}

\subsection{Problem Setting: Optimization}
\label{sec:evo_problem_setting_optimization}

\begin{tcolorbox}[summarybox, title={Notation handoff}]
Vectors of candidate solutions are written as \(\mathbf{x}\), objective values as \(J(\mathbf{x})\), and population members as individuals/chromosomes depending on encoding. If symbol reuse from earlier chapters becomes ambiguous, default to local definitions and consult \Cref{app:notation_collisions}.
\end{tcolorbox}

Consider an optimization problem where the goal is to find an input vector \( \mathbf{x} \in \mathbb{R}^n \) that minimizes (or maximizes) a given objective function \( J: \mathbb{R}^n \to \mathbb{R} \). Formally, we want to solve

\begin{align}
    \mathbf{x}^* = \arg \min_{\mathbf{x} \in \mathcal{D}} J(\mathbf{x}), \label{eq:optimization_problem}
\end{align}

where \(\mathcal{D} \subseteq \mathbb{R}^n\) is the feasible domain incorporating any bound, equality, or inequality constraints required by the application.

\paragraph{Challenges:}

\begin{itemize}
    \item The function \( J \) may be \emph{non-convex}, exhibiting multiple local minima and maxima.
    \item The objective or constraints may be undefined or discontinuous in parts of the domain, breaking gradient assumptions.
    \item The feasible set may include combinatorial or integer constraints that resist continuous methods.
    \item There may be no closed-form or deterministic method to find the global optimum, especially for NP-hard variants.
    \item The search space \(\mathcal{D}\) can be large or complex, making exhaustive search (brute force) computationally prohibitive.
    \item Real-time or practical constraints often require solutions within limited time frames.
\end{itemize}

\subsection{Illustrative Example}
\label{sec:evo_illustrative_example}

Imagine a function \( J \) with multiple peaks and valleys (local maxima and minima). The global minimum is the lowest valley, but many local minima exist that can trap naive optimization methods.

\paragraph{Goal:} Instead of guaranteeing the global optimum, evolutionary computing aims to find a \emph{good enough} solution---one that is sufficiently close to optimal and found within a reasonable computational budget.

\subsection{Why Not Brute Force?}
\label{sec:evo_why_not_brute_force}

While brute force search guarantees finding the global optimum by evaluating all possible candidates, it is often infeasible due to:

\begin{itemize}
    \item \textbf{Computational complexity:} The number of candidate solutions can be astronomically large.
    \item \textbf{Time constraints:} Real-world applications often require timely decisions, making exhaustive search impractical.
\end{itemize}

For example, in control systems, one might want to tune parameters to regulate temperature or pressure optimally. Waiting for a brute force search to complete could be unacceptable, whereas a near-optimal solution found quickly is valuable.

\subsection{Summary}
\label{sec:evo_summary}

Checkpoint: you now have the \emph{why} for evolutionary search---rugged landscapes, constraints, discontinuities, and budgets that make local improvement unreliable. The rest of this chapter builds one canonical template in three passes:
(i) a big-picture story (population + evaluation + variation),
(ii) operator-level definitions (selection/crossover/mutation and constraint handling),
and (iii) an implementation-facing sanity-check (flowchart, pseudocode, and a one-generation numeric trace).
Read repeated mentions of ``the GA loop'' as \emph{preview \(\rightarrow\) formalization \(\rightarrow\) checklist}, not as duplication.

Keep the ``rugged landscape'' picture in mind when you later interpret convergence traces, premature stagnation, and diversity metrics in population-based search.

\subsection{Challenges in Continuous Optimization and Motivation for Evolutionary Computing}
\label{sec:evo_challenges_in_continuous_optimization_and_motivation_for_evolutionary_computing}

In many continuous optimization problems, the objective function may be undefined or discontinuous in certain regions of the domain. For example, consider a function with singularities or points where the function value is not defined (akin to division by zero). Such characteristics pose significant challenges for classical optimization methods such as gradient descent or hill climbing, which rely on smoothness and continuity to navigate the search space effectively.

These issues compound the nonconvexity and constraint challenges above: discontinuities and combinatorial feasibility break gradient assumptions, and global guarantees are out of reach for many NP-hard variants. Given these challenges, deterministic approaches may be infeasible or computationally expensive. Instead, we can tolerate approximate solutions and employ heuristic or metaheuristic methods that explore the search space more flexibly. This motivates the use of \emph{evolutionary computing} methods.

\subsection{Evolutionary Computing at a Glance}
\label{sec:evo_introduction_to_evolutionary_computing}

Evolutionary computing (EC) is best viewed as a \emph{budgeted optimizer}: maintain a population of candidates, evaluate them with a fitness function, bias sampling toward better candidates (selection), generate variation (crossover/mutation), handle constraints, and replace the population. Repeat until a stopping rule triggers (budget, target quality, or no-improvement window).

\paragraph{Key Idea}
We evolve a \emph{set} of candidate solutions over successive generations. Unlike gradient-based methods, evolutionary algorithms do not require differentiability, and unlike brute force, they trade guarantees for tractable performance under noise, discontinuities, and combinatorial feasibility.

\paragraph{Genetic Algorithms (GAs)}
One of the most well-known evolutionary algorithms is the Genetic Algorithm (GA). GAs \emph{loosely} mimic evolutionary ideas using a simplified, abstracted model of mechanisms such as selection, crossover, and mutation.

\subsection{Biological Inspiration: Evolutionary Concepts}
\label{sec:evo_biological_inspiration_evolutionary_concepts}

This section is terminology and motivation. If you already think of a GA as ``selection + variation + replacement under a fitness budget,'' feel free to skim; the algorithmic details begin in \Cref{sec:evo_genetic_algorithms_modeling_chromosomes}.

To understand GAs, we briefly review relevant biological concepts:

\paragraph{Chromosomes and Genes}
In biology, an organism's genetic information is encoded in chromosomes, which are long sequences of DNA. Each chromosome contains many genes, which determine specific traits.

\paragraph{Cell Division: Mitosis vs. Meiosis}
\begin{itemize}
    \item \textbf{Mitosis:} A process where a cell divides to produce two genetically identical daughter cells, each containing the full chromosome set (e.g., 46 chromosomes, i.e., 23 pairs in humans). This process is responsible for growth and tissue repair.
    \item \textbf{Meiosis:} A specialized form of cell division that produces gametes (sperm or egg cells) with half the number of chromosomes (haploid). When two gametes combine during fertilization, they form a new cell with a full set of chromosomes (diploid), mixing genetic material from both parents.
\end{itemize}

\paragraph{Genetic Recombination and Variation}
\begin{sloppypar}
During meiosis, chromosomes undergo \emph{crossover} events. Segments of genetic material are exchanged between paired chromosomes. Recombination increases genetic diversity.
\end{sloppypar}

\paragraph{Inheritance and Heredity}
The offspring's chromosomes are a mixture of the parents' genetic material, but not a simple half-and-half split. Instead, genes from multiple previous generations contribute to the genetic makeup, introducing variability and enabling adaptation over time.

\subsection{Implications for Genetic Algorithms}
\label{sec:evo_implications_for_genetic_algorithms}

The biological processes suggest several principles that GAs incorporate:

\begin{itemize}
    \item \textbf{Population-based search:} Maintain a population of candidate solutions (analogous to organisms).
    \item \textbf{Selection:} Preferentially choose better solutions to reproduce, mimicking survival of the fittest.
    \item \textbf{Replacement/elitism:} Form the next generation by replacing some individuals with offspring; optionally preserve the best solutions explicitly.
    \item \textbf{Crossover (Recombination):} Combine parts of two or more parent solutions to create offspring solutions, promoting exploration of new regions in the search space.
    \item \textbf{Mutation:} Introduce random changes to offspring to maintain diversity and avoid premature convergence.
    \item \textbf{Generational evolution:} Repeat the process over multiple generations, gradually improving solution quality.
\end{itemize}

The stochastic nature of these operations allows GAs to explore complex, multimodal landscapes and handle problems where deterministic methods struggle.

\subsection{Summary of Biological Mechanisms Modeled in GAs}
\label{sec:evo_summary_of_biological_mechanisms_modeled_in_gas}

\begin{center}
\begin{tabularx}{0.86\linewidth}{@{}l >{\raggedright\arraybackslash}X@{}}
\toprule
\textbf{Biological Process} & \textbf{GA Analog} \\
\midrule
Chromosomes and genes & Encoding of candidate solutions (chromosomes) \\
Meiosis and fertilization & Crossover of parent chromosomes to produce offspring \\
Genetic recombination & Mixing of solution components \\
Mutation & Random perturbations in offspring \\
Selection & Fitness-based selection of parents and survivors \\
Generations & Iterative improvement over time \\
\bottomrule
\end{tabularx}
\end{center}

The remainder of this section formalizes how candidate solutions are encoded and how genetic operators manipulate those encodings during evolution.

\begin{tcolorbox}[summarybox, title={GA hyperparameters at a glance}]
As a starting point for binary encodings of length \(L\), choose population sizes between \(5L\) and \(10L\), tournament selection with small tournaments (2--4 individuals), one-point or uniform crossover, and mutation probability near \(1/L\) per bit. For real-coded GAs, replace bit-flips with Gaussian mutations on parameters and use simulated binary crossover (SBX) or blend-style crossover to mix parents smoothly \citep{DebAgrawal1995}; then tune population size and mutation scale empirically based on convergence speed and population diversity.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Author's note: population and mutation heuristics}]
Budget dictates population size and diversity mechanisms. If evaluations are cheap, spend them on larger populations to cover the search space; if evaluations are expensive, keep populations modest but invest in diversity-preserving steps (mutation, niching, restarts) to avoid premature convergence. Use the defaults in the box above as a first pass, then tune based on convergence traces and diversity diagnostics.
\end{tcolorbox}

\subsection{Genetic Algorithms: Modeling Chromosomes}
\label{sec:evo_genetic_algorithms_modeling_chromosomes}

In the previous discussion, we introduced the concept of diversity in genetic algorithms (GAs) and the probabilistic nature of evolutionary processes. We now delve deeper into modeling chromosomes and the mechanisms of genetic inheritance, crossover, and mutation, drawing parallels to optimization problems.

\begin{tcolorbox}[summarybox, title={Genetic algorithm at a glance}]
\textbf{Objective:} Optimize an objective \(J(\mathbf{x})\) over a discrete or continuous search space by evolving a population of encoded candidate solutions according to selection, crossover, and mutation.\par
\textbf{Key hyperparameters:} Population size, selection pressure (tournament size or selection temperature), crossover and mutation rates, encoding scheme, and stopping criteria (max generations, no-improvement window, target fitness).\par
\textbf{Common pitfalls:} Premature convergence due to excessive selection pressure or low mutation, deceptive fitness landscapes, constraint violations when mutation or crossover produce infeasible solutions, and overinterpreting stochastic runs without multiple seeds.
\end{tcolorbox}

\paragraph{Chromosomes as Information Carriers}

Recall that chromosomes in GAs represent candidate solutions encoded as strings of data. For modeling purposes, we consider each chromosome as a sequence of bits or symbols, each encoding a piece of information relevant to the problem domain. Formally, let a chromosome be represented as
\[
\mathbf{c} = (c_1, c_2, \ldots, c_L),
\]
where each gene \( c_i \) encodes a particular trait or parameter, and \( L \) is the chromosome length.

\paragraph{Inheritance and Crossover}

During reproduction, offspring inherit genes from parents via crossover and mutation (with some genes passing through unchanged). The next figure gives an intuitive operator-level picture; full selection and crossover details follow in \Cref{sec:evo_selection_in_genetic_algorithms} and \Cref{sec:evo_crossover_operator}.


\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
            group style={group size=3 by 1, horizontal sep=0.9cm},
            width=0.28\linewidth,
            height=0.28\linewidth,
            axis lines=none,
            xmin=0, xmax=1,
            ymin=0, ymax=1
        ]
            % Selection
            \nextgroupplot[title={Selection}]
                \node[draw, rounded corners, fill=cbBlue!10, minimum width=1.6cm, minimum height=0.5cm] (p1) {fit};
                \node[draw, rounded corners, fill=cbBlue!5, minimum width=1.6cm, minimum height=0.5cm, below=0.35cm of p1] (p2) {mid};
                \node[draw, rounded corners, fill=cbBlue!5, minimum width=1.6cm, minimum height=0.5cm, below=0.35cm of p2] (p3) {low};
                \draw[->, thick] (p1.east) -- ++(1.0cm,0) node[anchor=west, font=\scriptsize]{higher selection prob.};
                \draw[->, thick] (p2.east) -- ++(0.8cm,0);
                \draw[->, thick] (p3.east) -- ++(0.6cm,0);
            % Crossover
            \nextgroupplot[title={Crossover}]
                \node[draw, rounded corners, fill=cbOrange!15, minimum width=2.1cm, minimum height=0.5cm] (c1) {1011|001};
                \node[draw, rounded corners, fill=cbOrange!10, minimum width=2.1cm, minimum height=0.5cm, below=0.35cm of c1] (c2) {0100|111};
                \draw[->, thick] (c1.east) -- ++(0.6cm,-0.4cm);
                \draw[->, thick] (c2.east) -- ++(0.6cm,0.4cm);
                \node[draw, rounded corners, fill=cbGreen!20, minimum width=2.1cm, minimum height=0.5cm, right=1.8cm of c1] (o1) {1011|111};
                \node[draw, rounded corners, fill=cbGreen!20, minimum width=2.1cm, minimum height=0.5cm, right=1.8cm of c2] (o2) {0100|001};
                \draw[dashed, gray!70] (c1.east) ++(0.05cm,-0.25cm) -- ++(2.2cm,0);
            % Constraints
            \nextgroupplot[title={Constraints}]
                \node[draw, rounded corners, fill=cbBlue!10, minimum width=1.4cm, minimum height=0.5cm] (cand) {candidate};
                \node[draw, rounded corners, fill=cbOrange!20, minimum width=1.5cm, minimum height=0.5cm, right=1.1cm of cand] (repair) {repair};
                \node[draw, rounded corners, fill=cbGreen!15, minimum width=1.6cm, minimum height=0.5cm, right=1.1cm of repair] (eval) {evaluate};
                \draw[->, thick] (cand.east) -- (repair.west);
                \draw[->, thick] (repair.east) -- (eval.west);
                \node[font=\scriptsize, below=0.15cm of repair] {or penalty/feasible-first};
        \end{groupplot}
    \end{tikzpicture}
    \caption{Evolutionary micro-operators. Left: fitter individuals get sampled more often (roulette/tournament). Middle: crossover splices parents by a mask (one-point shown). Right: constraint handling routes offspring through repair/penalty/feasibility before evaluation. Use this to map an implementation to the canonical operator loop.}
    \label{fig:lec11-ea-micro}
\end{figure}

\Cref{fig:lec11-ea-micro} summarizes selection, crossover, and constraint handling at the operator level.

\paragraph{Modeling the Genetic Operations}

Let \(\mathbf{p}_1\) and \(\mathbf{p}_2\) be parent chromosomes. The offspring chromosome \(\mathbf{o}\) is formed by combining segments from \(\mathbf{p}_1\) and \(\mathbf{p}_2\) according to a crossover pattern, and then applying mutation:
\begin{align}
    \mathbf{o} = \text{Mutate}\big(\text{Crossover}(\mathbf{p}_1, \mathbf{p}_2, \mathbf{u})\big).
    \label{eq:evo_offspring_from_parents}
\end{align}

The crossover operator selects which genes come from which parent, often modeled by a binary mask \(\mathbf{u}\in\{0,1\}^L\), where
\[
o_i = \begin{cases}
(p_1)_i, & \text{if } u_i = 0, \\
(p_2)_i, & \text{if } u_i = 1.
\end{cases}
\]

Mutation (see \Cref{sec:evo_mutation_operator}) perturbs genes with a small probability \(p_m\).

\paragraph{Fitness and selection (preview)}
Chromosomes encode candidate solutions (phenotypes), and fitness scores quantify how well each candidate meets the objective. For example, consider chromosomes representing facial feature variants with fitness values
\[
f = \{80, 75, 60, 65, 40, 20\}.
\]
Higher-fitness chromosomes should be sampled more often, but not deterministically: occasional selection of weaker candidates preserves diversity and reduces premature convergence. A simple baseline is roulette selection: normalize nonnegative fitness values into probabilities and sample with replacement. The point is the \emph{bias} toward what works (not biological fidelity); we formalize selection and work a roulette example in \Cref{sec:evo_selection_in_genetic_algorithms}.

\subsection{Mapping Genetic Algorithms to Optimization Problems}
\label{sec:evo_mapping_genetic_algorithms_to_optimization_problems}

Genetic algorithms can be viewed as heuristic optimization methods. To formalize this analogy, consider the components of an optimization problem:

\begin{itemize}
    \item \textbf{Objective function:} \( J(\mathbf{x}) \), which we seek to maximize or minimize.
    \item \textbf{Constraints:} Conditions restricting the feasible set of solutions.
    \item \textbf{Input parameters:} Decision variables \( \mathbf{x} \).
\end{itemize}

In GAs, the chromosome encodes the input parameters \( \mathbf{x} \), and the fitness function corresponds to the objective function \( J(\mathbf{x}) \).

\paragraph{Key GA Components in Optimization Terms}

\begin{itemize}
    \item \textbf{Encoding:} The method of representing \( \mathbf{x} \) as chromosomes.
    \item \textbf{Initial population:} The starting set of candidate solutions.
    \item \textbf{Fitness evaluation:} Computing \( f(\mathbf{c}) = J(\mathbf{x}) \) for each chromosome.
    \item \textbf{Selection:} Choosing chromosomes for reproduction based on fitness.
    \item \textbf{Crossover and mutation:} Generating new candidate solutions by recombining and perturbing chromosomes.
    \item \textbf{Convergence criteria:} Determining when the algorithm has sufficiently optimized the objective.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Constraint handling in GAs}]
Realistic optimization problems often involve constraints \(g_j(\mathbf{x})\le 0\) or \(h_k(\mathbf{x})=0\). Common strategies include (i) \emph{repair}, which projects infeasible offspring back into the feasible set; (ii) \emph{penalties}, which modify the fitness as \(\tilde{f}(\mathbf{x}) = f(\mathbf{x}) - \rho \sum_j \max(0, g_j(\mathbf{x}))\) for some \(\rho>0\); and (iii) \emph{feasibility rules}, which prefer feasible individuals over infeasible ones at equal objective value. Penalty methods are simple and mesh well with existing GA code, while repair and feasibility rules are preferable when constraints encode hard physical or safety limits.
\end{tcolorbox}

\paragraph{Fitness as Objective Function Proxy}

The fitness function guides the search towards optimal solutions. The closer a chromosome's phenotype is to the desired optimum, the higher its fitness:
\[
f(\mathbf{c}) \propto \text{closeness to optimum}.
\]

This relationship allows GAs to explore the solution space stochastically, balancing exploitation of high-fitness regions and exploration via mutation and recombination.

\subsection{Encoding in Genetic Algorithms}
\label{sec:evo_encoding_in_genetic_algorithms}

We use the chromosome representation from \Cref{sec:evo_genetic_algorithms_modeling_chromosomes}; the focus here is how different encodings affect operators, constraints, and search efficiency.

\paragraph{Genotype and Phenotype}
\begin{itemize}
    \item \textbf{Genotype:} The encoded representation of a solution, e.g., a binary string.
    \item \textbf{Phenotype:} The decoded solution in the problem domain, e.g., real-valued parameters.
\end{itemize}

The goal is to design an encoding scheme that allows efficient exploration of the search space while respecting constraints and enabling effective genetic operations.

\subsubsection{Common Encoding Schemes}
\label{sec:evo_common_encoding_schemes_sub}

\paragraph{1. Binary Encoding}
Each parameter is represented as a binary string of fixed length. For example, if a parameter \( x_i \) is to be represented with precision \( p \), the length of the binary string is chosen accordingly.

\begin{itemize}
    \item Advantages: Simple, well-studied, easy to implement crossover and mutation.
    \item Disadvantages: May suffer from Hamming cliffs (large phenotypic changes from small genotypic changes).
\end{itemize}

\paragraph{2. Floating-Point Encoding}
Parameters are represented directly as floating-point numbers.

\begin{itemize}
    \item Advantages: No decoding needed, natural representation for real-valued parameters.
    \item Genetic operators can be adapted, e.g., crossover by averaging.
    \item Disadvantages: More complex mutation and crossover operators; may require specialized operators to maintain diversity.
\end{itemize}

\paragraph{3. Gray Coding}
A binary encoding where consecutive numbers differ by only one bit, reducing the Hamming distance between adjacent values.

\begin{itemize}
    \item Useful to reduce large jumps in phenotype space due to small genotypic changes.
    \item Decoding involves mapping Gray code to decimal values.
\end{itemize}

\subsubsection{Example: Binary Encoding of Parameters}
\label{sec:evo_example_binary_encoding_of_parameters_sub}

Suppose we want to encode four parameters \( x_1, x_2, x_3, x_4 \) each represented by a binary string of length \( l_i \). The genotype is the concatenation of these binary strings:

\[
\underbrace{b_{1,1} b_{1,2} \cdots b_{1, l_1}}_{x_1} \quad
\underbrace{b_{2,1} b_{2,2} \cdots b_{2, l_2}}_{x_2} \quad
\underbrace{b_{3,1} b_{3,2} \cdots b_{3, l_3}}_{x_3} \quad
\underbrace{b_{4,1} b_{4,2} \cdots b_{4, l_4}}_{x_4}
\]

For example, a genotype might look like:
\[
011 \quad 00100 \quad 0101 \quad 011110
\]

Each substring is decoded to a decimal or real value according to the encoding scheme.

\subsubsection{Example Problem: Minimization with Constraints}
\label{sec:evo_example_problem_minimization_with_constraints_sub}

Consider the problem:
\[
\min_{x} \quad f(x) = \frac{x}{2} + \frac{125}{x}
\]
subject to
\[
0 < x \leq 15
\]
For example, \(x=5\) gives \(f(x)=5/2+125/5=27.5\).

\paragraph{Encoding Strategy}
\begin{itemize}
    \item Since \( x \) is bounded between 0 and 15, we can encode \( x \) as a binary string representing integers in \([1, 15]\).
    \item For example, 4 bits can represent integers from 0 to 15, so we can use 4 bits and exclude zero.
    \item Each genotype corresponds to a candidate solution \( x \).
\end{itemize}

\paragraph{Decoding}
\[
x = \text{decimal value of binary string}
\]

If the decoded value is zero, it is invalid due to division by zero, so such genotypes are discarded or penalized.

\paragraph{Fitness Evaluation}
For minimization problems, selection still expects ``higher is better,'' so define fitness as a monotone transform of the cost (e.g., \(F(x)=-f(x)\) or \(F(x)=1/(1+f(x))\)), and then incorporate penalties or feasibility rules for constraint violations.

\subsection{Population Initialization and Size}
\label{sec:evo_population_initialization_and_size}

Once encoding is decided, the initial population is generated by randomly sampling genotypes within the feasible space.

\paragraph{Population Size}
\begin{itemize}
    \item Larger populations provide better coverage of the search space but increase computational cost.
    \item Smaller populations may converge prematurely.
    \item Typical sizes range from 20 to several hundreds depending on problem complexity.
\end{itemize}

\paragraph{Example}
For the problem above, a population of 50 individuals with 4-bit genotypes representing \( x \in [1,15] \) can be initialized by randomly generating 50 binary strings of length 4.

\subsection{Genetic Operators}
\label{sec:evo_genetic_operators}

After initialization, genetic operators are applied to evolve the population.

Selection is detailed in \Cref{sec:evo_selection_in_genetic_algorithms}, crossover in \Cref{sec:evo_crossover_operator}, and mutation in \Cref{sec:evo_mutation_operator}.

% Chapter 19 (continued)

\subsection{Selection in Genetic Algorithms}
\label{sec:evo_selection_in_genetic_algorithms}

We now formalize selection: how fitness scores translate into parent choice and survival probabilities so the search favors better solutions without collapsing diversity.

\subsubsection{Fitness and Selection Probability}
\label{sec:evo_fitness_and_selection_probability_sub}

Given a population of $N$ chromosomes, each chromosome $i$ has an associated fitness value $f_i$. The fitness function quantifies the quality of the solution represented by the chromosome.

A common approach to selection is to assign each chromosome a probability of being chosen proportional to its fitness. This can be expressed as:
\begin{align}
    p_i = \frac{f_i}{\sum_{j=1}^N f_j}, \quad i=1,2,\ldots, N,
    \label{eq:selection_probability}
\end{align}
where $p_i$ is the probability that chromosome $i$ is selected.

\paragraph{Practical note (fitness scaling).}
Proportional (roulette) selection assumes \emph{nonnegative} fitness values. If your objective can be negative (as in \Cref{tab:ga-toy}) or you are minimizing a cost, define a shifted/scaled fitness, e.g.,
\(\tilde f_i = f_i - \min_j f_j + \varepsilon\) with \(\varepsilon>0\), or use rank/tournament selection, which only relies on order comparisons.

\paragraph{Roulette Wheel Selection}

This proportional selection method is often called \emph{roulette wheel selection}. Imagine a wheel divided into $N$ slices, each slice corresponding to a chromosome and sized proportionally to $p_i$. To select a chromosome, a random number is generated to "spin" the wheel, and the chromosome corresponding to the slice where the wheel stops is chosen.

Key properties:
\begin{itemize}
    \item Chromosomes with higher fitness have a larger slice and thus a higher chance of being selected.
    \item The same chromosome can be selected multiple times, reflecting its relative superiority.
    \item This stochastic process maintains diversity but can be sensitive to fitness scaling.
\end{itemize}

\paragraph{Example}

Suppose we have 5 chromosomes with fitness values:
\[
f = [10, 20, 5, 15, 50].
\]
The total fitness is $100$, so the selection probabilities are:
\[
p = [0.10, 0.20, 0.05, 0.15, 0.50].
\]
For instance, \(p_2 = 20/100 = 0.20\) and \(p_5 = 50/100 = 0.50\).
Chromosome 5 has a 50\% chance of selection, making it likely to be chosen multiple times.

\subsubsection{Ranking Selection}
\label{sec:evo_ranking_selection_sub}

When fitness values are close or vary widely, roulette wheel selection may not perform well. For example, if fitness values are very close, selection probabilities become nearly uniform, reducing selection pressure. Conversely, if one chromosome dominates, diversity may be lost prematurely.

\emph{Ranking selection} addresses this by assigning selection probabilities based on the rank of chromosomes rather than raw fitness values.

\paragraph{Procedure}

\begin{enumerate}
    \item Sort chromosomes by fitness in descending order.
    \item Assign ranks $r_i$ such that the best chromosome has rank 1, the second best rank 2, and so forth.
    \item Define a selection probability function $p(r_i)$ decreasing with rank.
\end{enumerate}

A simple linear ranking scheme is:
\begin{align}
    p(r_i) = \frac{2 - s}{N} + \frac{2(r_i - 1)(s - 1)}{N(N - 1)},
    \label{eq:linear_ranking}
\end{align}
where $s \in [1,2]$ controls selection pressure. When $s=1$, all chromosomes have equal probability; when $s=2$, the best chromosome has twice the average probability.

\paragraph{Elitism}

Ranking selection can be combined with \emph{elitism}, where the best chromosome(s) are guaranteed to survive to the next generation. This ensures that the highest-quality solutions are preserved.

\paragraph{Advantages}

\begin{itemize}
    \item Controls selection pressure explicitly.
    \item Prevents premature convergence by maintaining diversity.
    \item Avoids issues with scaling fitness values.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Selection pressure and exploration/exploitation}]
\begin{itemize}
    \item \textbf{Knobs:} Tournament size, rank-selection parameter $s$, crossover/mutation rates, and elitism all modulate pressure.
    \item \textbf{High pressure} (large tournaments, strong elitism, low mutation) speeds exploitation but risks premature convergence.
    \item \textbf{Low pressure} (small tournaments, higher mutation) preserves diversity but slows progress.
    \item \textbf{Practical default:} start with tournament size 2--3, modest elitism (top 1--5\%), \(p_c \approx 0.8\text{--}0.9\), and mutation tuned so 1--5\% of bits/genes change per generation.
\end{itemize}
\end{tcolorbox}

\subsection{Crossover Operator}
\label{sec:evo_crossover_operator}

After selection, the \emph{crossover} operator generates new offspring chromosomes by recombining parts of two parent chromosomes; this formalizes the intuition previewed above and supports controlled exploration.

\subsubsection{One-Point Crossover}
\label{sec:evo_one_point_crossover_sub}

Consider two parent chromosomes represented as binary strings of length $L$:
\[
\text{Parent 1: } \mathbf{c}^{(1)} = (c^{(1)}_1, c^{(1)}_2, \ldots, c^{(1)}_L)
\]
\[
\text{Parent 2: } \mathbf{c}^{(2)} = (c^{(2)}_1, c^{(2)}_2, \ldots, c^{(2)}_L)
\]

One-point crossover proceeds as follows:

\begin{enumerate}
    \item Choose a crossover point $k$ uniformly at random from $\{1, 2, \ldots, L-1\}$.
    \item Create two offspring by exchanging the tails of the parents at point $k$:
    \begin{align*}
        \text{Offspring 1} &= (c^{(1)}_1, \ldots, c^{(1)}_k, c^{(2)}_{k+1}, \ldots, c^{(2)}_L), \\
        \text{Offspring 2} &= (c^{(2)}_1, \ldots, c^{(2)}_k, c^{(1)}_{k+1}, \ldots, c^{(1)}_L).
    \end{align*}
\end{enumerate}

This operator allows mixing of genetic material between parents to create novel combinations.

\paragraph{Multi-point and uniform crossover.}
Two-point or multi-point crossover swaps multiple segments between parents; uniform crossover swaps each gene independently with a fixed probability. These variants increase mixing but can disrupt building blocks if overused.

\paragraph{Crossover probability.}
Apply crossover with probability \(p_c\) (typically 0.6--0.9). When crossover is skipped, offspring are usually copied forward and then mutated, which helps preserve good structures while maintaining exploration.

\subsection{Mutation Operator}
\label{sec:evo_mutation_operator}

Mutation introduces random alterations to individual chromosomes, mimicking biological mutations. It serves to maintain genetic diversity within the population and helps the algorithm escape local optima.

\paragraph{Biological motivation} Mutation is a rare event in nature but crucial for evolution. For example, the white coloration of polar bears is a mutation that provided an adaptive advantage in snowy environments. Similarly, environmental pressures can select for mutations, such as female elephants in Africa evolving to lack ivory tusks to avoid poaching.

\paragraph{Role in optimization} Mutation allows the algorithm to explore new regions of the search space that are not reachable by crossover alone. Consider a fitness landscape with multiple local maxima and minima. Mutation can randomly perturb a solution, potentially moving it from a local minimum to a region near a global maximum.

\paragraph{Implementation of mutation} In binary-encoded chromosomes, mutation typically involves flipping a bit:
\[
0 \to 1, \quad 1 \to 0
\]
The mutation is applied with a small \emph{mutation probability} \(p_m\), often on the order of \(10^{-3}\) to \(10^{-1}\).

\paragraph{Mutation operator formalization}
Given a binary chromosome \(\mathbf{c}\in\{0,1\}^L\), mutation produces \(\mathbf{c}'\) by mutating each bit \(c_i\) independently with probability \(p_m\):
\[
c_i' = \begin{cases}
1 - c_i, & \text{with probability } p_m, \\
c_i, & \text{with probability } 1 - p_m.
\end{cases}
\]

\subsection{Summary of Genetic Operators and Their Probabilities}
\label{sec:evo_summary_of_genetic_operators_and_their_probabilities}

You have now seen each operator in isolation. When you implement a GA, you typically configure the loop with a small set of \emph{operator knobs}:
\begin{itemize}
    \item \textbf{Selection scheme + pressure:} roulette/ranking/tournament selection, plus an explicit pressure parameter (e.g., fitness scaling, rank parameter \(s\), tournament size) and often an elitism rate (top \(k\) or top \(e\%\) copied forward).
    \item \textbf{Crossover probability \(p_c\):} with probability \(p_c\), recombine two parents; otherwise copy parents forward (and still allow mutation).
    \item \textbf{Mutation probability \(p_m\):} for binary encodings, mutate each bit independently with probability \(p_m\) (a common default is \(p_m \approx 1/L\) for length \(L\)); for real encodings, \(p_m\) usually corresponds to a mutation \emph{scale} (e.g., Gaussian noise standard deviation) rather than bit flips.
\end{itemize}

To keep notation compact later, we write these scheme-dependent settings abstractly as
\[
\begin{aligned}
p_s &\equiv \text{selection pressure parameter(s) (scheme-dependent)},\\
p_c &= \text{probability of applying crossover},\\
p_m &= \text{mutation probability (per gene/bit, unless stated otherwise)}.
\end{aligned}
\]

Tuning these settings controls exploration versus exploitation and strongly affects whether the algorithm stagnates early or continues to make progress.

% Chapter 19 (continued)

\subsection{Known Issues in Genetic Algorithms}
\label{sec:evo_known_issues_in_genetic_algorithms}

\begin{tcolorbox}[summarybox, title={Risk \& audit}]
\begin{itemize}
    \item \textbf{Premature convergence:} aggressive selection or weak mutation collapses diversity before good regions are explored.
    \item \textbf{Budget mismatch:} comparisons are invalid unless algorithms share evaluation budgets and stopping rules.
    \item \textbf{Constraint leakage:} hidden infeasibility can inflate scores; use explicit feasibility checks in logs and plots.
    \item \textbf{Single-run overclaim:} stochastic algorithms require multi-seed reporting with spread, not one best trajectory.
    \item \textbf{Objective gaming:} fitness proxies can drift from deployment goals; audit secondary metrics and failure cases.
\end{itemize}
\end{tcolorbox}

While genetic algorithms (GAs) provide a powerful heuristic framework for optimization, several well-known issues can affect their performance and reliability:

\paragraph{Premature Convergence}
Because GAs rely on heuristic search without a global optimality guarantee, they often converge prematurely to local minima rather than the global minimum. This is especially common if the initial population is not diverse or if the selection pressure is too high, causing loss of genetic diversity early on.

\begin{tcolorbox}[summarybox, title={Diversity maintenance}]
\begin{itemize}
    \item \textbf{Crowding/sharing:} penalize overly similar individuals to keep multiple niches in multi-modal landscapes.
    \item \textbf{Restarts/islands:} run multiple subpopulations (often in parallel) with occasional migration; robust against stagnation.
    \item \textbf{Adaptive mutation:} increase mutation or inject random individuals when diversity drops.
\end{itemize}
\end{tcolorbox}

\paragraph{Mutation Interference}
Mutation is intended to introduce diversity and help escape local minima by randomly altering genes. However, excessive or poorly controlled mutation can cause oscillations, where beneficial mutations are undone by subsequent mutations. This back-and-forth effect can prevent convergence and degrade solution quality.

\paragraph{Deception}
Deception refers to situations where the encoding or representation of solutions misleads the GA's fitness evaluation. Low-order schemata with high observed fitness may actually guide the search away from the global optimum, so that combining ``good'' building blocks produces worse offspring. There is no single formal definition, but a deceptive fitness landscape is one in which local improvements inferred from schemata systematically lead the GA to suboptimal basins of attraction.

\paragraph{Fitness Misinterpretation}
Since selection is driven by fitness values, any inaccuracies or misleading fitness evaluations can cause the GA to make poor decisions about which individuals to propagate. This can arise from noisy fitness functions, poorly designed objective functions, or deceptive encodings.

\subsection{Convergence Criteria}
\label{sec:evo_convergence_criteria}

Determining when to stop the GA is a critical practical consideration. Common convergence criteria include:

\begin{itemize}
    \item \textbf{Fixed number of generations:} Run the GA for a predetermined number of iterations.
    \item \textbf{Time limit:} Stop after a fixed amount of computational time.
    \item \textbf{No improvement:} Terminate if the best fitness value has not improved over a specified number of generations.
    \item \textbf{Manual inspection:} Periodically inspect the population to decide if the solutions are satisfactory.
\end{itemize}

In practice, a combination of these criteria is often used. For example, one might stop if \emph{either} (a) no improvement in the best fitness is observed for 10 consecutive generations, \emph{or} (b) the run reaches 100 generations in total, whichever condition is met first. \Cref{fig:lec11-ga-progress} visualizes such a run, making it easy to spot plateaus and the persistent gap between best and mean fitness.
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            set layers,
            width=0.82\linewidth,
            height=0.42\linewidth,
            xlabel={Generation},
            ylabel={Normalized fitness},
            xmin=0, xmax=50,
            ymin=0.4, ymax=1.02,
            legend style={at={(0.5,1.03)}, anchor=south, legend columns=2}
        ]
            % no-improvement window shading (draw first so it stays behind the curves)
            \addplot[draw=none, fill=gray!15, on layer=axis background] coordinates {(32,0.4) (45,0.4) (45,1.0) (32,1.0)};
            \addplot[cbBlue, thick, mark=*, mark size=1.8pt] table[row sep=\\] {
                0 0.55 \\
                5 0.60 \\
                10 0.68 \\
                15 0.74 \\
                20 0.79 \\
                25 0.83 \\
                30 0.87 \\
                35 0.90 \\
                40 0.93 \\
                45 0.95 \\
                50 0.96 \\
            };
            \addlegendentry{Best fitness}
            \addplot[cbOrange, thick, dashed, mark=x, mark size=1.8pt] table[row sep=\\] {
                0 0.52 \\
                5 0.55 \\
                10 0.58 \\
                15 0.63 \\
                20 0.67 \\
                25 0.71 \\
                30 0.75 \\
                35 0.78 \\
                40 0.80 \\
                45 0.81 \\
                50 0.82 \\
            };
            \addlegendentry{Mean fitness}
            \addplot[gray, dash dot] coordinates {(32,0.4) (32,1.0)};
            \node[anchor=south east, font=\scriptsize] at (axis cs:31.5,0.82){Flat region $\Rightarrow$ consider stopping};
        \end{axis}
    \end{tikzpicture}
    \caption{Illustrative GA run showing the best and mean normalized fitness over 50 generations. Flat regions motivate ``no improvement'' stopping rules, while steady separation between best and mean indicates ongoing selection pressure. Helpful for diagnosing premature convergence versus ongoing exploration.}
    \label{fig:lec11-ga-progress}
\end{figure}


\subsection{Summary of Genetic Algorithm Workflow}
\label{sec:evo_summary_of_genetic_algorithm_workflow}

This is the implementation-facing assembly of the GA loop. Earlier sections motivated the need for population-based search and defined each operator; here we put them back together and give three complementary sanity-check views: \Cref{fig:lec11-ga-flow} for control flow, \Cref{tab:ga-toy} for a one-generation numeric trace, and \Cref{sec:evo_pseudocode_representation} for a minimal implementation template. All three describe the same loop at different levels of detail.

\begin{figure}[!ht]
\centering
\begin{adjustbox}{max width=\linewidth, center}
\begin{tikzpicture}[
  font=\small,
  >=Stealth,
  node distance=18mm and 26mm,
  line width=0.9pt,
  block/.style={
    draw, rounded corners=2pt,
    minimum width=38mm, minimum height=12mm,
    align=center
  },
  decision/.style={
    draw, diamond, aspect=2.2,
    inner sep=1.5pt, align=center,
    minimum width=24mm, minimum height=14mm
  },
  flow/.style={->},
  loop/.style={->, dashed},
  lab/.style={font=\footnotesize, inner sep=1pt}
]

% --- Nodes (stable layout) ---
\node[block]   (init) {Initialization\\random population};
\node[block, right=of init] (fit) {Fitness evaluation};
\node[decision, right=of fit] (term) {Termination?};

\node[block, below=18mm of fit] (sel) {Selection};
\node[block, right=of sel] (cross) {Crossover};
\node[block, right=of cross] (mut) {Mutation};
\node[block, below=18mm of cross] (rep) {Replacement};

% --- Main flow ---
\draw[flow] (init) -- (fit);
\draw[flow] (fit) -- (term);

\draw[flow] (term.south) -- node[right, lab, xshift=2pt]{no} (sel.north);
\draw[flow] (sel) -- (cross);
\draw[flow] (cross) -- (mut);

% mutation down then to replacement (right-angle, no wandering)
\draw[flow] (mut.south) -- ++(0,-10mm) -| (rep.east);

% replacement feeds next generation evaluation (clean dashed loop)
\draw[loop] (rep.north) -- node[left, lab, pos=0.35, yshift=1pt]{next generation} (fit.south);

% optional ``inner loop'' annotation (slightly above for readability)
\draw[<->, dashed] ([yshift=4mm]sel.east) -- node[above, lab, yshift=2pt]{inner loop} ([yshift=4mm]cross.west);

% yes/stop arrow drawn as overlay so it does not blow up the bounding box
\begin{scope}[overlay]
  \draw[flow] (term.north) -- ++(0,12mm) node[above, lab, yshift=1pt]{yes / stop};
\end{scope}

\end{tikzpicture}
\end{adjustbox}
\caption{GA flowchart showing the iterative process: initialization leads to fitness evaluation and a termination check. If not terminated, the algorithm proceeds through selection, crossover, mutation, and replacement, which then feeds the next generation's fitness evaluation. Reference when verifying that your implementation preserves the intended control flow.}
\label{fig:lec11-ga-flow}
\end{figure}

For the toy generation in \Cref{tab:ga-toy}, fitness values are computed using \(f(x)=\cos(5\pi x)\exp(-x^2)\) from \Cref{sec:evo_example_ga_for_a_constrained_optimization_problem}.

\begin{table}[h]
\centering
\small
\begin{tabularx}{\linewidth}{@{}l >{\raggedright\arraybackslash}X >{\raggedleft\arraybackslash}p{0.18\linewidth} >{\raggedleft\arraybackslash}p{0.16\linewidth}@{}}
\toprule
Step & Example bitstrings & Decoded \(x\) & Fitness \(f(x)\) \\
\midrule
Initial (subset) & \(\,0001_2\), \(\,0010_2\), \(\,0101_2\), \(\,0111_2\) & 0.0625, 0.125, 0.3125, 0.4375 & 0.553, -0.377, 0.177, 0.687 \\
Select parents (example) & \(\,0010_2\), \(\,0101_2\) & 0.125, 0.3125 & -0.377, 0.177 \\
Crossover (one-point) & Parents: \(\,00|10\), \(\,01|01\) \(\to\) offspring: \(\,00|01\), \(\,01|10\) & 0.0625, 0.3750 & 0.553, 0.803 \\
Mutation (flip one bit) & \(\,0110_2 \to 0111_2\) & 0.4375 & 0.687 \\
\bottomrule
\end{tabularx}
% Avoid inline math in captions; it wraps poorly in some EPUB renderers.
\caption{Toy GA generation on a bounded interval. One crossover and mutation illustrate how the fitness function guides selection before the next generation. Use this to explain how variation operators interact with selection pressure.}
\label{tab:ga-toy}
\end{table}

\Cref{tab:ga-toy} is the step-by-step toy generation trace used to ground the operator sequence.

\begin{tcolorbox}[summarybox, title={Defaults that work (DE/CMA-ES)}]
\textbf{Differential Evolution (DE):} start with population \(10\!-\!20\times D\) (dimension \(D\)), mutation scale \(F\in[0.5,0.8]\), crossover \(C_r\in[0.7,0.9]\).
\textbf{CMA-ES:} population \(\lambda\approx 4+\lfloor 3\ln D\rfloor\), initial step-size \(\sigma_0 \approx 0.3\) of the variable range. These defaults are robust first tries before tuning.
\end{tcolorbox}

\subsection{Pseudocode Representation}
\label{sec:evo_pseudocode_representation}

The GA can be expressed in pseudocode as follows. Treat this as the ``minimum viable'' skeleton; a production implementation additionally records random seeds, evaluation budgets, constraint handling decisions, and per-generation diagnostics (best/mean fitness, diversity) so results are reproducible and interpretable.

\begin{verbatim}
Initialize population P with N chromosomes
Evaluate fitness of each chromosome in P

while termination criteria not met do
    Select parents from P based on fitness
    Apply crossover to parents to create offspring
    Apply mutation to offspring
    Evaluate fitness of offspring
    Replace some or all of P with offspring
end while

Return best chromosome found
\end{verbatim}

\subsection{Example: GA for a Constrained Optimization Problem}
\label{sec:evo_example_ga_for_a_constrained_optimization_problem}

Consider the problem of \emph{maximizing} the function:
\[
f(x) = \cos(5 \pi x) \cdot \exp(-x^2)
\]
subject to the constraint:
\[
0 \leq x \leq 0.5
\]
with a precision of three decimal places.

\paragraph{GA Parameters:}
\begin{itemize}
    \item Population size: 10 chromosomes
    \item Encoding: Fixed-point with resolution 0.001: store an integer \(n\in\{0,1,\ldots,500\}\) and decode via \(x=n/1000\) (a 9-bit representation covers 0--511)
    \item Crossover probability: 25\%
    \item Mutation probability: 10\%
    \item Selection: Truncation selection (example): select the top five chromosomes by fitness as parents each generation
\end{itemize}

\paragraph{Initialization:}
Generate 10 random values of \(x\) uniformly distributed in \([0, 0.5]\), each rounded to three decimal places. When prior designs or surrogate models exist, \emph{warm start} a few chromosomes with those known-good solutions before filling the rest randomly; seeding accelerates convergence without losing diversity if you keep most of the population stochastic.

\paragraph{Fitness Evaluation:}
Calculate \(f(x)\) for each chromosome and treat it as a fitness score (higher is better). If instead you are minimizing a cost, convert it to fitness via a monotone transform (e.g., \(-J(x)\), a shifted score, or a rank-based scheme) so that selection still prefers better candidates.

\paragraph{Evolutionary Cycle:}
Apply selection, crossover, and mutation to produce new offspring, then evaluate their fitness. Repeat this process for multiple generations until convergence criteria are met.

\paragraph{Remarks:}
In practice, some initial chromosomes may fall outside the constraint bounds due to rounding or mutation; these should be clipped or repaired to maintain feasibility.

As an illustration, if the initial population contains
\[
\begin{aligned}
x &\in \{0.04,\;0.09,\;0.13,\;0.18,\;0.22,\;0.27,\\
&\qquad 0.31,\;0.36,\;0.42,\;0.48\},
\end{aligned}
\]
then the corresponding objective values are
\[
\begin{aligned}
f(x) &\in \{0.808,\;0.155,\;-0.446,\;-0.921,\;-0.906,\;-0.422,\\
&\qquad 0.142,\;0.711,\;0.797,\;0.245\},
\end{aligned}
\]
rounded to three decimals.
Each chromosome uses a 9\text{-}bit fixed-point code (3 fractional digits), decoded by interpreting the bits as an integer \(n\) and scaling via \(x=n/1000\). Any decoded values outside \([0,\,0.5]\) are repaired (e.g., clipped to bounds); note that \(x=0\) is allowed in this example.

A single generation could proceed as follows:
\begin{itemize}
    \item Select the top five chromosomes by fitness.
    \item Apply one-point crossover on the 9-bit fixed-point codes (MSB-first). For example, \(0.203 \mapsto 011001011_2\) and \(0.359 \mapsto 101100111_2\); cutting after 5 bits and swapping tails yields offspring \(011000111_2 \mapsto 0.199\) and \(101101011_2 \mapsto 0.363\).
    \item Mutate each bit with probability 0.1, ensuring all decoded values remain within \([0,\,0.5]\).
    \item Re-evaluate fitness and retain the best ten individuals for the next generation.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Constraint handling playbook}]
\begin{itemize}
    \item \textbf{Penalty methods} soften constraints by augmenting the fitness with a violation term, e.g., \(F(\mathbf{x}) = f(\mathbf{x})+\lambda \sum_k \max\{0, g_k(\mathbf{x})\}^2\); increase \(\lambda\) when infeasible individuals survive too often.
    \item \textbf{Repair operators} project infeasible chromosomes back into the feasible region (clip bound violations, renormalize equality constraints, or rerun a problem-specific solver) before evaluation.
    \item \textbf{Feasibility-first selection} ranks feasible candidates ahead of infeasible ones, then compares raw fitness only within each group; among infeasible solutions, select those with the smallest violation.
    \item \textbf{Deb's feasibility rules} \citep{Deb2001Book}: (i) if one solution is feasible and the other is not, pick the feasible one; (ii) if both are feasible, pick the better objective; (iii) if both are infeasible, pick the one with smaller total constraint violation. Adaptive penalties can be layered on top when violations persist.
\end{itemize}
\paragraph{Reproducibility and fair comparison} Fix random seeds when debugging, run many seeds (e.g., 20+) for reporting, match evaluation budgets across algorithms, and report mean/median best-so-far with variability bands. Log all hyperparameters and share code/configs to make comparisons fair, following \Cref{app:repro_standards} as the default reporting template.
Penalty terms are easy to implement, repair operators exploit domain knowledge, and feasibility-first policies are useful in safety-critical controllers where violating constraints is unacceptable even temporarily.
\end{tcolorbox}

% End of Chapter 19 (continued)

\subsection{Genetic Algorithms: Iterative Process and Convergence}
\label{sec:evo_genetic_algorithms_iterative_process_and_convergence}

This section focuses on convergence behavior; the GA cycle itself is summarized in \Cref{sec:evo_summary_of_genetic_algorithm_workflow} and the control flow in \Cref{fig:lec11-ga-flow}. Over generations, populations tend to cluster around better regions of the fitness landscape; flat best\hyp{}fitness curves signal stagnation, while a persistent gap between best and mean indicates ongoing selection pressure (see \Cref{fig:lec11-ga-progress}). Convergence typically occurs after a problem\hyp{}dependent number of generations, and the best\hyp{}so\hyp{}far solution should be treated as a high\hyp{}quality approximation rather than a guaranteed global optimum.

\subsection{Beyond canonical GAs: real-coded strategies}
\label{sec:evo_beyond_canonical_gas_real_coded_strategies}

Bit-string encodings are ideal for combinatorial search, yet most engineering problems have continuous decision variables. Two mature real-coded families are now standard tools:

\begin{itemize}
    \item \textbf{Covariance Matrix Adaptation Evolution Strategy (CMA-ES)} \citep{Hansen2001} maintains a multivariate Gaussian search distribution. Successful steps update the mean, adapt the global step size, and rotate the covariance to align with the landscape's principal directions. CMA-ES shines on smooth, ill-conditioned black-box functions where gradients are unavailable but the objective rewards second-order adaptation.
    \item \textbf{Differential Evolution (DE)} \citep{Storn1997} perturbs a target vector with scaled differences of two other individuals, \(\mathbf{v} = \mathbf{x}_r + F(\mathbf{x}_p - \mathbf{x}_q)\), then mixes \(\mathbf{v}\) with the original via binomial or exponential crossover. This simple mechanism balances exploration/exploitation with only three hyperparameters \((F,\, C_r,\, N)\) and handles noisy, non-smooth objectives well.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Evolution strategies (ES): step-size adaptation and self-adaptation}]
Evolution strategies are a real-coded evolutionary family that makes one engineering idea explicit: \emph{mutation step size is part of the algorithm's state, not a fixed constant.} In a simple isotropic form,
\[
\mathbf{x}'=\mathbf{x}+\boldsymbol{\epsilon},\qquad \boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\,\sigma^2\mathbf{I}),
\]
where \(\sigma\) controls exploration (large \(\sigma\)) versus local refinement (small \(\sigma\)). A classic adaptive heuristic is the ``1/5 success rule'': if more than 20\% of recent mutations improve fitness, increase \(\sigma\); if fewer than 20\% succeed, decrease \(\sigma\). Self-adaptive variants go one step further by embedding \(\sigma\) (and sometimes covariance structure) into the chromosome itself so selection pressure co-evolves the search behavior alongside the solution parameters.
\end{tcolorbox}

Both algorithms plug into the same evaluation loop shown earlier and can reuse the constraint-handling policies in the preceding box. In practice many teams prototype with DE (fast, few knobs) and switch to CMA-ES when the problem demands higher precision or adaptive covariance modeling.

\subsection{Genetic Programming (GP)}
\label{sec:evo_genetic_programming_gp}

Genetic programming extends the principles of genetic algorithms to the evolution of computer programs or symbolic expressions rather than fixed-length parameter vectors.

\paragraph{Problem Setup}

Consider a problem where the relationship between input variables $x_1, x_2, \ldots, x_n$ and output $y$ is unknown. Unlike traditional parameter estimation, we do not assume a fixed functional form. Instead, we want to discover the function $f$ such that
\[
y = f(x_1, x_2, \ldots, x_n).
\]

\paragraph{Representation of Programs}

In GP, candidate solutions are represented as tree-like structures encoding mathematical expressions or programs composed of:

\begin{itemize}
    \item \textbf{Terminals:} Input variables ($x_1, x_2, \ldots$) and constants.
    \item \textbf{Functions:} Arithmetic operations (addition, subtraction, multiplication, division), logical operations, or other domain-specific functions.
\end{itemize}

For example, a candidate program might represent the expression
\[
(x_1 \times x_3) + (x_1 + x_4).
\]

\paragraph{Genetic Operators in GP}

\begin{itemize}
    \item \textbf{Crossover:} Subtrees from two parent programs are exchanged to create offspring programs.
    \item \textbf{Mutation:} Random modifications are made to nodes in the program tree, such as changing an operator or replacing a subtree.
\end{itemize}

These operations allow the evolution of increasingly complex and effective programs.

\paragraph{Fitness Evaluation}

A candidate program is evaluated by executing it on a training set and comparing its outputs with the desired targets. Fitness functions often measure mean squared error, classification accuracy, or accumulated reward, and penalize programs that raise runtime exceptions or exceed resource limits. Individuals with higher fitness are more likely to be selected for reproduction.

\paragraph{Example}

Suppose we have the following initial program trees:

\begin{center}
\begin{tabular}{c c}
Parent 1: & $f_1 = (x_1 \times x_3) + (x_1 + x_4)$ \\
Parent 2: & $f_2 = (x_2 - 5) \times (x_4 + 1)$
\end{tabular}
\end{center}

Suppose we exchange the right subtree of $f_1$ (the addition node $x_1 + x_4$) with the left subtree of $f_2$ (the subtraction node $x_2 - 5$). The resulting offspring are
\[
f'_1 = (x_1 \times x_3) + (x_2 - 5), \qquad
f'_2 = (x_1 + x_4) \times (x_4 + 1).
\]
Mutation might then replace the terminal $x_4$ in $f'_1$ with a constant (e.g., $5$) or switch the addition operator to multiplication, thereby exploring nearby program structures while keeping the tree depth bounded.

\paragraph{Recursive and Modular Programs}

GP can evolve recursive functions and modular code blocks (subroutines), enabling the discovery of complex behaviors and algorithms. In practice this is achieved by allowing trees to reference automatically defined functions (ADFs) or macros that are evolved alongside the main program. The depth of the program trees and the number of reusable modules are usually constrained to prevent uncontrolled growth and to keep execution cost manageable.

\paragraph{Applications}

Genetic programming is particularly useful for:

\begin{itemize}
    \item Symbolic regression: discovering analytical expressions fitting data.
    \item Automated program synthesis: generating code for control, decision-making, or data processing.
    \item Robotics: evolving control programs for navigation, obstacle avoidance, or manipulation.
\end{itemize}

\paragraph{Example: Robot Obstacle Avoidance}

Consider evolving a program that controls a robot's movement based on sensor inputs indicating obstacles. The function set might include commands like \texttt{move\_forward}, \texttt{turn\_left}, \texttt{turn\_right}, and conditional statements. The GP evolves sequences and combinations of these commands to maximize the robot's ability to navigate without collisions.

\paragraph{Summary}

Genetic programming generalizes genetic algorithms by evolving program structures (often trees) rather than fixed-length chromosomes.

\subsection{Wrapping Up Genetic Algorithms and Genetic Programming}
\label{sec:evo_wrapping_up_genetic_algorithms_and_genetic_programming}

In this final segment of the chapter, we conclude our discussion on genetic algorithms (GAs) and genetic programming (GP), emphasizing their conceptual foundations, practical implications, and the distinctions between them.

\paragraph{Recap of Genetic Algorithms}

Genetic algorithms are population-based metaheuristics; the earlier sections already define selection, crossover, mutation, and the loop in detail. For recap, keep the design levers in view and revisit the worked traces when needed:
\begin{itemize}
    \item \textbf{Representation:} How solutions are encoded (bit strings, real vectors, trees) and what constraints must be preserved.
    \item \textbf{Fitness and evaluation:} What you reward, how you handle noisy measurements, and how constraints enter the score.
    \item \textbf{Operator/loop settings:} Selection pressure, crossover/mutation rates, and stopping criteria.
\end{itemize}
See \Cref{sec:evo_summary_of_genetic_algorithm_workflow} for control flow, \Cref{tab:ga-toy} for a numeric generation trace, and \Crefrange{sec:evo_selection_in_genetic_algorithms}{sec:evo_mutation_operator} for operator details.

\paragraph{Genetic Programming: Structure over Parameters}

Genetic programming extends the GA paradigm by evolving computer programs or symbolic expressions rather than fixed-length parameter vectors. The fundamental difference is that GP searches over the space of program structures (trees of functions and terminals) instead of numeric parameter values.

Key points about GP include:

\begin{itemize}
    \item \textbf{Representation:} Programs are represented as hierarchical trees, where internal nodes are functions (e.g., arithmetic operators, logical functions) and leaves are terminals (input variables, constants).
    \item \textbf{Evolution of Programs:} Genetic operators manipulate program trees:
    \begin{itemize}
        \item \emph{Crossover} exchanges subtrees between parent programs.
        \item \emph{Mutation} randomly modifies nodes or subtrees.
    \end{itemize}
    \item \textbf{Fitness Evaluation:} Programs are executed on input data, and their outputs are compared against desired outputs to compute fitness.
    \item \textbf{Emergent Solutions:} GP can discover novel program structures that model complex phenomena without explicit programming, often yielding surprising and insightful results.
\end{itemize}

\paragraph{Applications and Insights}

Genetic programming is particularly powerful for modeling complex systems where the underlying relationships are unknown or difficult to specify explicitly. For example, given inputs such as wind speed, humidity, and temperature, GP can evolve models that predict environmental phenomena without prior assumptions about the functional form.

This capability highlights the strength of GP as a tool for automated model discovery and symbolic regression.

\paragraph{Further Topics and Extensions}

While this chapter provided a concise overview, the field of evolutionary computation encompasses many advanced topics, including:

\begin{itemize}
    \item \textbf{Multi-objective Genetic Algorithms:} Handling optimization problems with multiple conflicting objectives.
    \item \textbf{Constraint Handling:} Incorporating problem-specific constraints into the evolutionary process.
    \item \textbf{Hybrid Methods:} Combining GAs/GP with other optimization or machine learning techniques.
    \item \textbf{Scalability and Parallelization:} Efficiently implementing evolutionary algorithms for large-scale problems.
\end{itemize}

Readers are encouraged to explore these topics through further reading and research; the short primer below highlights the most widely used multi-objective GA.

\subsection{Multi-objective search and NSGA-II}
\label{sec:evo_multi_objective_search_and_nsga_ii}
When two or more objectives conflict, we seek a set of Pareto-optimal solutions rather than a single best point. The Non-dominated Sorting Genetic Algorithm II (NSGA-II) sorts each generation into Pareto fronts: rank-1 individuals are non-dominated, rank-2 are dominated only by rank-1, etc. Replacement preserves all members of the best fronts and uses crowding distance to maintain diversity along the trade-off curve. NSGA-II's combination of elitist survival and \(O(N \log N)\) non-dominated sorting makes it the default baseline for multi-objective evolutionary optimization \citep{Deb2002}.
\paragraph{Metrics and variants} Hypervolume (area/volume dominated by the front with respect to a reference point) is a common scalar indicator; report it alongside the spread of solutions \citep{Zitzler2002}. MOEA/D decomposes objectives into weighted subproblems; SPEA2/IBEA are popular alternatives. Always plot the Pareto set and budget-matched hypervolume traces when comparing algorithms.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.58\linewidth]{lec19_pareto}
\caption{Sample Pareto front for two objectives. NSGA-II keeps all non-dominated points (blue) while pushing dominated solutions (orange) toward the front via selection, yielding a spread of trade-offs in one run. Use this when interpreting multi-objective results as trade-offs, not a single optimum.}
\label{fig:lec11-pareto}
\end{figure}

\Cref{fig:lec11-pareto} is the trade-off view used to interpret multi-objective runs.

\clearpage

\begin{tcolorbox}[summarybox, title={Key takeaways}]
\begin{itemize}
    \item Evolutionary algorithms optimize without gradients by iterating selection, variation, and replacement under a fitness evaluation loop.
    \item Constraint handling is part of the design: penalties, repair, and feasibility-first selection each encode a different notion of ``acceptable'' search.
    \item Multi-objective search replaces a single optimum with a Pareto front; NSGA-II is a standard baseline for producing diverse trade-offs.
\end{itemize}

\medskip
\noindent\textbf{Minimum viable mastery.}
\begin{itemize}
    \item Specify genotype, variation operators, selection scheme, and termination criteria in a way that is reproducible.
    \item Distinguish constraint handling strategies and justify the one used (penalty vs.\ repair vs.\ feasibility rules).
    \item Report multi-objective results as trade-offs (Pareto fronts) rather than as a single scalar score.
\end{itemize}

\noindent\textbf{Common pitfalls.}
\begin{itemize}
    \item Over-interpreting a single stochastic run (report multiple seeds and dispersion).
    \item Using aggressive selection and low mutation, collapsing diversity and getting stuck in deceptive basins.
    \item Comparing algorithms without matching evaluation budget (fitness calls), constraint handling, and stopping rules.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item Implement a simple GA for $f(x)=\cos(5\pi x)\exp(-x^2)$, experimenting with penalty vs.\ repair strategies for the $[0,0.5]$ constraint.
    \item Prototype a CMA-ES or Differential Evolution solver on a noisy Rosenbrock function and compare convergence traces against the canonical GA.
    \item Build a tiny GP to rediscover a closed-form expression (e.g., $y = x^3 + x$) from samples; report how often crossover/mutation produce valid programs.
\end{itemize}

\medskip
\noindent\textbf{If you are skipping ahead.} This chapter is largely self-contained. If you revisit earlier model chapters, keep the common thread in mind: every method still requires a clear objective, a diagnostic that detects failure, and a reporting protocol that survives replication.
\end{tcolorbox}

\paragraph{Where we head next.} This chapter closes the soft-computing thread. For targeted refreshers, \Cref{app:linear_systems} summarizes linear-systems prerequisites and \Cref{app:kernels} consolidates kernel-method context used earlier in the book.

\nocite{Holland1975, Koza1992, Goldberg1989, Deb2001Book, Mitchell1998}

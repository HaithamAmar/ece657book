% Chapter 9
\section{Introduction to Self-Organizing Networks and Unsupervised Learning}\label{chap:som}
\graphicspath{{assets/lec5/}}

\Cref{chap:rbf} kept nonlinearity close to linear algebra: once you fix a set of basis functions, you lift the inputs into a feature space and (often) solve a regularized least-squares problem for the output weights. Now we switch the question. Instead of ``how do I fit targets?'', we ask: when labels are missing, expensive, or not even the right interface, what structure is already present in the inputs?

This chapter opens the unsupervised neural thread with \emph{Self-Organizing Maps} (SOMs), also known as Kohonen maps. You learn a set of prototypes and place them on a grid so that, as training progresses, units that are neighbors on the grid tend to respond to similar inputs. \Cref{chap:hopfield} then studies Hopfield networks as an energy\hyp{}based associative memory model. Both operate without explicit targets, but they use that freedom differently: SOMs emphasize organization and visualization, while Hopfield emphasizes retrieval dynamics.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
\begin{itemize}
    \item Describe how competitive learning, cooperation, and annealing interact in SOM training.
    \item Monitor SOM quality via quantization error (QE), topographic error (TE), and interpret U-matrices (unified distance matrix plots).
    \item Connect SOMs to broader unsupervised techniques (clustering, dimensionality reduction) and know when to use each.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Competition plus cooperation: pick a winner, then let its neighbors learn too, so the map becomes both a clustering device and a visualization.
\end{tcolorbox}

\subsection{Overview of Self-Organizing Networks}
\label{sec:som_overview_of_self_organizing_networks}

Self-organizing networks aim to discover structure in input data without labels. The most prominent example is the \emph{Self-Organizing Map} (SOM), introduced by Teuvo Kohonen. SOMs are used for clustering and for building a visualization-friendly map of a high-dimensional dataset.

The idea is fairly intuitive. If the data have a meaningful notion of similarity in the original space, then repeated \emph{competition} (pick the best-matching unit) and \emph{cooperation} (update its neighbors as well) can organize a fixed 2D \emph{grid} of prototype vectors so that nearby grid locations tend to represent nearby regions of the data. The map is not a perfect geometric embedding; it is an engineered bias that often produces a useful, inspectable picture.

SOMs are trained without labeled outputs; the only signal is input similarity under your chosen distance. Learning is \emph{competitive} (each input picks a best\hyp{}matching unit) but also \emph{cooperative} (neighbors of the winner move too). That neighborhood coupling is the deliberate bias: it encourages nearby grid locations to represent similar inputs, so the map can be read both as a set of prototypes and as a visualization.

\begin{tcolorbox}[summarybox, title={Historical intuition: two sheets and topographic neighborhoods}]
A useful way to picture SOMs is as two coupled ``sheets'': an input space and a fixed 2D grid of units. Each input is connected (in principle) to the whole grid, but learning makes some regions respond strongly (excitation) while others respond weakly (inhibition). The payoff is a \emph{topographic} map: inputs that are far apart in the original space can end up near one another on the grid if they are statistically similar under the features the SOM has learned.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Author's note: tie SOMs back to clustering and dimensionality reduction}]
SOMs live in the same ecosystem as clustering and dimensionality reduction: they learn prototypes without labels and simultaneously organize those prototypes on a low-dimensional grid. Treat the update rules as a carefully annealed clustering algorithm whose output just happens to be arranged on a grid for interpretability.
\end{tcolorbox}

The neighborhood influence is usually controlled by a kernel (often Gaussian) whose amplitude decays with grid distance and shrinks as training progresses, so early updates promote global organization while later updates refine only the closest units. \Cref{fig:lec5-learning-rate} juxtaposes these two time scales: the left panel shows why coarse early steps help traverse the energy landscape quickly, while the right panel compares two decaying learning-rate schedules commonly used when training SOMs.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
            group style={group size=2 by 1, horizontal sep=1.2cm},
            width=0.45\linewidth, height=0.36\linewidth
        ]
        % Left: coarse-to-fine steps on a convex bowl
        \nextgroupplot[
            title={Coarse $\rightarrow$ fine steps on $f(x, y)$},
            axis lines=middle, xmin=-2.2, xmax=2.2, ymin=-2.2, ymax=2.2,
            xlabel={$x$}, ylabel={$y$}
        ]
            % Simple elliptical contours of a convex quadratic
            \addplot[very thin, gray!50, samples=200, domain=0:6.283] ({1.9*cos(x)}, {0.9*sin(x)});
            \addplot[very thin, gray!50, samples=200, domain=0:6.283] ({1.3*cos(x)}, {0.6*sin(x)});
            \addplot[very thin, gray!50, samples=200, domain=0:6.283] ({0.8*cos(x)}, {0.4*sin(x)});
            % Trajectory: large steps then small steps with markers
            \addplot[cbOrange, thick, mark=*, mark options={scale=0.7}]
                coordinates {(-1.8,1.6) (-0.8,0.2) (0.2,-0.05) (0.4,-0.02) (0.5,0)};
            \node[cbOrange] at (-0.6,0.5) {large steps};
            \node[cbOrange] at (0.6,-0.1) {small steps};

        % Right: learning-rate schedule
        \nextgroupplot[
            title={Decay of $\alpha(t)$}, xmin=0, xmax=50, ymin=0, ymax=0.35,
            xlabel={$t$}, ylabel={$\alpha$}
        ]
            \addplot[cbBlue, thick, samples=100, domain=0:50] {0.3*exp(-x/15)};
            \addplot[cbGreen, dashed, thick, samples=100, domain=0:50] {0.25*(0.85)^(x)};
            \legend{$\alpha(t)=0.3\, e^{-t/15}$,$\alpha(t)=0.25\cdot0.85^{t}$}
        \end{groupplot}
    \end{tikzpicture}
    \caption{Learning-rate scheduling intuition (schematic). On a smooth objective (left), large initial steps quickly cover ground and roughly align prototypes, while a decaying step-size refines the solution near convergence. Right: common exponential and multiplicative decays used in SOM training.}
    \label{fig:lec5-learning-rate}
\end{figure}


\Cref{fig:lec5-som-component-planes} pairs feature-plane views with U-Matrix diagnostics for SOM audits.

Before delving into the mathematical formulation and algorithmic details of SOMs, it is important to review two foundational concepts that underpin their operation: \emph{clustering} and \emph{dimensionality reduction}.

\subsection{Clustering: Identifying Similarities and Dissimilarities}
\label{sec:som_clustering_identifying_similarities_and_dissimilarities}

Clustering is the process of grouping a set of objects such that objects within the same group (cluster) are more similar to each other than to those in other groups. Formally, given a dataset $\mathcal{X} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\}$ where each $\mathbf{x}_i \in \mathbb{R}^d$ is represented by a feature vector, the goal is to partition the data into $K$ clusters $\{C_1, C_2, \ldots, C_K\}$ such that:
\begin{itemize}
    \item \textbf{Intra-cluster similarity} is maximized: points within the same cluster are close to each other.
    \item \textbf{Inter-cluster dissimilarity} is maximized: points in different clusters are far apart.
\end{itemize}
In the classical formulation used here (e.g., for K-means), the clusters form a partition of $\mathcal{X}$: they are disjoint and their union equals the entire dataset.

\paragraph{Example:} Think of clustering as ``discovering operating modes'' from measurements. Your \(\mathbf{x}_i\) could be a feature vector extracted from vibration spectra, network traffic, or sensor logs. Without labels, you still want the algorithm to separate one regime from another because the points inside a regime are genuinely similar in the feature space.

\paragraph{K-means Clustering:} A classical and widely used clustering algorithm is \emph{K-means}, which operates as follows:
\begin{enumerate}
    \item Initialize $K$ cluster centroids $\{\mathbf{m}_1, \mathbf{m}_2, \ldots, \mathbf{m}_K\}$ randomly.
    \item For each data point $\mathbf{x}_i$, assign it to the cluster with the nearest centroid:
    \begin{equation}
        c_i = \arg\min_{k} \|\mathbf{x}_i - \mathbf{m}_k\|_2,
        \label{eq:auto_som_a96d59060d}
    \end{equation}
    where $\|\cdot\|_2$ denotes the Euclidean norm.
    \item Update each centroid as the mean of all points assigned to it:
    \begin{equation}
        \mathbf{m}_k = \frac{1}{|C_k|} \sum_{\mathbf{x}_i \in C_k} \mathbf{x}_i,
        \label{eq:auto_som_5cd43ad14e}
    \end{equation}
    where $|C_k|$ is the number of points in cluster $C_k$.
    \item Repeat steps 2 and 3 until convergence (i.e., cluster assignments no longer change significantly).
\end{enumerate}

K-means is an unsupervised learning method because it does not require labeled data; it discovers clusters purely based on feature similarity.

Keep the K-means update in mind: SOMs reuse the same prototype-moving idea, but add a neighborhood on a fixed 2D grid so the prototypes are not only learned, but also \emph{organized} for inspection.

\subsection{Dimensionality Reduction: Simplifying High-Dimensional Data}
\label{sec:som_dimensionality_reduction_simplifying_high_dimensional_data}

Dimensionality reduction is what you do when the ambient dimension is too large to see and reason about directly. You accept some information loss, but you try to preserve the relationships you care about (variance, distances, neighborhoods) so the reduced view remains useful. This matters for:
\begin{itemize}
    \item \textbf{Visualization:} Humans can easily interpret data in two or three dimensions.
    \item \textbf{Computational efficiency:} Reducing dimensions can simplify subsequent processing.
    \item \textbf{Noise reduction:} Eliminating irrelevant or redundant features.
\end{itemize}

\paragraph{Example:} Consider a three-dimensional cube. Depending on its orientation, a linear projection (matrix multiplication by \(P: \mathbb{R}^3 \rightarrow \mathbb{R}^2\) with matrix representation in \(\mathbb{R}^{2 \times 3}\)) onto a two-dimensional plane can look like different shapes: a square arises from an orthogonal projection onto a face, whereas a hexagon appears under an oblique projection along a body-diagonal. This highlights that while the combinatorial adjacency (which vertices are connected) is preserved under such a projection, Euclidean lengths and angles are inevitably distorted. \Cref{fig:lec5-mds-projection} illustrates these two views.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
            group style={group size=2 by 1, horizontal sep=1.5cm},
            width=0.4\linewidth,
            height=0.32\linewidth,
            axis lines=middle,
            xmin=-1, xmax=1,
            ymin=-1, xmax=1,
            xtick={-1,0,1},
            ytick={-1,0,1},
            ticklabel style={font=\scriptsize},
            title style={font=\scriptsize}
        ]
        \nextgroupplot[title={Orthogonal projection}]
            \addplot[thick, cbBlue] coordinates {(-0.6,-0.6) (0.6,-0.6) (0.6,0.6) (-0.6,0.6) (-0.6,-0.6)};
            \addplot[only marks, mark=*, mark options={scale=0.6, fill=cbBlue}] coordinates {
                (-0.6,-0.6) (0.6,-0.6) (0.6,0.6) (-0.6,0.6)
            };
        \nextgroupplot[title={Oblique projection}]
            \addplot[thick, cbOrange] coordinates {
                (-0.7,0) (-0.3,0.55) (0.4,0.55) (0.7,0) (0.3,-0.55) (-0.4,-0.55) (-0.7,0)
            };
            \addplot[only marks, mark=*, mark options={scale=0.6, fill=cbOrange}] coordinates {
                (-0.7,0) (-0.3,0.55) (0.4,0.55) (0.7,0) (0.3,-0.55) (-0.4,-0.55)
            };
        \end{groupplot}
    \end{tikzpicture}
    \caption{Classical MDS intuition (schematic). Projecting a cube onto a plane via an orthogonal map yields a square (left), whereas an oblique projection along a body diagonal produces a hexagon (right). The local adjacency of vertices is preserved even though metric structure is distorted.}
    \label{fig:lec5-mds-projection}
\end{figure}


Common techniques include Principal Component Analysis (PCA), which preserves directions of maximum variance, and classical Multidimensional Scaling (MDS), which reconstructs a configuration whose pairwise distances match the original ones as closely as possible (via double-centering the squared-distance matrix and an eigen-decomposition). Nonlinear methods such as t\hyp{}distributed stochastic neighbor embedding (t\hyp{}SNE) or Uniform Manifold Approximation and Projection (UMAP) emphasize local neighborhoods but typically sacrifice global distance fidelity. SOMs will give us a different kind of reduction: a \emph{discrete} map built from learned prototypes on a 2D grid, not a continuous coordinate chart.

% Chapter 9 (continued)

\subsection{Dimensionality Reduction and Feature Mapping}
\label{sec:som_dimensionality_reduction_and_feature_mapping}

The dimensionality-reduction goals were covered in \Cref{sec:som_dimensionality_reduction_simplifying_high_dimensional_data}; here we make explicit what kind of ``mapping'' a SOM actually learns. A SOM does not return a continuous coordinate system like PCA or classical MDS. Instead, it learns a \emph{finite set} of prototype vectors \(\{\mathbf{w}_i\}\) arranged on a low-dimensional grid with fixed coordinates \(\{\mathbf{r}_i\}\). Each input \(\mathbf{x}\in\mathbb{R}^n\) is mapped to the grid by choosing its best matching unit (BMU),
\[
c(\mathbf{x})=\operatorname*{argmin}_i \|\mathbf{x}-\mathbf{w}_i\|_2^2,
\]
and then using \(\mathbf{r}_{c(\mathbf{x})}\) (or a neighborhood-smoothed variant) as the reduced representation. In that sense, the map \(f:\mathbb{R}^n \to \mathbb{R}^m\) is \emph{implicit}: it is realized by nearest-prototype assignment plus fixed grid coordinates.

This ``discrete embedding'' view is the reason SOMs are useful for visualization and clustering at the same time: nearby grid locations tend to represent nearby regions of the input space, but grid distance is only an \emph{approximate} proxy for geometry. Read the map primarily as neighborhood structure and qualitative ordering, not as a metric-preserving chart.

\begin{tcolorbox}[summarybox, title={How to read a SOM map as a feature representation}]
\begin{itemize}
    \item \textbf{Out-of-sample mapping:} a new point maps to its BMU index \(c(\mathbf{x})\) and grid coordinate \(\mathbf{r}_{c(\mathbf{x})}\).
    \item \textbf{What transfers from PCA/MDS:} you can still ask whether neighborhoods are preserved and whether the map is stable across runs.
    \item \textbf{What does not transfer:} do not treat grid distance as true Euclidean distance in data space; the representation is discrete and only approximately geometry-preserving.
\end{itemize}
\end{tcolorbox}

\subsection{Self-Organizing Maps (SOMs): Introduction}
\label{sec:som_self_organizing_maps_soms_introduction}

Self-Organizing Maps (SOMs), also known as Kohonen maps, sit at a practical intersection: they behave like a prototype-based clustering method, but the prototypes are arranged on a 2D grid so you can inspect how the dataset organizes itself. Unlike supervised neural networks, SOMs learn without explicit target outputs or labels. Instead, they organize a bank of prototype vectors so that nearby units on the grid tend to represent similar inputs.

\begin{tcolorbox}[summarybox, title={SOM at a glance}]
\textbf{What it learns:} Prototype vectors \(\mathbf{w}_i\) in input space, arranged on a fixed 2D grid so that neighboring units tend to represent neighboring regions of the data (topographic mapping).\\
\textbf{Knobs you actually tune:} Map size/topology, the learning-rate schedule \(\alpha(t)\), the neighborhood width \(\sigma(t)\) and its decay, and the distance metric (typically squared Euclidean).\\
\textbf{A practical starting point:} A 2D rectangular grid, squared Euclidean distance, exponential decays for \(\alpha(t)\) and \(\sigma(t)\), and a number of units comparable to (or slightly larger than) the number of clusters you expect to resolve.\\
\textbf{Common ways to fool yourself:} Using a map that is too small, shrinking \(\alpha(t)\) or \(\sigma(t)\) too quickly (the map freezes before it organizes), and reading grid distance as if it were a true metric in data space.
\end{tcolorbox}

\paragraph{Historical Context}

The concept of SOMs traces back to early models of self-organizing topographic maps, such as the two-sheet formulation of \citet{WillshawVonDerMalsburg1976}. Teuvo Kohonen later formalized and popularized the algorithmic framework in \citet{Kohonen1982} (see also \citealp{Kohonen2001}).

\paragraph{Basic Architecture}

Architecturally, a SOM pairs an input vector \(\mathbf{x}\in\mathbb{R}^n\) with a usually two-dimensional grid of units. Each unit \(i\) has (i) a fixed grid coordinate \(\mathbf{r}_i=[u_i,v_i]^\top\) with \(u_i,v_i\in\mathbb{Z}\), and (ii) a prototype (codebook) vector \(\mathbf{w}_i\in\mathbb{R}^n\). The coordinates \(\mathbf{r}_i\) define proximity \emph{on the grid} and enter the neighborhood function (\Cref{sec:som_neighborhood}); the prototypes \(\mathbf{w}_i\) are what you compare to data points when you pick the winner.

Each output neuron therefore possesses a weight vector of the same dimensionality as the input, so evaluating the match between an input and the map amounts to comparing the input against every stored prototype. The neurons then compete; the closest (best matching) unit "wins" and its neighbors are allowed to adapt by nudging their weight vectors toward the input, while distant units remain unchanged during that update. The resulting organization produces a discrete map that preserves qualitative ordering; it approximates the topology of the input space without providing a continuous Euclidean embedding.

\paragraph{Key Concept: Topographic Mapping}

The fundamental idea is simple: inputs that are similar in the original space should activate units that are close to each other on the map. In practice, that means if \(\mathbf{x}_1\) and \(\mathbf{x}_2\) are close in \(\mathbb{R}^n\), then their best matching units should end up near each other on the 2D grid. The neighborhood update is the mechanism that encourages this: when one unit wins, its neighbors are pulled in the same direction, so ``similar inputs'' repeatedly shape the same local region of the map.

If you want a compact formal statement, let \(\mathcal{N}_\epsilon(\mathbf{x})=\{\mathbf{z}\,|\,\|\mathbf{z}-\mathbf{x}\|_2<\epsilon\}\) be a small neighborhood in input space. SOM training aims (approximately) to map \(\mathcal{N}_\epsilon(\mathbf{x})\) into a small neighborhood around the BMU on the grid (see \citealp{Kohonen2001} for details and caveats). The guarantee is not exact; the engineering goal is a stable qualitative ordering that makes the map readable.

\subsection{Conceptual Description of SOM Operation}
\label{sec:som_conceptual_description_of_som_operation}

\begin{enumerate}
    \item \textbf{Initialization:} The weight vectors \(\mathbf{w}_i\) are initialized, often randomly or by sampling from the input space.

    \item \textbf{Competition:} For a given input \(\mathbf{x}\), find the best matching unit (BMU) or winning neuron:
    \begin{equation}
        c = \operatorname*{argmin}_{i} \|\mathbf{x} - \mathbf{w}_i\|_2^2, \label{eq:bmu}
    \end{equation}
    that is, the BMU index \(c\) minimizes the squared Euclidean distance between \(\mathbf{x}\) and the candidate prototype \(\mathbf{w}_i\).
    Minimizing the squared distance yields the same winner as minimizing the unsquared norm but streamlines gradient derivations, so we retain the squared form for consistency with later update rules.

    \item \textbf{Cooperation:} Define a neighborhood function \(h_{ci}(t)\) that determines the degree of influence the BMU has on its neighbors in the output grid. This function decreases with the distance between neurons \(c\) and \(i\) on the map and with time \(t\).

    \item \textbf{Adaptation:} Update the weight vectors of the BMU and its neighbors to move closer to the input vector:
    \begin{equation}
        \mathbf{w}_i(t+1) = \mathbf{w}_i(t) + \alpha(t) h_{ci}(t) \big(\mathbf{x} - \mathbf{w}_i(t)\big), \label{eq:som_update}
    \end{equation}
    where \(\alpha(t)\) is the learning rate, which decreases over time, and the effective width of \(h_{ci}(t)\) likewise shrinks so that large-scale ordering occurs early and fine-tuning occurs later (see \Cref{sec:som_neighborhood}).
\end{enumerate}

\begin{tcolorbox}[summarybox, title={Author's note: distance is part of the model}]
The BMU rule is only as sensible as your distance. Euclidean distance is the default because it makes the update rule simple, but it assumes features are commensurate. In practice you almost always normalize inputs (and sometimes whiten them) so that one coordinate does not dominate the match by sheer scale. If your notion of ``similar'' is directional rather than magnitude-based, cosine distance is often a better choice; if covariance is strongly anisotropic, a Mahalanobis distance can be justified.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Tiny numeric step (online update)}]
Input \(\mathbf{x}=[0.2,0.8]\), two map units with weights \(\mathbf{w}_1=[0.1,0.9]\), \(\mathbf{w}_2=[0.7,0.3]\), coordinates \(\mathbf{r}_1=[0,0]\), \(\mathbf{r}_2=[1,0]\), \(\alpha=0.5\), \(\sigma=1\). BMU \(c=1\) (closest to \(\mathbf{x}\)). Neighborhoods: \(h_{11}=1\), \(h_{21}=\exp(-1/2)\approx 0.607\). Updates:
\[
\mathbf{w}_1\leftarrow [0.15,\,0.85],\quad
\mathbf{w}_2\leftarrow [0.548,\,0.452].
\]
Even the neighbor moves toward \(\mathbf{x}\), illustrating cooperation.
\end{tcolorbox}

This iterative process causes the map to self-organize, with neurons specializing to represent clusters or features of the input space.

\subsection{Mathematical Formulation of SOM}
\label{sec:som_mathematical_formulation_of_som}

Let the input space be \(\mathcal{X} \subseteq \mathbb{R}^n\), and the output map be a grid of neurons indexed by \(i\), each with weight vector \(\mathbf{w}_i \in \mathbb{R}^n\).

\paragraph{Best Matching Unit (BMU)}

We reuse the BMU definition in \eqref{eq:bmu}; the same squared-distance criterion carries into the formal derivation here.

\paragraph{Neighborhood Function}

A common choice for the neighborhood kernel is the Gaussian function
\begin{equation}
    h_{ci}(t) = \exp\left(-\frac{\| \mathbf{r}_c - \mathbf{r}_i \|^2}{2\sigma^2(t)}\right),
    \label{eq:gaussian_neighborhood_short}
\end{equation}
where $\mathbf{r}_i$ denotes the grid coordinates of neuron $i$ and $\sigma(t)$ is the neighborhood radius that decreases monotonically with $t$. A common schedule is an exponential decay:
\begin{equation}
    \sigma(k) = \sigma_0 e^{-k/\tau},
    \label{eq:som_sigma_schedule}
\end{equation}
where $k$ counts updates and $\tau$ sets how quickly the neighborhood shrinks. Early in training $\sigma(t)$ is large, encouraging broad cooperation; as $\sigma(t)$ shrinks, only neurons near the BMU continue to adapt (\Cref{fig:lec5-gaussian-neighborhood}).
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.65\linewidth,
            height=5cm,
            xmin=0, xmax=4,
            ymin=0, ymax=1.05,
            xlabel={Grid distance $\|\mathbf{r}_c-\mathbf{r}_i\|_2$},
            ylabel={$h_{ci}(t)$},
            legend style={at={(0.02,0.98)}, anchor=north west, draw=none, fill=none}
        ]
            \addplot[cbBlue, thick, domain=0:4, samples=200]{exp(-0.5*(x/1.5)^2)};
            \addlegendentry{Early $\sigma(t)$ (broad)}
            \addplot[cbGreen, thick, dashed, domain=0:4, samples=200]{exp(-0.5*(x/0.7)^2)};
            \addlegendentry{Late $\sigma(t)$ (narrow)}
        \end{axis}
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Gaussian neighborhood weights in SOM training (schematic). Early iterations use a broad kernel so many neighbors adapt; later iterations shrink the neighborhood width \(\sigma(t)\) so only units near the BMU update.}
    \label{fig:lec5-gaussian-neighborhood}
\end{figure}


% Chapter 9 (continued)

\subsection{Kohonen Self-Organizing Maps (SOMs): Network Architecture and Operation}
\label{sec:som_kohonen_self_organizing_maps_soms_network_architecture_and_operation}

So far we have described SOMs as an algorithmic loop: find the BMU, update the BMU and its neighbors, and anneal the step size and neighborhood width over time. If you prefer to think in terms of a network diagram, this subsection restates the same story as a concrete architecture you can trace and implement.

\paragraph{Network Structure}

The map is a fixed 2D grid of units. Each unit \(i\) stores a prototype vector \(\mathbf{w}_i\) in input space and has a fixed grid coordinate \(\mathbf{r}_i\). Every input is compared against every stored prototype.

\paragraph{Mapping and Competition}

For a given input \(\mathbf{x}\), the units compete by similarity (or distance) between \(\mathbf{x}\) and \(\mathbf{w}_i\). The closest unit wins; this is the BMU rule in \eqref{eq:bmu}.

\paragraph{Weight Update Rule}

Only the winning unit and its neighbors on the grid update; the update rule is \eqref{eq:som_update}. The important qualitative effect is that one input does not just move one prototype; it moves a small \emph{patch} of prototypes in the same direction, which is what makes the grid organize into a readable map rather than a bag of unrelated cluster centers.

\subsection{Example: SOM with a \texorpdfstring{\(3 \times 3\)}{3x3} Output Map and 4-Dimensional Input}
\label{sec:som_example_som_with_a_3_3_3x3_output_map_and_4_dimensional_input}

To make the symbols concrete, consider a SOM whose inputs live in \(\mathbb{R}^4\) and whose output map is a \(3\times 3\) grid. Each unit \(i\in\{1,\dots,9\}\) stores a prototype vector \(\mathbf{w}_i\in\mathbb{R}^4\). For a single input \(\mathbf{x}=[x_1,x_2,x_3,x_4]^\top\), the job of the map is to (i) decide which prototype matches best, then (ii) move that prototype \emph{and a small neighborhood around it} toward \(\mathbf{x}\).

\paragraph{Feedforward Computation}

For a given input \(\mathbf{x}\), each neuron computes a similarity score. Two common choices are:
\begin{align}
    y_i &= \mathbf{w}_i^\top \mathbf{x} && \text{(dot-product similarity)}, \label{eq:neuron_activation}\\
    d_i &= \|\mathbf{x} - \mathbf{w}_i\|_2^2 && \text{(squared Euclidean distance)}. \label{eq:neuron_distance}
\end{align}
In both expressions \(\mathbf{w}_i\) and \(\mathbf{x}\) are column vectors, so \( \mathbf{w}_i^\top \mathbf{x}\) is a scalar similarity score while \(d_i\) computes the squared Euclidean distance.

When using dot products we select the neuron with the maximum \(y_i\); when using distances we equivalently select the neuron with the minimum \(d_i\) (or the maximum of \(-d_i\)):
\[
c =
\begin{cases}
\arg \max_i y_i, & \text{if similarities are measured via } \eqref{eq:neuron_activation},\\
\arg \min_i d_i, & \text{if distances are used as in } \eqref{eq:neuron_distance}.
\end{cases}
\]
In practice the distance form is the most common in SOM code. If you do use dot-product similarity, normalize inputs (and often prototypes) so that ``largest dot product'' corresponds to your notion of ``closest.''

\paragraph{Weight Initialization and Update}

Weights \(\mathbf{w}_i\) are typically initialized randomly (often by sampling from the data) or by a PCA-style seeding if you want a stable orientation. One training step then has a predictable shape: compute the match scores (usually \(d_i\)), pick the BMU \(c\), compute neighborhood weights from the grid distances \(\|\mathbf{r}_i-\mathbf{r}_c\|\), and apply the update in \eqref{eq:som_update} to the BMU and a small neighborhood patch. For a fully worked numeric update with actual numbers, see the earlier \emph{Tiny numeric step (online update)} box; the point of the present \(3\times 3\) example is to keep the dimensions explicit so you can track what gets computed where.

This process repeats over many inputs, gradually organizing the map such that neighboring neurons respond to similar inputs, effectively performing a topology-preserving dimensionality reduction.

The grid coordinates \(\mathbf{r}_i \in \mathbb{Z}^2\) introduced for the neighborhood kernel serve as the geometry of the output map; distances such as \(\|\mathbf{r}_i - \mathbf{r}_c\|_2\) determine how strongly each neuron responds when \(c\) wins. Broad kernels (large \(\sigma(t)\)) encourage global ordering early in training, whereas shrinking \(\sigma(t)\) confines adaptation to local neighborhoods so that fine-grained structure emerges. Alternative kernel shapes (e.g., Epanechnikov, bubble) can be used, though Gaussians provide smooth decay and convenient derivatives.

SOM training is typically stochastic: each input triggers an update, so the map continuously refines prototypes as data arrive. Batch variants exist, but online updates capture streaming data and mirror Kohonen's original algorithm.

\subsection{Key Properties of Kohonen SOMs}
\label{sec:som_key_properties_of_kohonen_soms}

\begin{itemize}
    \item \textbf{Fixed output dimension:} The grid size is a design choice specified a priori and does not automatically scale with the input dimension.
    \item \textbf{Winner-takes-all competition:} Only the best matching unit and its neighbors adapt their weights, encouraging topological ordering.
    \item \textbf{Neighborhood cooperation:} Updating neighboring neurons enforces smooth transitions across the map.
\end{itemize}
% Chapter 9 (continued)

\subsection{Winner-Takes-All Learning and Weight Update Rules}
\label{sec:som_winner_takes_all_learning_and_weight_update_rules}

Recall that in competitive learning networks, the neuron with the highest discriminant value for a given input \(\mathbf{x}\) is declared the \emph{winner}. This subsection analyzes the classical \emph{winner-takes-all} (WTA) principle in which only the winning neuron updates its weights, while all others remain unchanged. In the SOM setting discussed earlier, a softened variant is used in which the winner and its grid neighbors update together.

\paragraph{Discriminant Function and Similarity Measures}

The discriminant measures in \eqref{eq:neuron_activation} (dot-product similarity) and \eqref{eq:neuron_distance} (squared Euclidean distance) are the same ones used in the earlier example; we reuse them here, favoring the distance form when deriving updates.

\paragraph{Weight Update Rule}

Once the winning neuron \(c\) is identified, WTA is the SOM update \eqref{eq:som_update} with a collapsed neighborhood: set \(h_{ci}(t)=1\) for \(i=c\) and \(h_{ci}(t)=0\) otherwise. The learning rate \(\alpha(t)\) still controls step size so the winner moves toward \(\mathbf{x}\) gradually rather than collapsing to it in a single update.

\paragraph{Learning Rate Schedule}

The learning rate \(\alpha(t)\) controls the magnitude of weight updates. It typically decreases over time to ensure convergence and stability:

\[
\alpha(t+1) \leq \alpha(t), \quad \lim_{t \to \infty} \alpha(t) = 0.
\]

This schedule allows large adjustments early in training (rapid learning) and fine-tuning later (stabilization).
Practitioners often start with \(\alpha(0)\) in the range \(0.05\)--\(0.5\) and decay it toward \(10^{-3}\) or smaller so that updates remain responsive initially but become conservative as the map stabilizes.

\paragraph{Summary of the Competitive Learning Algorithm}

\begin{enumerate}
    \item Initialize weights \(\mathbf{w}_j(0)\) randomly or heuristically.
    \item For each input \(\mathbf{x}\):
    \begin{enumerate}
        \item Compute discriminant functions \(g_j(\mathbf{x})\) or distances \(d_j(\mathbf{x})\).
        \item Select winning neuron:
        \[
        c = \arg \max_j g_j(\mathbf{x}) \quad \text{or} \quad c = \arg \min_j d_j(\mathbf{x})
        \]
        \item Update the winning neuron's weights using \eqref{eq:som_update} with \(h_{ci}(t)\) collapsed to a winner-only update.
    \end{enumerate}
    \item Decrease learning rate \(\alpha(t)\) according to schedule.
    \item Repeat until convergence or maximum iterations reached.
\end{enumerate}

\subsection{Numerical Example of Competitive Learning}
\label{sec:som_numerical_example_of_competitive_learning}

Consider a simple example with:

\begin{itemize}
    \item Four input vectors \(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3, \mathbf{x}_4 \in \mathbb{R}^4\).
    \item A competitive layer with three neurons (clusters).
    \item Initial learning rate \(\alpha(0) = 0.3\) with multiplicative decay \(\alpha(t) = 0.3 \times 0.5^{t}\) (ensuring \(\alpha(t) > 0\)).
    \item No neighborhood function (i.e., only the winner updates).
\end{itemize}

\paragraph{Initial Weights}

The initial weights \(\mathbf{w}_j(0)\) for neurons \(j=1,2,3\) are:

\[
\mathbf{W}(0) =
\begin{bmatrix}
0.2 & 0.3 & 0.5 & 0.1 \\
0.2 & 0.3 & 0.1 & 0.4 \\
0.3 & 0.5 & 0.2 & 0.3
\end{bmatrix}
\]

where row \(j\) contains the initial weight vector \(\mathbf{w}_j(0)\) for neuron \(j = 1,2,3\).

\paragraph{Input vectors}
Let the four inputs be
\[
\mathbf{x}_1=
\begin{bmatrix}
0.1\\ 0.3\\ 0.4\\ 0.2
\end{bmatrix},\quad
\mathbf{x}_2=
\begin{bmatrix}
0.8\\ 0.7\\ 0.2\\ 0.1
\end{bmatrix},\quad
\mathbf{x}_3=
\begin{bmatrix}
0.2\\ 0.2\\ 0.1\\ 0.9
\end{bmatrix},\quad
\mathbf{x}_4=
\begin{bmatrix}
0.4\\ 0.6\\ 0.5\\ 0.3
\end{bmatrix}.
\]
We walk through one update explicitly for \(\mathbf{x}_1\); the remaining inputs follow the same steps.

\paragraph{Step 1: pick the winner for \(\mathbf{x}_1\).}
Using squared Euclidean distance \(d_j=\|\mathbf{x}_1-\mathbf{w}_j(0)\|_2^2\),
\begin{align*}
d_1 &= (0.2-0.1)^2 + (0.3-0.3)^2 + (0.5-0.4)^2 + (0.1-0.2)^2 = 0.03,\\
d_2 &= \|\mathbf{x}_1-\mathbf{w}_2(0)\|_2^2 = 0.14,\qquad
d_3 = \|\mathbf{x}_1-\mathbf{w}_3(0)\|_2^2 = 0.13.
\end{align*}
So the winner is \(c=\arg\min_j d_j = 1\).

\paragraph{Step 2: update the winner.}
At \(t=0\) the learning rate is \(\alpha(0)=0.3\), and only the winner updates:
\[
\mathbf{w}_1(1)=\mathbf{w}_1(0)+\alpha(0)\big(\mathbf{x}_1-\mathbf{w}_1(0)\big)
=
\begin{bmatrix}
0.17\\ 0.30\\ 0.47\\ 0.13
\end{bmatrix}.
\]
For the next input, \(t=1\) so \(\alpha(1)=0.3\times 0.5 = 0.15\), and the same computation repeats with the new winner.

% QC-BEGIN: som_competitive_learning_example
% d 0.03 0.14 0.13 winner 1
% w1_new 0.17 0.30 0.47 0.13
% alpha0 0.30 alpha1 0.15
% QC-END: som_competitive_learning_example

\subsection{Winner-Takes-All Learning Recap}
\label{sec:som_winner_takes_all_learning_recap}

The WTA selection and update were defined in \Cref{sec:som_winner_takes_all_learning_and_weight_update_rules}; the numerical example above uses the same BMU criterion \eqref{eq:bmu} and the winner-only form of the update in \eqref{eq:som_update}.

\paragraph{How WTA relates to SOMs.}
You can view WTA learning as a limiting case of SOM training where the neighborhood collapses to a single unit: \(h_{ci}(t)=\mathbf{1}[i=c]\). Adding a nontrivial neighborhood \(h_{ci}(t)\) is what turns pure prototype learning into a \emph{topographic} map: neighbors are encouraged to represent similar inputs, and the grid becomes a structured visualization rather than an unstructured codebook.

\paragraph{Practical considerations.}
In both SOMs and WTA networks, input vectors are commonly normalized (e.g., zero mean and unit variance) so that distance comparisons are meaningful. Training is typically terminated when weight changes fall below a small threshold or after a prescribed number of epochs.

\subsection{Regularization and Monitoring During SOM Training}
\label{sec:som_regularization_and_monitoring_during_som_training}

Even though SOMs are inherently unsupervised, their training dynamics still benefit from the same regularization heuristics used in supervised settings. Two complementary diagnostics are especially useful in practice.

\paragraph{Bias--variance view.} Increasing the grid resolution or keeping the kernel width large for too long can overfit local noise. \Cref{fig:lec5-bias-variance} visualizes the familiar \(U\)-shaped trade-off: the left regime underfits (high bias), whereas the right regime yields jagged maps (high variance).

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.82\linewidth,
            height=5cm,
            xlabel={Model capacity},
            ylabel={Error},
            xmin=0, xmax=1,
            ymin=0, ymax=0.9,
            legend style={at={(0.5,1.04)}, anchor=south, legend columns=3}
        ]
            \addplot[cbBlue, thick, domain=0:1, samples=200]{0.55*exp(-4*x) + 0.08};
            \addlegendentry{Bias}
            \addplot[cbOrange, thick, domain=0:1, samples=200]{0.12 + 0.65*x^1.8};
            \addlegendentry{Variance}
            \addplot[cbGreen, thick, domain=0:1, samples=200]{0.55*exp(-4*x) + 0.65*x^1.8 + 0.08};
            \addlegendentry{Total error}
            \addplot[cbPink, dashed, domain=0:1, samples=2]{0.3};
            \node[anchor=west, font=\scriptsize, text=cbPink] at (axis cs:0.45,0.32){validation floor};
        \end{axis}
    \end{tikzpicture}
    \caption{Illustrative bias--variance trade-off when sweeping SOM capacity (number of units or kernel width). The optimum appears near the knee where bias and variance intersect.}
    \label{fig:lec5-bias-variance}
\end{figure}


\paragraph{Loss-landscape smoothing.} Adding small cooperative penalties (e.g., weight decay between neighbors) produces smoother loss contours and accelerates convergence, as sketched in \Cref{fig:lec5-regularization}. The penalty discourages neighboring prototypes from diverging and keeps the map topologically ordered.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
            group style={group size=2 by 1, horizontal sep=2.2cm},
            width=0.42\linewidth,
            height=4.8cm,
            xmin=-2, xmax=2,
            ymin=-2, ymax=2,
            xtick=\empty,
            ytick=\empty,
            xlabel={$w_1$},
            ylabel={$w_2$},
            xlabel style={at={(axis description cs:0.5,-0.08)}, anchor=north},
            ylabel style={at={(axis description cs:-0.06,0.5)}, anchor=south},
            title style={yshift=0.2em},
            view={35}{25},
            samples=35,
            samples y=35,
            ztick=\empty
        ]
            \nextgroupplot[title={Unregularized}]
                \ifdefined\HCode
                    \addplot3[surf, shader=flat, opacity=0.92, domain=-2:2, y domain=-2:2]{0.4*x^2 + 0.1*y^2 + 0.8*x*y};
                \else
                    \addplot3[surf, shader=interp, opacity=0.92, domain=-2:2, y domain=-2:2]{0.4*x^2 + 0.1*y^2 + 0.8*x*y};
                \fi
            \nextgroupplot[title={Neighbor-coupled}]
                \ifdefined\HCode
                    \addplot3[surf, shader=flat, opacity=0.92, domain=-2:2, y domain=-2:2]{0.25*x^2 + 0.25*y^2 + 0.2*(x-y)^2};
                \else
                    \addplot3[surf, shader=interp, opacity=0.92, domain=-2:2, y domain=-2:2]{0.25*x^2 + 0.25*y^2 + 0.2*(x-y)^2};
                \fi
        \end{groupplot}
    \end{tikzpicture}
    \caption{Regularization smooths an objective surface (schematic). Coupling neighboring prototypes (right) yields wider, flatter basins than the jagged unregularized landscape (left).}
    \label{fig:lec5-regularization}
\end{figure}


\paragraph{Quantization vs. information preservation.} Classical SOM optimizes a topology-preserving vector quantization objective; it does not include cross\hyp{}entropy terms. Modern variants sometimes introduce \emph{auxiliary} regularizers to encourage codebook utilization (e.g., entropy penalties on assignment histograms) or draw analogies to VQ\hyp{}VAE. Monitoring both quantization error and an entropy-style regularizer, as in \Cref{fig:lec5-crossentropy}, helps reveal when the map is collapsing to a few units or when density variations are no longer represented faithfully.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.75\linewidth,
            height=5.2cm,
            xlabel={Quantization error},
            ylabel={Entropy penalty},
            zlabel={Objective},
            view={40}{30},
            domain=0:1,
            y domain=0:1,
            samples=31,
            samples y=31,
            colormap/viridis
        ]
            \ifdefined\HCode
                \addplot3[surf, shader=flat, opacity=0.95]{0.6*(x-0.35)^2 + 0.4*(y-0.55)^2 + 0.25*x*(1-y)};
            \else
                \addplot3[surf, shader=interp, opacity=0.95]{0.6*(x-0.35)^2 + 0.4*(y-0.55)^2 + 0.25*x*(1-y)};
            \fi
        \end{axis}
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Illustrative objective surface combining quantization error and an entropy-style regularizer (a modern SOM variant; for example, a negative sum of \(p\log p\) over unit usage). Valleys arise when prototypes cover the space evenly; ridges highlight collapse or poor topological preservation.}
    \label{fig:lec5-crossentropy}
\end{figure}


\paragraph{Quantization vs. topographic error.} Given data points \(\{\mathbf{x}_i\}\) and best-matching units \(b_i = \operatorname{BMU}(\mathbf{x}_i)\), the \emph{quantization error} is
\[
    \text{QE} = \frac{1}{N}\sum_{i=1}^N \bigl\|\mathbf{x}_i - \mathbf{w}_{b_i}\bigr\|_2,
\]
which measures reconstruction fidelity. The \emph{topographic error} is the fraction of inputs whose first- and second-best BMUs are not adjacent on the grid (default: 4-neighbor connectivity), capturing topology preservation. Both metrics reappear in later figures; we monitor QE for representation quality and TE for magnification distortions.

\begin{tcolorbox}[summarybox, title={Tiny numeric check: QE vs.\ TE (they can disagree)}]
To see what these metrics measure, consider a \(2\times 2\) SOM grid with fixed coordinates
\(\mathbf{r}_1=[0,0]\), \(\mathbf{r}_2=[1,0]\), \(\mathbf{r}_3=[0,1]\), \(\mathbf{r}_4=[1,1]\) (4-neighbor adjacency). Let the prototypes in input space be
\[
\mathbf{w}_1=[0,0],\quad \mathbf{w}_2=[2,0],\quad \mathbf{w}_3=[0,2],\quad \mathbf{w}_4=[0.25,0.25].
\]
Notice that \(\mathbf{w}_4\) is close to \(\mathbf{w}_1\) in input space, even though units 4 and 1 are diagonal neighbors on the grid.

Now evaluate four inputs, using squared Euclidean distance to pick the best and second-best matches:
\[
\mathbf{x}_1=[0.30,0.30],\ \mathbf{x}_2=[1.80,0.20],\ \mathbf{x}_3=[0.20,1.70],\ \mathbf{x}_4=[0.05,0.05].
\]
The BMU/second-BMU pairs are \((4,1)\), \((2,4)\), \((3,4)\), \((1,4)\). Under 4-neighbor adjacency, pairs \((4,1)\) and \((1,4)\) are \emph{not} adjacent (diagonal), so \(\text{TE}=2/4=0.5\).
Meanwhile the average distance to the BMU is \(\text{QE}\approx 0.1962\).

The lesson is that QE is a ``prototype fit'' score, while TE is an ``ordering'' score. You can improve QE by moving prototypes toward the data while still tearing the map if nearby grid units do not represent nearby regions of the input space.
\end{tcolorbox}

% QC-BEGIN: som_qe_te_example
% w1 0.00 0.00
% w2 2.00 0.00
% w3 0.00 2.00
% w4 0.25 0.25
% x1 0.30 0.30 bmu 4 sbmu 1 adj 0
% x2 1.80 0.20 bmu 2 sbmu 4 adj 1
% x3 0.20 1.70 bmu 3 sbmu 4 adj 1
% x4 0.05 0.05 bmu 1 sbmu 4 adj 0
% QE 0.196205 TE 0.500000
% QC-END: som_qe_te_example

\begin{tcolorbox}[summarybox, title={Batch SOM in practice}, breakable]
Online SOM updates one sample at a time: pick a best-matching unit (BMU), nudge it and its neighbors, move on. Batch SOM instead aggregates responsibilities across a dataset (or mini\hyp{}batch) before shifting prototypes:
\begin{align*}
h_{j, i}(t) &= \kappa\big(\text{dist}(j, b(i)); \sigma_t\big),\\
\mathbf{w}_j^{(t+1)} &= \frac{\sum_i h_{j, i}(t)\,\mathbf{x}_i}{\sum_i h_{j, i}(t)}.
\end{align*}
Key differences:
\begin{itemize}
    \item \textbf{Deterministic passes.} Batch updates remove stochastic noise and converge in fewer epochs on static datasets, making results reproducible (useful for dashboards/visual analytics).
    \item \textbf{Parallelism.} Computations collapse to matrix ops (compute BMUs, accumulate weighted sums), so GPUs/CPUs can process large mini\hyp{}batches efficiently.
    \item \textbf{Streaming trade-off.} Online updates remain preferable when data arrive continuously or when you need the map to adapt mid-stream; batch SOM suits offline datasets.
\end{itemize}
Most modern SOM libraries expose both modes, so choose the update rule that matches your data pipeline and stability requirements.
\end{tcolorbox}

\paragraph{Stopping criteria.} Because stochastic updates can eventually increase topographic error, it is standard to stop training once a moving-average validation curve plateaus. \Cref{fig:lec5-early-stopping} shows the canonical trend: fast initial improvement followed by saturation.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.82\linewidth,
            height=5cm,
            xlabel={Epoch},
            ylabel={Error},
            xmin=0, xmax=60,
            ymin=0, ymax=1,
            legend style={at={(0.5,1.03)}, anchor=south, legend columns=2}
        ]
            \addplot[cbBlue, thick, smooth] table {
                epoch quant
                0 0.95
                5 0.78
                10 0.6
                15 0.48
                20 0.39
                25 0.33
                30 0.29
                35 0.27
                40 0.26
                45 0.26
                50 0.27
                55 0.29
                60 0.32
            };
            \addlegendentry{Quantization error}
            \addplot[cbOrange, thick, smooth, dashed] table {
                epoch topo
                0 0.9
                5 0.72
                10 0.55
                15 0.43
                20 0.35
                25 0.3
                30 0.27
                35 0.25
                40 0.245
                45 0.245
                50 0.25
                55 0.27
                60 0.3
            };
            \addlegendentry{Topographic error}
            \addplot[cbPink, very thick, domain=0:60]{0.25} node[pos=0.62, anchor=south west, font=\scriptsize, text=cbPink]{stop window};
        \end{axis}
    \end{tikzpicture}
    \caption{Illustrative validation curves used to identify an early\hyp{}stopping knee. When both quantization and topographic errors flatten (shaded band), further training risks map drift.}
    \label{fig:lec5-early-stopping}
\end{figure}


\subsection{Limitations of Winner-Takes-All and Motivation for Cooperation}
\label{sec:som_limitations_of_winner_takes_all_and_motivation_for_cooperation}

While WTA is simple and effective for clustering, it has some limitations:
\begin{itemize}
    \item Only one neuron updates per input, which can lead to slow convergence.
    \item The hard competition ignores relationships among neighboring neurons.
    \item The resulting clusters correspond to hard assignments, so boundaries between codebook vectors are sharp with little smoothing across neighboring neurons.
\end{itemize}
There is also a very practical failure mode: you can drive the prototypes toward the data (so QE improves) while the \emph{grid organization} never really forms. If you start with \(\sigma(t)\approx 1\) from the very beginning (or you drop neighborhood updates entirely), nearby grid locations can end up representing unrelated regions of the input space. The map looks like a shuffled deck: the U-Matrix becomes speckled, and TE stays stubbornly high because the second-best match for a point often lives far away on the grid. The fix is not a new objective; it is simply the training schedule: use a broad neighborhood early so large-scale ordering can settle, then shrink \(\sigma(t)\) so the map can refine without smearing away detail.
More importantly for SOMs, plain WTA gives you \emph{prototypes} but not necessarily a readable \emph{map}. If only the winner moves, nothing forces neighboring grid units to represent neighboring regions of the data; the prototypes can end up arranged arbitrarily across the grid, and small changes in initialization can produce very different-looking maps. Cooperation is the fix: early in training you use a broad neighborhood (large \(\sigma(t)\)) so an input shapes a local patch rather than a single unit, which encourages global ordering; later you shrink \(\sigma(t)\) so the map can refine without smearing away detail.
The geometric effect of these limitations is easiest to see in \Cref{fig:lec5-softmax-regions}: the left panel shows the brittle Voronoi partitions created by a strict winner-takes-all rule, whereas the right panel demonstrates how shrinking the neighborhood kernel produces softer responsibilities and smoother maps.

\pgfmathdeclarefunction{somindex}{2}{%
    \pgfmathsetmacro{\da}{(#1-0.2)^2 + (#2-0.2)^2}%
    \pgfmathsetmacro{\db}{(#1-0.8)^2 + (#2-0.25)^2}%
    \pgfmathsetmacro{\dc}{(#1-0.28)^2 + (#2-0.78)^2}%
    \pgfmathsetmacro{\dd}{(#1-0.78)^2 + (#2-0.78)^2}%
    \pgfmathparse{%
        (\da<=\db) && (\da<=\dc) && (\da<=\dd)? 0:
        ((\db<=\da) && (\db<=\dc) && (\db<=\dd)? 1:
        ((\dc<=\da) && (\dc<=\db) && (\dc<=\dd)? 2: 3))}%
}
\pgfmathdeclarefunction{somsoftpeak}{2}{%
    \pgfmathsetmacro{\beta}{8}%
    \pgfmathsetmacro{\ta}{exp(-\beta*((#1-0.2)^2 + (#2-0.2)^2))}%
    \pgfmathsetmacro{\tb}{exp(-\beta*((#1-0.8)^2 + (#2-0.25)^2))}%
    \pgfmathsetmacro{\tc}{exp(-\beta*((#1-0.28)^2 + (#2-0.78)^2))}%
    \pgfmathsetmacro{\td}{exp(-\beta*((#1-0.78)^2 + (#2-0.78)^2))}%
    \pgfmathsetmacro{\sumw}{\ta+\tb+\tc+\td}%
    \pgfmathsetmacro{\maxw}{max(max(\ta,\tb), max(\tc,\td))}%
    \pgfmathparse{\maxw/\sumw}%
}
\pgfplotsset{colormap={somregions}{
        color(0cm)=(cbBlue);
        color(0.33cm)=(cbOrange);
        color(0.66cm)=(cbGreen);
        color(1cm)=(cbPink)
}}
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
            group style={group size=2 by 1, horizontal sep=1.1cm},
            width=0.4\linewidth,
            height=0.42\linewidth,
            xmin=0, xmax=1,
            ymin=0, ymax=1,
            xlabel={$x_1$},
            ylabel={$x_2$},
            axis on top,
            enlargelimits=false
        ]
            \nextgroupplot[
                title={Hard BMU regions},
                view={0}{90},
                colorbar style={title={Prototype}, yshift=-2ex},
                zmin=0, zmax=3,
                colormap name=somregions,
                ticks=none
            ]
                \addplot3[surf, shader=flat, point meta=rawz,
                    domain=0:1, y domain=0:1, samples=45, samples y=45]
                    {somindex(x, y)};
                \addplot+[only marks, mark=*, mark size=1.8pt, color=cbBlue] coordinates {(0.2,0.2)};
                \addplot+[only marks, mark=*, mark size=1.8pt, color=cbOrange] coordinates {(0.8,0.25)};
                \addplot+[only marks, mark=*, mark size=1.8pt, color=cbGreen] coordinates {(0.28,0.78)};
                \addplot+[only marks, mark=*, mark size=1.8pt, color=cbPink] coordinates {(0.78,0.78)};
                \node[font=\scriptsize, anchor=west, cbBlue] at (axis cs:0.22,0.22){$\mathbf{w}_1$};
                \node[font=\scriptsize, anchor=west, cbOrange] at (axis cs:0.82,0.25){$\mathbf{w}_2$};
                \node[font=\scriptsize, anchor=south west, cbGreen] at (axis cs:0.3,0.82){$\mathbf{w}_3$};
                \node[font=\scriptsize, anchor=south east, cbPink] at (axis cs:0.78,0.82){$\mathbf{w}_4$};
            \nextgroupplot[
                title={Soft assignments},
                view={0}{90},
                colormap/viridis,
                colorbar style={title={Max softmax prob}, yshift=-2ex},
                ticks=none
            ]
                \addplot3[surf, shader=flat, point meta=rawz,
                    domain=0:1, y domain=0:1, samples=45, samples y=45]
                    {somsoftpeak(x, y)};
                \addplot+[only marks, mark=*, mark size=1.8pt, color=black, fill=white] coordinates {(0.2,0.2)(0.8,0.25)(0.28,0.78)(0.78,0.78)};
                \node[font=\scriptsize, anchor=south east, fill=white, inner sep=1pt] at (axis cs:0.98,0.05){$\sigma(t)\ \text{small}$};
            \end{groupplot}
    \end{tikzpicture}
    \caption{Voronoi-like regions induced by fixed prototypes in input space (left) and the corresponding soft assignments after sharpening the neighborhood kernel (right). Softer updates blur the decision frontiers and reduce jagged mappings between adjacent units (schematic illustration).}
    \label{fig:lec5-softmax-regions}
\end{figure}


To address these issues, the concept of \emph{cooperation} among neurons is introduced. Instead of a single winner neuron updating its weights, a neighborhood of neurons around the winner also update their weights, albeit to a lesser extent. This idea leads to smoother mappings and better topological ordering.

\subsection{Cooperation in Competitive Learning}
\label{sec:som_neighborhood}

\paragraph{Neighborhood Concept}

Consider the output layer arranged in a 2D grid of neurons. For each input \(\mathbf{x}\), after determining the winning neuron \(c\), we define a neighborhood \(\mathcal{N}(c)\) consisting of neurons close to \(c\) on the grid. In practice the neighborhood weight is supplied by the kernel \(h_{jc}(t)\) of \eqref{eq:gaussian_neighborhood_short}, which is positive for units inside the neighborhood (and decays with the grid distance \(\|\mathbf{r}_j - \mathbf{r}_c\|\)) and zero for units far away.

The neighborhood size typically shrinks over time during training, starting large to encourage global ordering and gradually reducing to fine-tune local details.

\paragraph{Weight Update with Neighborhood Cooperation}
The grid structure and how the best matching unit (BMU) influences nearby neurons are visualized in \Cref{fig:lec5-som-lattice-umatrix}. The U-Matrix on the right provides a quick diagnostic for cluster boundaries during training.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[scale=0.9]
        % Left panel: grid
        \begin{scope}
            % draw grid of neurons
            \foreach \i in {0,...,4} {
                \foreach \j in {0,...,4} {
                    \filldraw[fill=white, draw=gray!60] (\i,\j) circle (2.2pt);
                }
            }
            % BMU and neighbors
            \filldraw[fill=cbBlue, draw=cbBlue] (2,2) circle (2.4pt);
            \foreach \p in {(1,2),(3,2),(2,1),(2,3),(1,1),(1,3),(3,1),(3,3)} {
                \filldraw[fill=cbGreen!60, draw=cbGreen!60] \p circle (2.2pt);
            }
            \draw[cbBlue, thick] (2,2) circle (1.2);
            % lattice distance scale (one-step neighbor)
            \draw[<->, gray!70] (2,2.2) -- (3,2.2);
            \node[font=\scriptsize, gray!70] at (2.5,2.45) {$\|\mathbf{r}_j-\mathbf{r}_c\|=1$};
            \node at (2,-0.5) {$\mathbf{r}_c$};
            \node[align=center] at (2,-1.1) {SOM grid and BMU neighborhood};
        \end{scope}

        % Right panel: U-Matrix (toy heatmap; colored but grayscale-friendly)
        \begin{scope}[shift={(7,0)}]
            \begin{axis}[
                width=0.40\linewidth,
                height=0.40\linewidth,
                view={0}{90},
                xmin=-0.5, xmax=4.5,
                ymin=-0.5, ymax=4.5,
                xtick=\empty, ytick=\empty,
                axis lines=none,
                colormap/viridis,
                point meta min=0.30,
                point meta max=0.90,
                colorbar,
                colorbar style={
                    title={avg.\ distance},
                    ytick={0.30,0.60,0.90},
                    yticklabel style={font=\scriptsize},
                    title style={font=\scriptsize},
                },
                title={U-Matrix (neighbor distances)},
                title style={font=\scriptsize},
            ]
                \addplot[
                    matrix plot*,
                    mesh/cols=5,
                    mesh/rows=5,
                    point meta=explicit,
                ] table [meta=z] {
                    x y z
                    0 4 0.85
                    1 4 0.65
                    2 4 0.60
                    3 4 0.70
                    4 4 0.90
                    0 3 0.75
                    1 3 0.55
                    2 3 0.50
                    3 3 0.65
                    4 3 0.85
                    0 2 0.70
                    1 2 0.55
                    2 2 0.45
                    3 2 0.60
                    4 2 0.80
                    0 1 0.65
                    1 1 0.35
                    2 1 0.30
                    3 1 0.40
                    4 1 0.70
                    0 0 0.80
                    1 0 0.60
                    2 0 0.55
                    3 0 0.60
                    4 0 0.85
                };
            \end{axis}
        \end{scope}
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Left: a 5-by-5 SOM grid with the best matching unit (blue) and neighbors inside the Gaussian-kernel radius (green). Right: a toy U-Matrix (grayscale-safe colormap) showing average distances between neighboring codebook vectors; larger distances suggest likely cluster boundaries. Treat a U-Matrix as a qualitative boundary hint unless preprocessing and scaling are fixed.}
    \label{fig:lec5-som-lattice-umatrix}
\end{figure}

\begin{figure}[t]
    \centering
            \begin{tikzpicture}
                    \begin{groupplot}[
                        group style={group size=3 by 1, horizontal sep=0.55cm},
                        width=0.30\linewidth,
                        height=0.30\linewidth,
                        view={0}{90},
                        xmin=-0.5, xmax=4.5,
                        ymin=-0.5, ymax=4.5,
                        xtick=\empty, ytick=\empty,
                        colormap/viridis,
                        point meta min=0,
                        point meta max=0.7,
                        nodes near coords,
                        nodes near coords style={font=\scriptsize},
                        every node near coord/.append style={fill=white, fill opacity=0.8, text opacity=1, inner sep=0.6pt},
                        mesh/check=false
                    ]
                \nextgroupplot[title={Plane: pixel 1}]
                    \addplot[matrix plot*, mesh/cols=5, mesh/rows=5, point meta=explicit] table [meta=z] {
                        x y z
                        0 0 0.2
                    1 0 0.3
                    2 0 0.4
                    3 0 0.6
                    4 0 0.7
                    0 1 0.1
                    1 1 0.2
                    2 1 0.3
                    3 1 0.5
                    4 1 0.6
                    0 2 0.0
                    1 2 0.1
                    2 2 0.2
                    3 2 0.4
                    4 2 0.5
                    0 3 0.0
                    1 3 0.1
                    2 3 0.2
                    3 3 0.3
                    4 3 0.4
                    0 4 0.0
                    1 4 0.1
                    2 4 0.1
                    3 4 0.2
                        4 4 0.3
                    };
                \nextgroupplot[title={Plane: pixel 2}]
                    \addplot[matrix plot*, mesh/cols=5, mesh/rows=5, point meta=explicit] table [meta=z] {
                        x y z
                        0 0 0.7
                    1 0 0.5
                    2 0 0.4
                    3 0 0.3
                    4 0 0.2
                    0 1 0.6
                    1 1 0.4
                    2 1 0.3
                    3 1 0.2
                    4 1 0.1
                    0 2 0.5
                    1 2 0.3
                    2 2 0.2
                    3 2 0.1
                    4 2 0.0
                    0 3 0.4
                    1 3 0.2
                    2 3 0.1
                    3 3 0.0
                    4 3 0.0
                    0 4 0.3
                    1 4 0.1
                    2 4 0.0
                    3 4 0.0
                        4 4 0.0
                    };
                \nextgroupplot[
                        title={Plane: pixel 3},
                        colorbar,
                        colorbar style={
                            ytick={0,0.35,0.7},
                            yticklabel style={font=\scriptsize},
                        },
                    ]
                    \addplot[matrix plot*, mesh/cols=5, mesh/rows=5, point meta=explicit] table [meta=z] {
                        x y z
                        0 0 0.1
                    1 0 0.2
                    2 0 0.3
                    3 0 0.4
                    4 0 0.5
                    0 1 0.1
                    1 1 0.2
                    2 1 0.3
                    3 1 0.4
                    4 1 0.5
                    0 2 0.1
                    1 2 0.2
                    2 2 0.3
                    3 2 0.4
                    4 2 0.5
                    0 3 0.1
                    1 3 0.2
                    2 3 0.3
                    3 3 0.4
                    4 3 0.5
                    0 4 0.1
                    1 4 0.2
                    2 4 0.3
                    3 4 0.4
                    4 4 0.5
                };
        \end{groupplot}
    \end{tikzpicture}
\caption{Component planes for three features on a trained SOM (toy data). Each plane maps one feature across the grid; aligned bright/dark regions reveal correlated features and complement the U-Matrix in \Cref{fig:lec5-som-lattice-umatrix}. Interpret brightness comparatively within a plane rather than as an absolute scale.}
    \label{fig:lec5-som-component-planes}
\end{figure}

The weight update rule generalizes to:
\begin{align}
\mathbf{w}_j(t+1) = \mathbf{w}_j(t) + \alpha(t) \, h_{j c}(t) \left( \mathbf{x} - \mathbf{w}_j(t) \right),
\label{eq:coop_weight_update}
\end{align}
where
\begin{itemize}
    \item \(h_{j c}(t)\) is the \emph{neighborhood function} that quantifies the degree of cooperation between neuron \(j\) and the winner \(c\).
    \item \(\alpha(t)\) is the learning rate at time \(t\).
\end{itemize}

The neighborhood function satisfies:
\[
h_{j c}(t) = \begin{cases}
1, & j = c \\
\in (0,1), & j \in \mathcal{N}(c), j \neq c \\
0, & \text{otherwise}
\end{cases}
\]

\paragraph{Gaussian Neighborhood Function}

A common choice for \(h_{j c}(t)\) is a Gaussian function based on the distance between neurons \(j\) and \(c\) on the output grid:
\begin{align}
h_{j c}(t) = \exp \left( - \frac{\| \mathbf{r}_j - \mathbf{r}_{c} \|^2}{2 \sigma^2(t)} \right),
\label{eq:gaussian_neighborhood}
\end{align}
where
\begin{itemize}
    \item \(\mathbf{r}_j\) and \(\mathbf{r}_{c}\) are the coordinates of neurons \(j\) and \(c\) on the output grid.
    \item \(\sigma(t)\) is the neighborhood radius (width) at time \(t\), which decreases over training.
\end{itemize}

This function ensures that neurons closer to the winner receive larger updates, while distant neurons are updated less or not at all.

\paragraph{Interpretation}

The cooperative update encourages neighboring neurons to become sensitive to similar inputs, thereby preserving topological relationships in the input space. This is the key principle behind Self-Organizing Maps (SOMs).

\subsection{Example: Neighborhood Update Illustration}
\label{sec:som_example_neighborhood_update_illustration}

Suppose the output neurons are arranged in a 2D grid as shown schematically in \Cref{fig:som_neighborhood}, where each neuron is indexed by its grid coordinates. For an input \(\mathbf{x}\), neuron \(c\) wins. The neighborhood \(\mathcal{N}(c)\) might include neurons within a radius \(\sigma\) around \(c\).

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[scale=0.8]
        \foreach \x in {0,...,3} {
            \foreach \y in {0,...,3} {
                \node[draw, circle, minimum size=0.5cm, fill=gray!5] (n\x\y) at (\x,\y) {};
            }
        }
        \node[draw, circle, minimum size=0.55cm, fill=cbBlue!40] at (2,1) {};
        \draw[cbGreen, dashed, thick] (2,1) circle (1.5);
        \node at (2,1.5) {\scriptsize BMU $c$};
        \node[cbGreen!70!black] at (3.6,1) {\scriptsize radius $\sigma(t)$};
    \end{tikzpicture}
    \caption{SOM grid with the best-matching unit (BMU) highlighted in blue and a dashed neighborhood radius indicating which prototype vectors receive cooperative updates (schematic).}
    \label{fig:som_neighborhood}
\end{figure}


Each neuron \(j\) in \(\mathcal{N}(c)\) updates its weight vector according to \eqref{eq:coop_weight_update}, with the magnitude of update modulated by \(h_{j c}(t)\).

\subsection{Summary of Cooperative Competitive Learning Algorithm}
\label{sec:som_summary_of_cooperative_competitive_learning_algorithm}

\begin{enumerate}
    \item Present an input vector and identify the winning neuron using the discriminant function.
    \item Update the winning neuron's weights and those of its neighbors according to the cooperative rule.
    \item Decrease the learning rate and neighborhood radius according to the annealing schedule.
    \item Repeat for all inputs until the map stabilizes or a maximum number of epochs is reached.
\end{enumerate}
\subsection{Wrapping Up the Kohonen Self-Organizing Map (SOM) Derivations}
\label{sec:som_wrapping_up_the_kohonen_self_organizing_map_som_derivations}

At this point the SOM training story is complete: you have a similarity rule (to pick a winner), a neighborhood rule (to decide who else learns), and schedules that anneal both over time. The only subtlety is that these pieces interact. A large neighborhood early is what gives you global organization; a small neighborhood late is what lets prototypes settle into fine detail without tearing the map.

Recall the weight update rule for neuron \( j \) at time step \( t \):
\begin{align}
    \Delta \mathbf{w}_j(t) = \alpha(t) \, h_{j, c}(t) \left[ \mathbf{x}(t) - \mathbf{w}_j(t) \right].
    \label{eq:auto:lecture_5_part_i:2}
\end{align}
where:
\begin{itemize}
    \item \(\mathbf{x}(t)\) is the input vector at time \( t \).
    \item \(\mathbf{w}_j(t)\) is the weight vector of neuron \( j \) at time \( t \).
    \item \(c\) is the index of the winning neuron (best matching unit) for input \(\mathbf{x}(t)\).
    \item \(\alpha(t)\) is the learning rate, a monotonically decreasing function of time.
    \item \(h_{j, c}(t)\) is the neighborhood function centered on the winning neuron \( c \), also decreasing over time.
\end{itemize}

\paragraph{Neighborhood Function and Its Role}

The neighborhood function \( h_{j, c}(t) \) typically takes a Gaussian form:
\begin{align}
    h_{j, c}(t) = \exp\left(-\frac{\| \mathbf{r}_j - \mathbf{r}_{c} \|^2}{2\sigma^2(t)}\right).
    \label{eq:neighborhood_function}
\end{align}
where:
\begin{itemize}
    \item \(\mathbf{r}_j\) and \(\mathbf{r}_{c}\) are the positions of neurons \( j \) and \( c \) on the SOM grid.
    \item \(\sigma(t)\) is the neighborhood radius, which decreases over time.
\end{itemize}

This function ensures that neurons closer to the winning neuron receive larger updates, while those farther away receive smaller or zero updates. Initially, \(\sigma(t)\) is large, allowing broad neighborhood cooperation, but it shrinks as training progresses, focusing updates increasingly on the winning neuron itself.

\paragraph{Time-Dependent Parameters}

Both the learning rate \(\alpha(t)\) and neighborhood radius \(\sigma(t)\) decrease over time, typically following exponential decay laws:
\begin{align}
    \alpha(t) &= \alpha_0 \exp\left(-\frac{t}{\tau_\alpha}\right), \\
    \sigma(t) &= \sigma_0 \exp\left(-\frac{t}{\tau_\sigma}\right),
    \label{eq:decay_parameters}
\end{align}
where \(\alpha_0\) and \(\sigma_0\) are initial values, and \(\tau_\alpha, \tau_\sigma\) are time constants controlling the decay rates.

\paragraph{Summary of the Six Learning Steps}
\label{sec:som_training_steps}

One run of SOM training is repetitive in a good way: you keep presenting inputs, keep finding the BMU, and keep nudging a neighborhood patch until the map stops moving. The following pseudocode is the same loop written in a way you can implement directly.

\begin{tcolorbox}[summarybox, title={Self-Organizing Map (SOM) training pseudocode}]
\begin{enumerate}
    \item \textbf{Initialize} weight vectors \(\mathbf{w}_j(0)\) randomly or from samples.
    \item For iteration \(t=0,\ldots, T\):
    \begin{enumerate}
        \item Sample an input \(\mathbf{x}(t)\).
        \item Find the best matching unit (BMU) \(c = \arg\min_{j} \|\mathbf{x}(t)-\mathbf{w}_j(t)\|_2^2\).
        \item Compute neighborhood coefficients \(h_{j, c}(t)\).
        \item Update every neuron:
        \[
            \mathbf{w}_j(t+1) = \mathbf{w}_j(t) + \alpha(t)\, h_{j, c}(t)\Bigl(\mathbf{x}(t)-\mathbf{w}_j(t)\Bigr).
        \]
        \item Decay learning-rate \(\alpha(t)\) and neighborhood radius \(\sigma(t)\) (e.g., exponentially).
    \end{enumerate}
\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Batch SOM (deterministic pass)}]
\begin{enumerate}
    \item Fix centers \(\mathbf{r}_j\) on the grid; initialize \(\mathbf{w}_j\) (random data or along PCA directions).
    \item Repeat until convergence or max epochs:
    \begin{enumerate}
        \item Assign each \(\mathbf{x}_n\) to its BMU \(c_n\).
        \item For each neuron \(j\), update
        \[
        \mathbf{w}_j \leftarrow \frac{\sum_n h_{j, c_n}(t)\,\mathbf{x}_n}{\sum_n h_{j, c_n}(t)}.
        \]
        \item Decay \(\alpha(t),\sigma(t)\).
    \end{enumerate}
\end{enumerate}
Batch SOM (one pass per epoch) is deterministic given the assignments; it often stabilizes faster than purely online updates.
\end{tcolorbox}

\noindent I find it useful to tell the SOM story in \emph{three stages}, even though the code is often written as a longer checklist. The stages are: \textbf{Initialization} (seed the prototype grid), \textbf{Competition} (pick a best-matching unit for each input), and \textbf{Cooperation} (update a neighborhood and decay \(\alpha(t)\) and \(\sigma(t)\)). The six-step procedure in \Cref{sec:som_training_steps} is simply one way to operationalize those stages when you implement the algorithm and decide when to stop.

\subsection{Applications of Kohonen Self-Organizing Maps}
\label{sec:som_applications_of_kohonen_self_organizing_maps}

Kohonen SOMs show up when you want \emph{both} a prototype model and a picture you can reason about. In practice they tend to be used in three roles:

\begin{itemize}
    \item \textbf{Clustering / regime discovery:} Group similar points without supervision and summarize each region by a prototype vector (e.g., operating modes in telemetry features).
    \item \textbf{Exploratory reduction:} Map high-dimensional data onto a 2D grid index for visualization and exploratory analysis. You do not get a continuous coordinate system; you get a discrete map that is often easier to inspect.
    \item \textbf{Visualization diagnostics:} Use U-Matrices and component planes as ``instrument panels'' that reveal boundaries, smoothness, and which input features drive the organization.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Author's note: when a SOM is the wrong tool}]
If your only goal is to assign points to \(K\) groups and summarize each group by a center, k-means is usually the simpler baseline. If you want a continuous low-dimensional chart with a clear linear story, start with PCA; if you need a global distance-preserving embedding, MDS is a better conceptual match. If your goal is a local-neighborhood visualization and you accept distortion of global geometry, methods like UMAP/t-SNE are often more visually dramatic.

I reach for a SOM when I want three things at once: (i) prototypes I can inspect, (ii) a fixed 2D organization that is stable enough to compare across runs, and (iii) diagnostics (U-Matrix/component planes, QE/TE) that tell me whether the picture is trustworthy. If you cannot define a sensible distance (or you cannot normalize features so distance means something), a SOM will happily organize noise.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Application example: website sessions as behavioral modes}]
Consider a website where the system you are trying to understand is not a motor or a circuit, but a stream of user sessions. Each session becomes a data point \(\mathbf{x}\): dwell time, number of pages, scroll depth, time-to-first-interaction, device class, referrer type, returning vs.\ new, and a few content signals (e.g., the category of the first page visited, or an embedding of the landing-page text). You may not have labels for ``intent,'' but you do expect recurring modes: quick bounce, comparison shopping, documentation lookup, checkout flow, support troubleshooting, and so on.

A SOM is useful here because it gives you prototypes \(\mathbf{w}_i\) (representative session fingerprints) \emph{and} a 2D grid you can inspect. After training, each session maps to a best-matching unit (BMU). Dense patches of mapped sessions indicate common behavior; ridges in the U-Matrix suggest boundaries where neighboring prototypes are far apart, which often corresponds to genuinely different behaviors under your feature choices. Component planes then turn the picture into an explanation: you can see, for example, that one boundary is mostly driven by dwell time and scroll depth, while another is driven by referrer and device. In practice you read these plots as heat maps over the 2D grid: different behaviors show up as different regions you can name, compare, and inspect.

\textbf{Recommendation:} treat preprocessing as part of the model. Normalize continuous features, log-scale heavy-tailed counts, and be deliberate about how you represent categorical variables (one-hot, grouped buckets, or a small embedding) so Euclidean distance matches your notion of similarity.

\textbf{Audit hook:} rerun with a few seeds and a time-based split. If the map reorganizes completely across runs or across weeks, you are seeing instability or seasonality rather than durable structure.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Application example: mapping page archetypes and performance drivers}]
A second, complementary use is to map \emph{pages} (or queries) rather than sessions. Here each data point \(\mathbf{x}\) describes a URL or content item: a text embedding of the page, topic/category tags, layout signals (word count, number of images), and aggregate behavior metrics (click-through rate (CTR) from search, median dwell time, bounce rate, conversion rate, and the fraction of visits from mobile). You are effectively asking: which pages are similar in content and behavior, and where do the sharp breaks live?

A SOM turns this into a visual inventory. Pages with similar content and similar usage patterns compete for the same region of the grid, and the U-Matrix highlights boundaries between page families (e.g., ``documentation-like'' pages versus ``marketing-like'' pages). Component planes are the practical payoff: they show which variables line up with those boundaries. If one region lights up on ``mobile fraction'' and ``bounce rate,'' you immediately get a hypothesis about responsiveness or page speed. If another region is coherent in ``topic embedding'' but not in conversion, you have found a content family that is consistent but not effective.

\textbf{Recommendation:} separate what you want to \emph{organize by} (content similarity) from what you want to \emph{diagnose} (performance metrics). A simple approach is to train the SOM on content-heavy features, then overlay performance as color/slices; otherwise the map can end up organizing primarily by whatever has the largest numeric range.

\textbf{Audit hook:} overlay slices (language, device, traffic source). If the map's dominant structure is ``source domain'' or ``mobile vs.\ desktop,'' that may be a useful finding, but it is also a warning that your feature design is driving the story.
\end{tcolorbox}

\paragraph{Relation to k-means and modern variants}

When the neighborhood is collapsed to a single winner (no neighbors), the update reduces to a k-means-style prototype move without any notion of grid organization \citep{MacQueen1967}; SOMs therefore sit between pure clustering (k-means) and manifold learning, adding a topographic prior that encourages neighboring units to represent similar inputs. Recent ``neural-SOM'' hybrids embed SOM-like updates inside deep networks, but still rely on the same BMU search and neighborhood-weighted updates described above.

\begin{tcolorbox}[summarybox, title={Theory notes and recipes}]
\textbf{Convergence/magnification (theory lens):} With decays \(\alpha(t)\to 0\), \(\sigma(t)\to 0\) and \(\sum_t \alpha(t)=\infty\), \(\sum_t \alpha^2(t)<\infty\), SOM updates converge under mild assumptions \citep{ErwinObermayerSchulten1992,CottrellFort1986}. One qualitative takeaway is \emph{magnification}: dense regions of the data tend to attract more units of the map.\\
\textbf{A practical recipe (engineering lens):} If I have no prior, I treat these as starting points, not rules. Use \(5\sqrt{N}\)--\(10\sqrt{N}\) units when unsure; hex grids reduce anisotropy; wrap-around (toroidal) grids reduce edge effects. Initialize \(\mathbf{w}_j\) from data or along the first two PCs; pick \(\alpha_0\in[0.1,0.5]\) and decay toward \(\alpha_{\min}\approx 10^{-3}\); set \(\sigma_0\) to roughly the map radius and decay toward 1--1.5 cells.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Related and growing variants}]
\textbf{Neural Gas / Growing Neural Gas} \citep{MartinetzBerkovichSchulten1993,Fritzke1994GrowingNeuralGas} drop the fixed grid and instead learn neighborhood structure (and, in the growing variants, add units dynamically). \textbf{Generative topographic mapping (GTM)} (Bishop et al.) provides a probabilistic, topographic embedding with likelihoods/uncertainty. I like to remember the landscape this way: k-means gives prototypes with no organization; SOMs add a fixed organizing prior; these variants let the organization itself adapt.
\end{tcolorbox}

\paragraph{Complexity and out-of-sample mapping.} A full online epoch costs \(O(NM)\) (N data, M units); batch passes cost similar but fewer epochs. For large M, use approximate nearest-neighbor BMU search (k-d trees or vector-index libraries such as FAISS). New points map via their BMU; optional soft responsibilities use \(h_{ci}\) for smoothing.
\paragraph{Theory link to other chapters.} SOMs learn prototypes like the centers in \Cref{chap:rbf} but add a topographic prior; quality diagnostics (QE/TE) can be tracked with the validation-curve diagnostics in \Cref{chap:supervised}. For task-driven embeddings, see \Cref{chap:cnn,chap:nlp}; Hopfield (\Cref{chap:hopfield}) contrasts with energy-based associative recall.

\paragraph{Quality measures and magnification.} Two diagnostics are standard when reporting SOM quality:
\begin{itemize}
    \item \textbf{Quantization error (QE):} average Euclidean distance between each input and its BMU. Lower QE indicates prototypes that better represent the data manifold.
    \item \textbf{Topographic error (TE):} fraction of inputs whose first- and second-best BMUs are not adjacent, quantifying topology preservation (magnification factor).
\end{itemize}
Tracking both metrics reveals whether the neighborhood decay is too slow (over-smoothing) or too aggressive (tearing the topology). \Cref{chap:supervised}'s learning-curve plots suggest early\hyp{}stopping heuristics: stop when QE/TE on a validation split flatten.

\begin{tcolorbox}[summarybox, title={Key takeaways}]
\begin{itemize}
    \item SOMs perform topology-preserving vector quantization on a discrete grid.
    \item A shrinking neighborhood and decaying learning rate drive coarse-to-fine organization.
    \item U-Matrices and quantization/topographic errors are practical diagnostics for convergence.
\end{itemize}

\medskip
\noindent\textbf{Minimum viable mastery.}
\begin{itemize}
    \item Given \(x\), compute the BMU, write the weight update \(w_i \leftarrow w_i + \alpha(t) h_{ci}(t) (x-w_i)\), and explain how \(h_{ci}\) enforces cooperation.
    \item Interpret a U-Matrix and component planes as diagnostics for neighborhood structure and convergence.
    \item Choose sensible \(\alpha(t)\) and \(\sigma(t)\) schedules and justify them using QE/TE validation curves.
\end{itemize}

\noindent\textbf{Common pitfalls.}
\begin{itemize}
    \item Using a map that is too small or a neighborhood decay that is too aggressive (tearing the topology).
    \item Skipping input normalization, causing prototypes to track scale rather than structure.
    \item Treating grid distance as a true metric in data space (SOM topology is approximate, not exact geometry).
    \item Over-interpreting a single run without checking QE/TE plateaus and sensitivity to initialization.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item Train a \(10\times 10\) SOM on handwritten digits (MNIST) and plot component planes; report quantization/topographic errors as training progresses.
    \item Implement the six-step SOM procedure with both Gaussian and rectangular neighborhood functions and compare convergence speed.
    \item Visualize the effect of annealing schedules by freezing the learning rate and neighborhood radius at different epochs and observing the resulting U-Matrix.
    \item Compare SOM prototypes to K-means centers on the same dataset; sweep \((\sigma(t))\) schedules and map sizes; report QE/TE and a trustworthiness@k measure.
\end{itemize}

\medskip
\noindent\textbf{If you are skipping ahead.} Keep the notion of an ``energy landscape'' from this chapter in mind: \Cref{chap:hopfield} makes that idea explicit with a Lyapunov energy, and later attention models (\Cref{chap:transformers}) can be read as learned, content-based neighborhood selection.
\end{tcolorbox}

\medskip
\paragraph{Where we head next.} \Cref{chap:hopfield} extends the unsupervised thread with energy-based associative memory, complementing SOM topology learning with retrieval dynamics that foreshadow later attention-style mechanisms.

% Chapter 5
\section{Introduction to Neural Networks}\label{chap:perceptron}

\Cref{chap:logistic} established linear and logistic models as strong baselines, but their decision boundaries stay linear in the original feature space. We now shift from a statistical lens to a biological abstraction: simple neurons whose composition yields nonlinear decision surfaces. This chapter introduces neuron models, perceptrons, activation functions, and the first learning rules (perceptron and Adaline); \Cref{fig:roadmap} marks this as the start of the neural strand.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
After this chapter, you should be able to:
\begin{itemize}
  \item Describe the core ingredients of neural networks (architecture, activations, learning).
  \item Explain at a high level why multilayer perceptrons rely on smooth activations and gradient\hyp{}based training (\Cref{chap:mlp,chap:backprop}).
  \item Identify common pitfalls (saturation, poor initialization) and basic remedies.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Biology as engineering abstraction: start with simple units, make the update rule explicit, and use geometry to build intuition before the algebra gets deep.
\end{tcolorbox}

\subsection{Biological Inspiration}
\label{sec:perceptron_biological_inspiration}

Neural networks borrow a simple but powerful idea from biology: complex behavior can emerge from many simple units interacting through many simple connections. The goal is not to model the brain in detail, but to steal an engineering abstraction we can write down and implement: signals flow through a network of units, and learning adjusts connection strengths so the overall system changes its behavior.

\paragraph{Neurons and Neural Activity}

A biological neuron can be viewed as a processing unit that receives multiple inputs, integrates them, and produces an output if certain conditions are met. In the simplified picture we use here, the key parts are:

\begin{itemize}
    \item \textbf{Dendrites:} Receive incoming signals from other neurons.
    \item \textbf{Cell body (soma):} Integrates incoming signals.
    \item \textbf{Axon:} Transmits the output signal to other neurons.
    \item \textbf{Synapses:} Junctions where signals are transmitted between neurons.
\end{itemize}

Incoming signals can excite or inhibit the neuron. When the combined influence crosses a threshold, the neuron ``fires'' and sends an impulse down the axon to connected neurons. Real neurons are richer than this sketch (timing, frequency, and graded effects matter), but the abstraction is enough to motivate an artificial unit that combines inputs, applies a nonlinearity, and emits an output.

\paragraph{Complexities and unknowns (and what we steal anyway).}
Real neural tissue is a biophysical system: spikes, timing, chemistry, adaptation, and many interacting feedback loops. We do not pretend to model that faithfully here. Instead, we borrow an engineering abstraction that has two virtues: (i) it is composable, and (ii) it gives us a learning rule we can write down, debug, and scale.

So what do we \emph{keep} from biology? The idea that many small units, each doing a simple computation, can be wired into a larger system whose behavior is richer than any single unit. And what do we \emph{let go} of? The detailed mechanisms (spike timing, ion channels, and biological realism) that matter for neuroscience but are not necessary to build useful learning machines.

A good analogy is circuit design: you can build powerful systems out of simple, idealized components without simulating electron physics at every wire. In the same spirit, we keep the compositional story (many units, many connections) and insist that the update rule is explicit and learnable. This is a recurring theme in the book: we look to nature for candidate mechanisms behind intelligent behavior, describe them in scientific terms, and then simplify them until they become concrete engineering problems we can test and improve.

\subsection{From Biological to Artificial Neural Networks}
\label{sec:perceptron_from_biological_to_artificial_neural_networks}

Artificial neural networks (ANNs) are computational models inspired by the structure and function of biological neural systems. The goal is to create systems that can process information, learn from data, and perform tasks that require intelligence.

\paragraph{Key Features of Artificial Neural Networks}

To design an ANN that captures essential aspects of biological neural processing, several features must be considered:

\begin{enumerate}
    \item \textbf{Architecture:} The arrangement and connectivity of neurons within the network. This includes the number of layers, the pattern of connections (e.g., feedforward, recurrent), and the flow of information.

    \item \textbf{Signal Propagation:} How input signals are transmitted through the network, transformed by neurons, and produce outputs.

    \item \textbf{Learning Mechanism:} The method by which the network adjusts its parameters (e.g., weights) based on data to improve performance. This involves capturing and retaining knowledge from experience.

    \item \textbf{Activation Dynamics:} The rules governing neuron activation, including how neurons decide to fire based on inputs, the degree of activation, and inhibition mechanisms.
\end{enumerate}

\paragraph{Historical Context}

The concept of artificial neural networks dates back to the early 1940s, with pioneering work that attempted to mathematically model neuron behavior. Over the past eight decades, ANNs have evolved significantly, leading to a variety of architectures and learning algorithms. This evolution reflects ongoing efforts to better approximate biological intelligence and to address practical challenges in computation and learning.

\paragraph{Where this fits in Part II.}\label{sec:perceptron_outline_of_neural_network_study}
This chapter starts the neural strand by introducing the perceptron neuron (defined below) as the basic
trainable unit: a weighted sum followed by an activation/threshold, together with the first learning rules
(mistake-driven and gradient-driven). The point is not to model biology literally; the point is to steal an
engineering abstraction we can write down, debug, and scale.

\begin{tcolorbox}[summarybox, title={Part II local roadmap: what stays the same, what changes}]
\textbf{What stays the same.} We still follow the ERM loop: choose a representation, define a loss, optimize, then verify with honest evaluation and diagnostics. As we proceed, you will find that this theme generalizes across many of the algorithms we discuss.

\medskip
\textbf{What changes (the design space).} Instead of choosing one linear model, we build systems by composing
many simple units. Used on its own, a perceptron-like unit still induces a linear decision boundary in the
original feature space (as in logistic regression). Its importance is that it is \emph{composable}: by changing
how units are connected, what nonlinearities they apply, and what signals drive learning, we can move from a
single linear boundary to richer nonlinear decision surfaces when the task demands it. Many of these design
choices are loosely inspired by how we think biological systems process information (as sketched in the
section that follows), but the target remains an engineering model, not a faithful brain simulation.

\medskip
\textbf{How the rest of Part II builds.}
\begin{itemize}
  \item \textbf{Units \(\rightarrow\) multilayer systems:} \Cref{chap:mlp} chains units into multilayer networks;
  \Cref{chap:backprop} shows how to compute all gradients efficiently (the training engine).
  \item \textbf{Architectures as inductive bias:} later chapters introduce structured connectivity (e.g., convolution,
  recurrence, attention) that changes what patterns are easy to represent and what information can flow.
  \item \textbf{Audit hooks:} keep learning curves, calibration, and slice checks alongside accuracy. A good loss is
  not automatically a good decision.
\end{itemize}
\end{tcolorbox}

% Chapter 5 (continued)

\subsection{Neural Network Architectures}
\label{sec:perceptron_neural_network_architectures}

Neural networks can be broadly categorized based on the flow of information through their structure. Understanding these architectures is crucial for designing and analyzing neural models that mimic biological neural systems.

\paragraph{Feedforward Neural Networks}

Feedforward neural networks (FNNs) are characterized by a unidirectional flow of information from input to output layers without any cycles or loops. The information propagates forward through successive layers of neurons, each layer transforming the input received from the previous layer.

Conceptually, this can be thought of as a cascade of neuron activations where each neuron receives input signals, processes them, and passes the output to the next layer. This architecture aligns with the idea that sensory information in biological systems is processed in a hierarchical manner.

Mathematically, if we denote the input vector as \(\mathbf{x}\), the output of layer \(l\) as \(\mathbf{a}^{(l)}\), and the weight matrix connecting layer \(l-1\) to layer \(l\) as \(\mathbf{W}^{(l)}\), the feedforward operation is given by:
\begin{align}
    \mathbf{z}^{(l)} &= \mathbf{a}^{(l-1)} \mathbf{W}^{(l)} + \mathbf{b}^{(l)} \\
    \mathbf{a}^{(l)} &= f(\mathbf{z}^{(l)}).
    \label{eq:feedforward}
\end{align}
where \(\mathbf{b}^{(l)}\) is the bias vector and \(f(\cdot)\) is the activation function applied element-wise.
\paragraph{Shapes and convention.} We use the row-major (deep-learning) convention. A single example is a row vector \(\mathbf{a}^{(l)}\in\mathbb{R}^{1\times n_l}\), a mini\hyp{}batch stacks examples by rows \(\mathbf{A}^{(l)}\in\mathbb{R}^{B\times n_l}\), and weights map features by right multiplication \(\mathbf{W}^{(l)}\in\mathbb{R}^{n_{l-1}\times n_l}\). Biases \(\mathbf{b}^{(l)}\in\mathbb{R}^{n_l}\) broadcast across the batch: \(\mathbf{Z}^{(l)}=\mathbf{A}^{(l-1)}\mathbf{W}^{(l)}+\mathbf{1}(\mathbf{b}^{(l)})^\top\). For a concrete check, if \(B=4\), \(n_{l-1}=3\), and \(n_l=5\), then \(\mathbf{A}^{(l-1)}\in\mathbb{R}^{4\times 3}\), \(\mathbf{W}^{(l)}\in\mathbb{R}^{3\times 5}\), and \(\mathbf{Z}^{(l)}\in\mathbb{R}^{4\times 5}\). We reserve \(\phi(\cdot)\) for kernel feature maps (\Cref{app:kernels}).

\paragraph{Recurrent Neural Networks}

In contrast, recurrent neural networks (RNNs) allow information to flow in cycles, enabling feedback connections. This means that the network's state at a given time depends not only on the current input but also on previous states, effectively creating a form of memory.

The recurrent architecture is more flexible and biologically plausible since neurons can influence each other bidirectionally and inputs/outputs can be introduced or sampled at various points in the network. This allows modeling of temporal sequences and dynamic behaviors. A simple recurrent update is
\[
\mathbf{h}_t = f(\mathbf{x}_t \mathbf{W}_{xh} + \mathbf{h}_{t-1} \mathbf{W}_{hh} + \mathbf{b}_h), \qquad
\mathbf{y}_t = \mathbf{h}_t \mathbf{W}_{hy} + \mathbf{b}_y,
\]
with the full treatment deferred to \Cref{chap:rnn}.

\subsection{Activation Functions}
\label{sec:perceptron_activation_functions}

Activation functions determine how the input to a neuron is transformed into an output signal, effectively controlling the neuron's excitation level. They play a critical role in enabling neural networks to model complex, nonlinear relationships.

\paragraph{Biological Motivation}

In biological neurons, excitation occurs when the combined chemical signals exceed a certain threshold, triggering an action potential (a "fire"). Similarly, artificial neurons use activation functions to decide whether to activate (fire) based on their input.

\paragraph{Common Activation Functions}
Activation functions map the aggregated input \(z\) to a neuron's output \(y=f(z)\); they inject nonlinearity and control gradient flow during learning. Different choices trade off biological plausibility, numerical stability, and ease of optimization.

\begin{itemize}
    \item \textbf{Step Function (Heaviside)}:
    \[
        f(x) = \begin{cases}
            1 & x > 0 \\
            0 & x \leq 0
        \end{cases}
    \]
    Models a binary firing behavior but is not differentiable, limiting its use in gradient-based learning.

    \item \textbf{Sign Function}:
    \[
        f(x) = \begin{cases}
            1 & x > 0 \\
            0 & x = 0 \\
            -1 & x < 0
        \end{cases}
    \]
        Allows for inhibitory (negative) outputs, mimicking excitatory and inhibitory neuron behavior. We adopt the convention \(f(0)=0\); some authors either leave \(\mathrm{sign}(0)\) undefined or set it to \(+1\), so it is helpful to state the choice explicitly.

    \item \textbf{Linear Function}:
    \[
        f(x) = x
    \]
    Useful in some contexts but cannot model nonlinearities alone.

    \item \textbf{Sigmoid Function}:
    \[
        f(x) = \frac{1}{1 + e^{-x}}
    \]
        Smoothly maps inputs to \((0,1)\), differentiable, and historically popular. Because sigmoid outputs saturate near \(0\) and \(1\), gradients can become small in deep stacks; later chapters discuss practical workarounds and alternatives.

    \item \textbf{Hyperbolic Tangent (tanh)}:
    \[
        f(x) = \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
    \]
    Maps inputs to \((-1,1)\), zero-centered, often preferred over sigmoid.

    \item \textbf{ReLU (Rectified Linear Unit)}:
    \[
        f(x) = \max(0, x)
    \]
        Computationally efficient and helps mitigate vanishing gradient problems.

\end{itemize}

\begin{tcolorbox}[summarybox, title={Notation note: activations and thresholds}]
In this chapter we use \(f(\cdot)\) as a generic placeholder for an activation function; when we need the logistic sigmoid specifically we write \(\sigma(\cdot)\). Elsewhere in the book, \(\phi(\cdot)\) denotes a kernel feature map. For thresholded functions we adopt \(H(0)=1\) (Heaviside) and \(\mathrm{sgn}(0)=0\) by convention. These choices do not affect continuous models but keep examples consistent.
\end{tcolorbox}

\subsection{Learning Paradigms in Neural Networks}
\label{sec:perceptron_learning_paradigms_in_neural_networks}

When building a neural network, whether feedforward or recurrent, the fundamental process involves producing an output, comparing it with a target, and then adjusting the network parameters based on the error. This iterative process is the essence of \emph{learning}. We distinguish several learning paradigms depending on the availability and nature of the target information:

\paragraph{Supervised Learning}
In supervised learning, the network is provided with input-output pairs. The network produces an output for a given input, compares it to the known target output, computes an error, and updates its parameters to reduce this error. This requires labeled data and is the most common learning paradigm in practice.

\paragraph{Unsupervised Learning}
In unsupervised learning, there is no explicit target output. The network must discover patterns or structure in the input data by itself. This often involves competition among different patterns, where some patterns become dominant and reinforce themselves, while others are suppressed. The network evolves until it reaches an equilibrium state where the learned representations stabilize.
Beyond competitive learning, unsupervised methods encompass clustering, density estimation, dimensionality reduction, autoencoders, and modern self-supervised objectives---any setting where structure is inferred directly from the inputs.

\paragraph{Reinforcement Learning}
Reinforcement learning (RL) models learning from interaction with feedback. An agent with policy \(\pi(a\mid s)\) selects actions, collects rewards, and updates \(\pi\) to improve expected return. Full RL treatments appear later; here the point is that not all learning is supervised, and neural-network controllers are common in modern RL.

\subsection{Fundamentals of Artificial Neural Networks}
\label{sec:perceptron_fundamentals_of_artificial_neural_networks}

The foundational model of artificial neural networks dates back to \citet{McCullochPitts1943}, who proposed a simple neuron model capturing essential features of biological neurons.

\paragraph{McCulloch-Pitts Neuron Model}

Consider a single neuron with multiple binary inputs \( x_i \in \{0,1\} \), \( i=1, \ldots, n \). Each input is associated with a weight \( w_i \), which can be positive (excitatory) or negative (inhibitory). The neuron computes a weighted sum of its inputs:

\begin{align}
    S = \sum_{i=1}^n w_i x_i.
    \label{eq:weighted_sum}
\end{align}

The output \( y \) of the neuron is determined by comparing \( S \) to a threshold \(\theta\):

\begin{align}
    y =
    \begin{cases}
        1, & \text{if } S \geq \theta, \\
        0, & \text{otherwise}.
    \end{cases}
    \label{eq:threshold_output}
\end{align}

Key characteristics of this model include:

\begin{itemize}
    \item \textbf{Binary inputs:} Inputs are either active (1) or inactive (0).
    \item \textbf{Excitatory and inhibitory weights:} Weights \( w_i > 0 \) excite the neuron, while \( w_i < 0 \) inhibit it.
    \item \textbf{Thresholding:} The neuron fires (outputs 1) only if the weighted sum exceeds the threshold.
\end{itemize}

\paragraph{Interpretation}

This simple neuron can be viewed as a linear classifier that partitions the input space into two regions separated by the hyperplane defined by the equation

\begin{align}
    \sum_{i=1}^n w_i x_i = \theta.
    \label{eq:auto:lecture_3_part_i:1}
\end{align}

The learning task reduces to finding appropriate weights \( w_i \) and threshold \(\theta\) that correctly classify inputs.

\paragraph{Excitation and Inhibition}

The neuron can be excited or inhibited depending on the sign and magnitude of the weights. For example:

\begin{itemize}
    \item If all \( w_i > 0 \), the neuron is purely excitatory.
    \item If some \( w_i < 0 \), those inputs inhibit the neuron.
    \item The balance of excitation and inhibition determines the neuron's response.
\end{itemize}
In biological circuits inhibition is carried by specialized interneurons, whereas here a negative weight is an abstract shortcut; the sign simply indicates whether an input pushes the weighted sum above or below the threshold. Artificial neurons are function approximators; similarity to biology is inspirational, not mechanistic.

\paragraph{Learning Objective}

In this model, learning can be interpreted as adjusting the weights \( w_i \) and threshold \(\theta\) to achieve desired input-output mappings. The challenge is to find these parameters such that the neuron outputs 1 for inputs belonging to a certain class and 0 otherwise.

\subsection{Mathematical Formulation of the Neuron Output}
\label{sec:perceptron_mathematical_formulation_of_the_neuron_output}

To summarize, the neuron output is given by

\begin{align}
    y = f\left( \sum_{i=1}^n w_i x_i - \theta \right),
    \label{eq:neuron_output}
\end{align}

where \( f(\cdot) \) is the activation function, which in the McCulloch-Pitts model is a Heaviside step function:

\begin{align}
    f(z) =
    \begin{cases}
        1, & z \geq 0, \\
        0, & z < 0.
    \end{cases}
    \label{eq:auto:lecture_3_part_i:2}
\end{align}
We explicitly set \(f(0)=1\); other texts sometimes use \(f(0)=\tfrac{1}{2}\), so documenting the convention avoids confusion when comparing derivations. It is also common to absorb the threshold into an augmented weight vector by defining \(x_0 = 1\) and \(w_0 = -\theta\), yielding a pure inner product \(\mathbf{w}^\top \mathbf{x}\) that we will reuse in the perceptron section.

This model laid the groundwork for later developments in neural networks, including the introduction of differentiable nonlinearities that enable gradient-based learning.

\subsection{McCulloch-Pitts neuron: examples and limits}
\label{sec:perceptron_mcculloch_pitts_neuron_examples_and_limits}

Recall the MP neuron definition in \eqref{eq:weighted_sum}--\eqref{eq:threshold_output} (equivalently \eqref{eq:neuron_output}); here we focus on logic-gate examples and the limitations that motivate learnable variants.

\paragraph{Example: AND and OR gates}

- For an AND gate with inputs \(x_1, x_2\), set weights \(w_1 = w_2 = 1\) and threshold \(\theta = 2\). The output is 1 only if both inputs are 1, matching the AND truth table.

- For an OR gate, keep weights \(w_1 = w_2 = 1\) but set \(\theta = 1\). The output is 1 if at least one input is 1, matching the OR truth table.

This demonstrates how the MP neuron can implement simple logical functions by appropriate choice of weights and threshold.

\paragraph{Limitations of the MP model}

Despite its conceptual simplicity, the MP neuron has significant limitations:

\begin{itemize}
    \item \textbf{No learning mechanism:} The weights and threshold must be manually assigned or guessed. There is no algorithmic way to adjust parameters based on data.
    \item \textbf{Limited computational power:} The MP neuron can only represent linearly separable functions. Complex patterns requiring nonlinear decision boundaries cannot be modeled.
    \item \textbf{Binary inputs and outputs:} The model is restricted to binary signals, limiting its applicability to real-valued data.
\end{itemize}

These limitations motivated the development of more sophisticated neuron models and learning algorithms.

\subsection{From MP Neuron to Perceptron and Beyond}
\label{sec:perceptron}

The MP neuron laid the groundwork for subsequent models that introduced learning capabilities and continuous-valued inputs and outputs.

\paragraph{Perceptron model}

The perceptron, introduced by Rosenblatt in 1958, extends the MP neuron by incorporating a learning algorithm to adjust weights based on training data. The perceptron output is
\begin{align}
    y = \begin{cases}
    1 & \text{if } \mathbf{w}^\top \mathbf{x} + b \geq 0, \\
    0 & \text{otherwise},
    \end{cases}
    \label{eq:perceptron}
\end{align}
where \(\mathbf{x}\) is the input vector, \(\mathbf{w}\) the weight vector, and \(b\) the bias (threshold).
\begin{tcolorbox}[summarybox, title={Targets and encodings}]
We switch between labels in \texttt{\{0,1\}} (probability view) and labels in \texttt{\{-1,+1\}} (margin view).
Convert with \texttt{y\_pm = 2*y01 - 1} and \texttt{y01 = (y\_pm + 1)/2}.
Perceptron updates below use the \texttt{\{-1,+1\}} encoding.
\end{tcolorbox}

The perceptron learning rule iteratively updates weights to reduce classification errors, enabling the model to learn from data rather than relying on manual parameter selection. The signed-margin derivation below yields the update used in practice; the induced separating hyperplane and signed distance are illustrated in \Cref{fig:lec3-perceptron-geometry}.

\paragraph{Perceptron update from the signed margin.} Let \(d_i = y_i(\mathbf{w}^\top \mathbf{x}_i + b)\) be the signed margin. If \(d_i \ge 0\) the example is correctly classified; if \(d_i < 0\) the example is misclassified. A common perceptron criterion is
\[
J(\mathbf{w}, b) = - \sum_{i \in \mathcal{M}} d_i = - \sum_{i \in \mathcal{M}} y_i (\mathbf{w}^\top \mathbf{x}_i + b),
\]
where \(\mathcal{M}\) is the set of misclassified examples. Taking a gradient step on \(J\) yields
\[
\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i,\qquad
b \leftarrow b + \eta y_i,
\]
which is exactly the perceptron update. In augmented form, set \(x_0=1\) and \(w_0=b\), and the update becomes \(\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i\). Geometrically, each mistake nudges the hyperplane so the signed distance \(d_i\) increases.

\begin{tcolorbox}[summarybox, title={Worked example: learning an OR gate by mistake-driven updates}]
We use labels \(y\in\{-1,+1\}\) (so OR has targets \((-1,+1,+1,+1)\) for \((0,0),(0,1),(1,0),(1,1)\)),
learning rate \(\eta=1\), and initialize \(\mathbf{w}=(0,0)\), \(b=0\). Cycle through the four inputs in that
fixed order; whenever \(y(\mathbf{w}^\top\mathbf{x}+b)\le 0\), apply \(\mathbf{w}\leftarrow \mathbf{w}+y\mathbf{x}\),
\(b\leftarrow b+y\).

\medskip
\noindent The sequence of updates (after each mistake) is:

% QC-BEGIN: perceptron_or_trace
% step epoch idx x1 x2 y w1 w2 b
% 1 1 1 0 0 -1 0 0 -1
% 2 1 2 0 1  1 0 1  0
% 3 1 3 1 0  1 1 1  1
% 4 2 1 0 0 -1 1 1  0
% 5 3 1 0 0 -1 1 1 -1
% 6 3 2 0 1  1 1 2  0
% 7 4 1 0 0 -1 1 2 -1
% 8 4 3 1 0  1 2 2  0
% 9 5 1 0 0 -1 2 2 -1
% QC-END: perceptron_or_trace
\[
\begin{array}{r|c|c|c}
\text{step} & (x_1,x_2),\,y & (w_1,w_2) & b \\
\hline
1 & (0,0),-1 & (0,0) & -1\\
2 & (0,1),+1 & (0,1) & 0\\
3 & (1,0),+1 & (1,1) & 1\\
4 & (0,0),-1 & (1,1) & 0\\
5 & (0,0),-1 & (1,1) & -1\\
6 & (0,1),+1 & (1,2) & 0\\
7 & (0,0),-1 & (1,2) & -1\\
8 & (1,0),+1 & (2,2) & 0\\
9 & (0,0),-1 & (2,2) & -1\\
\end{array}
\]
After these 9 updates, predicting \(+1\) when \(\mathbf{w}^\top\mathbf{x}+b\ge 0\) (and \(-1\) otherwise) with
\(\mathbf{w}=(2,2)\), \(b=-1\) labels all four OR inputs correctly.
\end{tcolorbox}

\paragraph{Perceptron convergence theorem.} If a training set is linearly separable with margin \(\gamma > 0\), the perceptron learning algorithm is guaranteed to find a separating hyperplane after at most \((R/\gamma)^2\) updates, where \(R\) bounds the input norms. Rescaling features changes \(R\) and \(\gamma\), so standardizing inputs tightens the bound. When the data are not separable the algorithm can cycle forever; \Cref{sec:mlp-limitations} (and \Cref{chap:mlp}) therefore emphasize feature scaling, bias terms, and the move to differentiable multilayer models to handle nonlinear problems.

\begin{tcolorbox}[summarybox, title={Perceptron convergence theorem (proof sketch)}]
Assume there exists a unit vector \(\mathbf{w}^\star\) such that \(y_i\,\mathbf{w}^\star\!\cdot\!\mathbf{x}_i \ge \gamma\) for all \(i\) and that \(\|\mathbf{x}_i\|\le R\).
Let w(t) denote the perceptron weights after t mistakes. Each mistake updates:
\[
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + y_i \mathbf{x}_i.
\]
\begin{enumerate}
    \item \textbf{Progress along the separator.} The inner product with w* grows by at least gamma each mistake:
    \[
    \begin{aligned}
    \mathbf{w}^{(t+1)} \cdot \mathbf{w}^\star
    &= \mathbf{w}^{(t)} \cdot \mathbf{w}^\star + y_i\,\mathbf{x}_i \cdot \mathbf{w}^\star \\
    &\ge \mathbf{w}^{(t)} \cdot \mathbf{w}^\star + \gamma.
    \end{aligned}
    \]
    Thus after T mistakes, the dot product with w* is at least T*gamma.
    \item \textbf{Bounding the norm.} The squared norm grows slowly:
    \[
    \begin{aligned}
    \|\mathbf{w}^{(t+1)}\|^2
    &= \|\mathbf{w}^{(t)}\|^2 + \| \mathbf{x}_i\|^2 + 2 y_i \mathbf{x}_i \cdot \mathbf{w}^{(t)} \\
    &\le \|\mathbf{w}^{(t)}\|^2 + R^2,
    \end{aligned}
    \]
    because the mistake condition implies \(y_i \mathbf{x}_i \cdot \mathbf{w}^{(t)} \le 0\). Inductively, \(\|\mathbf{w}^{(T)}\|^2 \le TR^2\).
    \item \textbf{Combine via Cauchy--Schwarz.}
    \[
    \begin{aligned}
    T\gamma
    &\le \mathbf{w}^{(T)} \cdot \mathbf{w}^\star \\
    &\le \|\mathbf{w}^{(T)}\| \|\mathbf{w}^\star\|
    \le \sqrt{T}\, R,
    \end{aligned}
    \]
    which implies \(T \le (R/\gamma)^2\).
\end{enumerate}
Therefore the perceptron halts after finitely many mistakes on separable data. If the data are not separable, some \(\gamma > 0\) cannot be found, and the above argument no longer applies; hence the need for multilayer networks.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Common perceptron pitfalls}]
\begin{itemize}
    \item \textbf{Feature scaling:} Large-magnitude features dominate updates; standardize inputs first.
    \item \textbf{Random seed sensitivity:} Different initial weights can lead to drastically different separating hyperplanes.
    \item \textbf{Non-separable data:} Without slack variables or kernels the perceptron will not converge; diagnose this before training indefinitely. XOR is the canonical counterexample.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Geometry intuition: beyond XOR (two triangles)}]
XOR is the canonical non-separable toy, but the limitation is geometric, not specific to a truth table. A second picture to keep in mind is \emph{two interleaved triangles}: one class occupies the vertices of a large triangle, while the other occupies the vertices of a smaller triangle rotated inside it. No single line separates the two sets; any separating boundary must ``bend'' or be assembled from multiple linear pieces.
\medskip

\noindent\textbf{Why this matters.} A perceptron draws exactly one hyperplane, so it cannot represent such boundaries. \Cref{chap:mlp} restores expressivity by composing units so the overall decision boundary can be piecewise-linear (or curved) while still being trainable by gradients.
\end{tcolorbox}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            axis lines=middle,
            axis line style={gray!70},
            xmin=-3.2, xmax=3.2,
            ymin=-3.0, ymax=3.0,
            width=0.64\linewidth,
            height=0.5\linewidth,
            xtick={-2,0,2},
            ytick={-2,0,2},
            clip=false,
            ticklabel style={font=\scriptsize, gray!70},
            grid=both,
            minor grid style={gray!10},
            major grid style={gray!20},
            legend style={font=\scriptsize, draw=none, fill=none, at={(0.02,0.98)}, anchor=north west}
        ]
            \addplot[thick, gray!80] coordinates {(-3.2,2.1) (3.2,-2.1)}
                node[pos=0.82, rotate=-45, anchor=south west, font=\scriptsize, text=gray!70]
                {$\mathbf{w}^\top\mathbf{x}+b=0$};
            \addplot[dashed, gray!60] coordinates {(-3.2,2.9) (3.2,-1.3)};
            \addplot[dashed, gray!60] coordinates {(-3.2,1.3) (3.2,-2.9)};
            \node[anchor=south east, font=\scriptsize, gray!70] at (axis cs:3.1,-2.15) {$C{=}1$};

            \addplot[only marks, mark=*, mark size=1.8pt, mark options={draw=white, line width=0.4pt}, color=cbBlue]
                coordinates {(-1.2,1.5) (-0.8,2.1) (-2,1.3)};
            \addlegendentry{Class $+1$}
            \addplot[only marks, mark=triangle*, mark size=2.1pt, mark options={draw=white, line width=0.4pt}, color=cbOrange]
                coordinates {(1.4,-1.3) (2.2,-0.6) (0.9,-2.1)};
            \addlegendentry{Class $-1$}

            \addplot[thick, cbPink, -{Stealth[length=2.2mm]}] coordinates {(0.6,-0.3) (0.15,0.2)};
            \node[cbBlue!70!black, font=\scriptsize] at (0.85,0.3) {$d(\mathbf{x},\mathcal{H})$};
        \end{axis}
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Perceptron geometry. Points on opposite sides of the separating hyperplane receive different labels, and signed distance to the boundary controls both prediction and update magnitude. Compare with \Cref{fig:lec2-logistic-boundary} in \Cref{chap:logistic}: both are linear separators, but logistic smooths the boundary into calibrated probabilities.}
    \label{fig:lec3-perceptron-geometry}
\end{figure}


\paragraph{Adaline model}

The Adaptive Linear Neuron (Adaline), developed in the 1960s, further improves on the perceptron by using a linear activation function and minimizing a continuous error function (mean squared error). This allows the use of gradient descent for training, leading to more stable convergence.

\paragraph{Adaline weight update (derivation)}
Adaline uses a linear output \(y = \mathbf{w}^\top \mathbf{x} + b\) and the squared error
\[
E = \tfrac{1}{2}(t-y)^2.
\]
The gradient is \(\partial E / \partial \mathbf{w} = -(t-y)\mathbf{x}\) and \(\partial E/\partial b = -(t-y)\), so the update is
\[
\mathbf{w} \leftarrow \mathbf{w} + \eta (t-y)\mathbf{x}, \qquad
b \leftarrow b + \eta (t-y).
\]
Unlike the perceptron, Adaline updates on every example and scales the step by the residual \(t-y\); this is the first explicit appearance of gradient-based weight optimization in the neural narrative.

The perceptron and Adaline models are limited to linearly separable problems. To overcome this, multilayer perceptrons (MLPs) with hidden layers were introduced; \Cref{chap:mlp} and \Cref{chap:backprop} develop the mechanics in full.

\begin{tcolorbox}[summarybox, title={Perceptron vs.\ logistic regression}]
Linear score \(s(\mathbf{x})=\mathbf{w}^\top\mathbf{x}+b\). The perceptron predicts \(\mathbb{1}[s\ge 0]\) and updates \(\mathbf{w}\leftarrow \mathbf{w}+\eta y_i\mathbf{x}_i\) (and \(b\leftarrow b+\eta y_i\)) only on mistakes, with \(y_i\in\{-1,+1\}\). Logistic regression predicts \(\sigma(s)\), minimizes cross\hyp{}entropy \(-\sum_i y_i\log\sigma(s_i)+(1-y_i)\log(1-\sigma(s_i))\), and steps by \(\sum_i(\sigma(s_i)-y_i)\mathbf{x}_i\). Prefer logistic when calibrated probabilities and smooth optimization are needed (\Cref{chap:logistic}).
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Author's note: what a single perceptron cannot express}]
A single perceptron makes one global, all-or-none decision: one hyperplane, one threshold, one set of weights shared across every example. That simplicity is the point of the model, and it is also the source of the limitation.

Many real problems need \emph{communities} of units that can specialize. Different hidden units respond to different regions, features, or patterns, and the model combines those responses into a richer decision surface. Multi-layer networks do not only add parameters; they add internal structure that lets different parts of the model ``care about'' different parts of the data.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Key takeaways}]
\textbf{Minimum viable mastery}
\begin{itemize}
    \item The perceptron and Adaline turn threshold units into trainable classifiers by updating weights from data.
    \item Geometry (hyperplanes and signed distance) explains predictions and update magnitude.
    \item Logistic regression keeps the same linear score but learns calibrated probabilities via a smooth loss (\Cref{chap:logistic}).
    \item Nonlinear tasks (e.g., XOR) require multilayer networks and backpropagation (\Crefrange{chap:mlp}{chap:backprop}).
\end{itemize}
\medskip
\textbf{Common pitfalls}
\begin{itemize}
    \item Expecting a single hyperplane to solve nonconvex structure: without hidden units you cannot express XOR-like logic.
    \item Mixing label codings (\(\{-1,+1\}\) vs.\ \(\{0,1\}\)) without adjusting the loss/update formulas.
    \item Treating linear scores as probabilities: calibrated probabilities require a probabilistic model/loss (e.g., logistic).
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item Implement a minimal example from this chapter and visualize intermediate quantities (plots or diagnostics) to match the pseudocode.
    \item Stress-test a key hyperparameter or design choice discussed here and report the effect on validation performance or stability.
    \item Re-derive one core equation or update rule by hand and check it numerically against your implementation.
\end{itemize}
\medskip
\noindent\textbf{If you are skipping ahead.} Remember the two bottlenecks that force multilayer networks: expressivity (nonlinear boundaries) and trainability (smooth gradients). \Cref{chap:mlp} and \Cref{chap:backprop} assume these motivations.
\end{tcolorbox}

\medskip
\paragraph{Where we head next.} Perceptrons are intentionally simple: hard thresholds and uniform updates. Their strengths (linear separation) and limits (for example XOR) motivate multilayer models. \Cref{chap:mlp} continues this thread by chaining units, defining a loss, and asking the key training question: \emph{how should weights change to improve performance?} That question leads directly to the chain rule and smooth activations. \Cref{chap:backprop} then scales the same cache-and-chain-rule accounting to arbitrary depth and efficient implementation.

\nocite{McCullochPitts1943, Rosenblatt1958, WidrowHoff1960, Rumelhart1986}

% Chapter 7
\section{Backpropagation Learning in Multi-Layer Perceptrons}\label{chap:backprop}
\graphicspath{{assets/lec4/}}

Building on the two\hyp{}neuron derivation in \Cref{chap:mlp}, we scale the same idea to an \(L\)\hyp{}layer network. Nothing new is introduced: it is still the chain rule. What changes is the bookkeeping. We compute local error signals (the \(\delta\)'s) once, reuse them to obtain every weight and bias gradient, and do it in an order that is efficient enough to train deep models in practice.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
\begin{itemize}
    \item Derive the layerwise \(\boldsymbol{\delta}\) recursion for an \(L\)\hyp{}layer MLP and use it to write gradients for \(\mathbf{W}^{(l)}\) and \(\mathbf{b}^{(l)}\).
    \item Keep a consistent ``shape ledger'' for activations, pre\hyp{}activations, and error signals so vectorized code matches the algebra.
    \item Run one tiny numeric trace and one finite\hyp{}difference gradient check to confirm your implementation before scaling up.
    \item Recognize the two common output\hyp{}layer patterns: squared error (general) and softmax + cross\hyp{}entropy (simplified \(\delta\)).
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Treat the network as a computation you can trace. Cache what the forward pass computes, then sweep backward once and reuse the same local error signals to update every parameter.
\end{tcolorbox}

\subsection{Context and Motivation}
\label{sec:backprop_context_and_motivation}

Multi\hyp{}layer networks raise a specific question: \emph{How do we update the weights across multiple layers when the only explicit error signal is at the output?} In a single\hyp{}layer perceptron, the output error touches the weights directly; in a deep network, a change in one layer propagates through subsequent layers and alters the output in a nonlinear, intertwined way.

Shallow networks (one hidden layer) already move beyond linear separability, but more complex tasks demand deeper hierarchies of features. The multi\hyp{}layer perceptron stacks these layers to learn richer decision boundaries, and backpropagation is the mechanism that makes that depth trainable.

\begin{tcolorbox}[summarybox, title={Intuition: backprop as a controlled ripple effect}]
Changing one early weight can only affect the loss by moving the intermediate quantities it feeds into. Backpropagation does not introduce a new idea; it is the chain rule written in an efficient order: run the forward pass once, cache the intermediates you will need, then flow sensitivities backward so each layer gets its own local error signal. Identities like \(\sigma'(p)=y(1-y)\) (from \Cref{chap:mlp}) are valuable because they let you compute a derivative from cached forward values.
\end{tcolorbox}

\paragraph{Implementation lens.}
A practical MLP rarely updates one coordinate at a time; gradients are treated as full vectors so every weight moves coherently. Backpropagation is what turns a multilayer diagram into a trainable model: it reuses local error signals so you can compute \emph{all} gradients efficiently, making it practical to learn hidden representations (not just tune a final linear layer) while keeping the same validation\(\rightarrow\)audit discipline (learning curves, early stopping, and slice checks).

\subsection{Problem Setup}
\label{sec:backprop_problem_setup}

Consider a multi-layer perceptron with layers indexed by $l = 0, 1, \ldots, L$, where $l=0$ is the input layer and $L$ is the output layer. Each layer $l$ contains neurons indexed by $i$, and the output of neuron $i$ in layer $l$ is denoted by $a_i^{(l)}$. The input to this neuron before activation is denoted by $z_i^{(l)}$. The weights connecting neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$ are denoted by $w_{ij}^{(l)}$.

The forward pass through the network is given by:
\begin{align}
    z_j^{(l)} &= \sum_i a_i^{(l-1)} w_{ij}^{(l)} + b_j^{(l)}, \label{eq:forward_z} \\
    a_j^{(l)} &= f\big(z_j^{(l)}\big), \label{eq:forward_a}
\end{align}
where $b_j^{(l)}$ is the bias term for neuron $j$ in layer $l$, and $f(\cdot)$ is the activation function, typically nonlinear (e.g., sigmoid, ReLU). Equation~\eqref{eq:forward_z} makes it explicit that we sum over every incoming neuron $i$ in layer $l-1$ to form the affine pre-activation $z_j^{(l)}$.

\subsection{Loss and Objective}
\label{sec:backprop_loss_and_objective}

To keep the story linear (and aligned with \Cref{chap:mlp}), we will use a simple squared\hyp{}error objective. Let the network output be \(\mathbf{a}^{(L)}\) and let \(\mathbf{t}\) be the target (one\hyp{}hot targets for classification are fine; we do not need a separate regression/classification split yet). A standard loss is
\begin{equation}
    \mathcal{L} = \frac{1}{2} \sum_k \left( t_k - a_k^{(L)} \right)^2. \label{eq:error_function}
\end{equation}
The goal of learning is to adjust the weights \(\{w_{ij}^{(l)}\}\) to minimize \(\mathcal{L}\). Later in this chapter, we briefly note how common alternatives (notably cross\hyp{}entropy with sigmoid/softmax outputs) simplify the output\hyp{}layer error term; the backprop recursion itself does not change.

\paragraph{Challenges in weight updates}
\label{sec:backprop_challenges_in_weight_updates}

With more than one layer, the error you measure at the output does not tell each earlier weight what to do directly. Each weight affects the loss only through a chain of intermediate quantities: it shifts a pre\hyp{}activation, changes an activation, and that change fans out through downstream units.

One way to phrase the bottleneck is \emph{credit assignment}. If a small change in \(w_{ij}^{(l)}\) would make the loss smaller, we want that weight to move in that direction; if it would make the loss larger, we want it to move the other way. The chain rule gives both the sign and the magnitude, but only if we compute the right intermediate sensitivities.

Backpropagation is the bookkeeping that makes those sensitivities reusable: it defines a local error signal (the \(\delta\)'s), computes them once from the output back toward the input, and then turns them into gradients for every weight and bias.

\subsection{Notation for Layers and Neurons}
\label{sec:backprop_notation_for_layers_and_neurons}

To formalize this, we introduce the following notation:
\begin{itemize}
    \item $l$: layer index, with $l=0$ representing the input layer, and $l=L$ the output layer.
    \item $i$: neuron index in layer $l-1$.
    \item $j$: neuron index in layer $l$.
    \item $k$: neuron index in layer $L$ (output layer).
    \item $a_i^{(l)}$: activation of neuron $i$ in layer $l$.
    \item $z_j^{(l)}$: weighted input to neuron $j$ in layer $l$.
    \item $w_{ij}^{(l)}$: weight from neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$.
    \item $b_j^{(l)}$: bias of neuron $j$ in layer $l$.
    \item $f(\cdot)$: activation function.
\end{itemize}
These definitions carry directly into the forward-pass recap below, where we chain the affine map and nonlinearity across layers.
\paragraph{Notation handoff.}
Across this chapter, \(a\) denotes activations and \(z\) denotes pre-activations; this pairing is reused in later deep-learning chapters. If you jump into chapters out of order, keep \Cref{app:notation_collisions} nearby for symbol overloads.

\subsection{Forward Pass Recap}
\label{sec:backprop_forward_pass_recap}

The forward pass computes activations layer by layer:
\begin{align}
    z_j^{(l)} &= \sum_i a_i^{(l-1)} w_{ij}^{(l)} + b_j^{(l)}, \\
	    a_j^{(l)} &= f\big(z_j^{(l)}\big).
	    \label{eq:auto:lecture_4_part_i:1}
	\end{align}
	
	The output layer activations \(a_k^{(L)}\) are compared to the targets \(t_k\) to form a loss (e.g., \Cref{eq:error_function}), and backpropagation propagates that output error backward through the layers to compute every weight gradient efficiently.
	
	\begin{tcolorbox}[summarybox, title={Mini example (aligned with this chapter): MSE backprop on a two-layer MLP}]
\footnotesize
\begin{verbatim}
# Shapes: X in R^{B x d}, W1 in R^{d x h}, W2 in R^{h x 1}
# Activations: H = f(Z1), y = f(Z2)  (use a smooth f)
def step_mse(X, t, params, eta):
    W1, b1, W2, b2 = params
    B = X.shape[0]
    # Forward pass
    Z1 = X @ W1 + b1
    H  = f(Z1)
    Z2 = H @ W2 + b2
    y  = f(Z2)
    # Loss (mean squared error over batch)
    P = 0.5 * ((y - t)**2).mean()
    # Backward pass (delta = dP/dZ)
    delta2 = (y - t) * fprime(Z2) / B
    grad_W2 = H.T @ delta2
    grad_b2 = delta2.sum(axis=0)
    delta1 = (delta2 @ W2.T) * fprime(Z1)
    grad_W1 = X.T @ delta1
    grad_b1 = delta1.sum(axis=0)
    # GD step
    return (W1 - eta * grad_W1, b1 - eta * grad_b1,
            W2 - eta * grad_W2, b2 - eta * grad_b2)
\end{verbatim}
\normalsize
The elementwise product \texttt{*} mirrors the Hadamard notation from \Crefrange{eq:forward_z}{eq:forward_a}. The key technical point is the \texttt{fprime} factors: this is exactly where hard thresholds break the learning signal.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Shape ledger for an $L$-layer MLP (batch size $B$)}]
\noindent\textbf{If your gradients look ``plausible'' but training fails, check shapes first.} For each layer:
\begin{itemize}
    \item \(\mathbf{A}^{(l-1)}\in\mathbb{R}^{B\times n_{l-1}},\; \mathbf{Z}^{(l)},\boldsymbol{\delta}^{(l)}\in\mathbb{R}^{B\times n_l}\)
    \item \(\mathbf{W}^{(l)}\in\mathbb{R}^{n_{l-1}\times n_l},\; \mathbf{b}^{(l)}\in\mathbb{R}^{n_l}\)
    \item \(\partial L/\partial \mathbf{W}^{(l)} = (\mathbf{A}^{(l-1)})^\top \boldsymbol{\delta}^{(l)} / B \in \mathbb{R}^{n_{l-1}\times n_l}\)
    \item \(\partial L/\partial \mathbf{b}^{(l)} = \mathrm{batch\_mean}(\boldsymbol{\delta}^{(l)}) \in \mathbb{R}^{n_l}\)
\end{itemize}
Layers share this structure; convolutional/sequence models reuse the same calculus with different bookkeeping.
\end{tcolorbox}

\subsection{Backpropagation: Recursive Computation of Error Terms}
\label{sec:backprop_backpropagation_recursive_computation_of_error_terms}

Recall that our goal is to compute the gradient of the loss with respect to the weights in the network, specifically for weights connecting layer \( l \) to layer \( l+1 \). We denote the weight connecting neuron \( i \) in layer \( l \) to neuron \( j \) in layer \( l+1 \) as \( w_{ij}^{(l)} \).

We will continue with the squared\hyp{}error loss from \Cref{chap:mlp}:
\begin{equation}
\mathcal{L} = \frac{1}{2} \sum_{k} (t_k - a_k^{(L)})^2.
\label{eq:auto_backprop_4c440b0502}
\end{equation}
where \( t_k \) is the target output and \( a_k^{(L)} \) is the activation of output neuron \( k \). Other losses change only a few local derivatives (most notably at the output layer), but the backprop recursion and bookkeeping are the same.

To update the weights using gradient descent, we need to compute
\[
\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}.
\]

\paragraph{Chain rule decomposition}

By the chain rule, we have
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \frac{\partial \mathcal{L}}{\partial z_j^{(l+1)}} \cdot \frac{\partial z_j^{(l+1)}}{\partial w_{ij}^{(l)}}.
\label{eq:chain_rule_weight}
\end{equation}
where \( z_j^{(l+1)} \) is the weighted input to neuron \( j \) in layer \( l+1 \):
\[
z_j^{(l+1)} = \sum_i a_i^{(l)} w_{ij}^{(l)} + b_j^{(l+1)}.
\]
Here \( a_i^{(l)} \) is the activation of neuron \( i \) in layer \( l \), and \( b_j^{(l+1)} \) the bias term.

Since \( z_j^{(l+1)} \) is linear in \( w_{ij}^{(l)} \), we have
\[
\frac{\partial z_j^{(l+1)}}{\partial w_{ij}^{(l)}} = a_i^{(l)}.
\]

Thus,
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \delta_j^{(l+1)} a_i^{(l)},
\label{eq:weight_gradient}
\end{equation}
where we define the \emph{error term}
\[
\delta_j^{(l+1)}:= \frac{\partial \mathcal{L}}{\partial z_j^{(l+1)}}.
\]
Collecting the \(\delta_j^{(l+1)}\) for all neurons in layer \(l+1\) forms a vector \(\boldsymbol{\delta}^{(l+1)}\) with the same dimension as \(z^{(l+1)}\), ensuring the gradient \(\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}\) has the same shape as the weight matrix.

\paragraph{Interpretation of \(\delta_j^{(l+1)}\)}

The term \(\delta_j^{(l+1)}\) measures how sensitive the error is to changes in the net input \( a_j^{(l+1)} \). Our task reduces to computing these \(\delta\) terms for all neurons in the network.

\subsubsection{Output layer error terms}
\label{sec:backprop_output_layer_error_terms_sub}

For the output layer \( L \), the activation of neuron \( k \) is
\[
a_k^{(L)} = f\big(z_k^{(L)}\big),
\]
where \(f(\cdot)\) is the activation function.

The error term for output neuron \( k \) is
\begin{align}
\delta_k^{(L)} &= \frac{\partial \mathcal{L}}{\partial z_k^{(L)}} \\
&= \frac{\partial \mathcal{L}}{\partial a_k^{(L)}} \frac{\partial a_k^{(L)}}{\partial z_k^{(L)}} \\
&= \big(a_k^{(L)} - t_k\big) \, \phi'\big(z_k^{(L)}\big),
\label{eq:delta_output}
\end{align}
where \(\phi'\) denotes the derivative of the activation function evaluated element-wise.
For cross\hyp{}entropy with sigmoid/softmax output, \(\phi'\) cancels and \(\delta_k^{(L)} = a_k^{(L)}-t_k\); for MSE retain the factor above.

\subsubsection{Hidden layer error terms}
\label{sec:backprop_hidden_layer_error_terms_sub}

For a hidden neuron \( j \) in layer \( l \), the error term \(\delta_j^{(l)}\) depends on the error terms of the neurons in the next layer \( l+1 \) to which it connects. Using the chain rule,
\begin{align}
\delta_j^{(l)} &= \frac{\partial \mathcal{L}}{\partial z_j^{(l)}} \\
&= \sum_{k} \frac{\partial \mathcal{L}}{\partial z_k^{(l+1)}} \frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}} \\
&= \sum_{k} \delta_k^{(l+1)} \frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}}.
\label{eq:delta_hidden_chain}
\end{align}

Since
\[
z_k^{(l+1)} = \sum_m a_m^{(l)}\, w_{mk}^{(l)} + b_k^{(l+1)},
\]
and \( a_j^{(l)} = \phi\big(z_j^{(l)}\big) \), we have
\[
\frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}} = w_{jk}^{(l)} \, \phi'\big(z_j^{(l)}\big).
\]

Substituting into \eqref{eq:delta_hidden_chain} yields
\begin{equation}
\delta_j^{(l)} = \phi'\big(z_j^{(l)}\big) \sum_{k} w_{jk}^{(l)} \delta_k^{(l+1)}.
\label{eq:delta_hidden}
\end{equation}

For sigmoid activations $\phi$, the derivative simplifies to $\phi'(z_j^{(l)}) = a_j^{(l)} (1 - a_j^{(l)})$; other activations require substituting their respective derivatives in \eqref{eq:delta_hidden}.

\paragraph{Summary: Backpropagation recursion}
Backpropagation is reverse-mode automatic differentiation on the network graph. A forward pass caches intermediates; a reverse pass reuses caches to get all gradients in \(O(P)\) time (versus \(O(P^2)\) for finite differences) for \(P\) parameters. Frameworks (PyTorch/JAX/TF) automate this; the algebra below is its manual derivation.

The step-by-step output-layer derivation and the hidden-layer recursion are already established in \Cref{sec:backprop_output_layer_error_terms_sub,sec:backprop_hidden_layer_error_terms_sub}; we now focus on how those gradients are accumulated in batch and stochastic settings.

\subsection{Batch and Stochastic Gradient Descent}
\label{sec:backprop_batch_and_stochastic_gradient_descent}

Given a training set of \( N \) examples \(\{(\mathbf{x}^{(n)}, t^{(n)})\}_{n=1}^N\), the weight updates can be computed in different ways:

\begin{itemize}
    \item \textbf{Batch gradient descent:} Compute the gradient over the entire dataset and update weights once per epoch:
    \[
        \Delta w = -\frac{\eta}{N} \sum_{n=1}^N \delta^{(n)} \mathbf{x}^{(n)}.
    \]

    \item \textbf{Stochastic gradient descent (SGD):} Update weights after each training example using the instantaneous gradient \(-\eta \, \delta^{(n)} \mathbf{x}^{(n)}\). Although the updates are noisy, SGD often converges faster in practice and can escape shallow local minima.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Optimizer and stability notes}]
Start with plain SGD so you can see the learning signal clearly, then add the tools that improve stability. Momentum and Adam/AdamW (see \Cref{chap:cnn}) usually reduce sensitivity to curvature and step-size tuning. If you use weight decay, be explicit about whether it is implemented as an L2 term inside the gradient or decoupled (AdamW). For deep or ill-conditioned nets, gradient clipping can prevent explosions. For classification, cross\hyp{}entropy with the usual log\hyp{}sum\hyp{}exp stabilization is the default recipe (\Cref{chap:cnn}).
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Implementation pattern (modern practice): softmax + cross-entropy}]
\footnotesize
\begin{verbatim}
# Shapes: X in R^{B x d}, Y in R^{B x c} (one-hot)
#         W1 in R^{d x h}, W2 in R^{h x c}
def step_ce_softmax(X, Y, params, eta, wd=1e-4, p_drop=0.1):
    W1, b1, W2, b2 = params
    B = X.shape[0]
    # Forward pass
    Z1 = X @ W1 + b1
    H1 = relu(Z1)
    mask1 = (np.random.rand(*H1.shape) > p_drop).astype(H1.dtype)
    # Inverted dropout
    H1 = H1 * mask1 / (1 - p_drop)
    Z2 = H1 @ W2 + b2
    Yhat = softmax(Z2)
    # Backward pass: for softmax + CE, delta2 = Yhat - Y
    delta2 = (Yhat - Y) / B
    grad_W2 = H1.T @ delta2 + wd * W2
    grad_b2 = delta2.sum(axis=0)
    delta1 = (delta2 @ W2.T) * relu_deriv(Z1)
    # Dropout backprop: reuse mask + scale
    delta1 = delta1 * mask1 / (1 - p_drop)
    grad_W1 = X.T @ delta1 + wd * W1
    grad_b1 = delta1.sum(axis=0)
    # SGD step
    return (W1 - eta * grad_W1, b1 - eta * grad_b1,
            W2 - eta * grad_W2, b2 - eta * grad_b2)
\end{verbatim}
\normalsize
This uses the same bookkeeping as the squared\hyp{}error derivation; only the output-layer error simplifies.
\end{tcolorbox}


\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[
        >={Stealth[round, length=2.5mm, width=1.75mm]},
        node distance=1.2cm and 1.5cm,
        font=\small\sffamily,
        base/.style={draw, line width=0.9pt, rounded corners=4pt, align=center},
        var/.style={base, circle, fill=white, draw=gray!40, minimum size=12mm, inner sep=1pt},
        block/.style={base, rectangle, fill=cbBlue!18, draw=cbBlue!75!black, minimum height=12mm, minimum width=18mm},
        act/.style={base, rectangle, fill=cbBlue!8, draw=cbBlue!75!black, minimum height=12mm, minimum width=18mm},
        param/.style={base, rectangle, fill=cbGreen!18, draw=cbGreen!75!black, minimum height=10mm, minimum width=26mm},
        loss/.style={base, rectangle, fill=cbPink!12, draw=cbPink!80!black, minimum height=12mm, minimum width=18mm},
        fwd/.style={->, line width=1.3pt, draw=gray!55},
        bwd/.style={->, line width=1.1pt, draw=cbOrange!85!black, dashed, rounded corners=10pt},
        grad/.style={->, line width=1.1pt, draw=cbGreen!75!black, dashed}
    ]
        \node[var] (x) {$\mathbf{x}$};

        \node[block, right=of x] (Z1) {$\mathbf{Z}^{(1)}$};
        \node[param, above=1.5cm of Z1] (W1) {$\mathbf{W}^{(1)},\, \mathbf{b}^{(1)}$};
        \node[act, right=0.8cm of Z1] (A1) {$\mathbf{A}^{(1)}$\\[-2pt]{\scriptsize $f(\cdot)$}};

        \node[right=0.8cm of A1, font=\large\bfseries, text=gray!40] (dots) {$\cdots$};

        \node[block, right=0.8cm of dots] (ZL) {$\mathbf{Z}^{(L)}$};
        \node[param, above=1.5cm of ZL] (WL) {$\mathbf{W}^{(L)},\, \mathbf{b}^{(L)}$};
        \node[act, right=0.8cm of ZL] (AL) {$\mathbf{A}^{(L)}$\\[-2pt]{\scriptsize $\sigma(\cdot)$}};

        \node[loss, right=1.2cm of AL] (L) {$\mathcal{L}$};
        \node[var, above=1.5cm of L] (t) {$t$};

        % Cache box (manual corners; no fit/backgrounds/shadows libraries needed)
        % Raise the cache-box top so the legend sits above the parameter nodes without overlap.
        \coordinate (boxTop) at ($(W1.north)+(0,2.2cm)$);
        \coordinate (cacheNW) at ($(Z1.west |- boxTop)+(-0.6,0)$);
        \coordinate (cacheSE) at ($(AL.south east)+(0.6,-0.55)$);
        % Use fill opacity so the cache box doesn't obscure the nodes (drawn after nodes).
        \draw[draw=gray!20, dashed, rounded corners=12pt, fill=gray!5, fill opacity=0.35, draw opacity=1]
            (cacheNW) rectangle (cacheSE);
        \node[anchor=north east, font=\scriptsize\itshape, text=gray!60, inner sep=4pt]
            at (cacheSE) {Forward cache};

        % Legend (top-left inside cache box)
        \node[anchor=north west, font=\scriptsize\sffamily, fill=white, draw=gray!20, rounded corners=3pt, inner sep=5pt]
            at ($(cacheNW)+(0.25,-0.25)$) {%
            \begin{tabular}{@{}l@{\,\,}l@{}}
            \textcolor{gray!55}{\rule{6mm}{1.5pt}} & Forward \\
            \textcolor{cbOrange!85!black}{\rule[2pt]{6mm}{1.5pt}} & Backprop ($\delta$) \\
            \textcolor{cbGreen!75!black}{\rule[2pt]{6mm}{1.5pt}} & Grads ($\nabla$) \\
            \end{tabular}
        };

        % Forward pass
        \draw[fwd] (x) -- (Z1);
        \draw[fwd] (W1) -- (Z1);
        \draw[fwd] (Z1) -- (A1);
        \draw[fwd] (A1) -- (dots);
        \draw[fwd] (dots) -- (ZL);
        \draw[fwd] (WL) -- (ZL);
        \draw[fwd] (ZL) -- (AL);
        \draw[fwd] (AL) -- (L);
        \draw[fwd] (t) -- (L);

        % Backward pass
        \draw[bwd] (L.south) |- ++(0,-1.3) -|
            node[pos=0.25, below, font=\scriptsize] {$\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{(L)}}$} (AL.south);
        \draw[bwd] (AL.south) |- ++(0,-1.3) -|
            node[pos=0.25, below, font=\scriptsize] {$\boldsymbol{\delta}^{(L)}$} (ZL.south);
        \draw[bwd] (ZL.south) |- ++(0,-1.3) -|
            node[pos=0.5, below, font=\scriptsize, fill=white, inner sep=3pt] {chain rule via $W^\top, f'$} (Z1.south);
        \node[font=\scriptsize, text=cbOrange!85!black, below=0.1cm of Z1.south, xshift=0.4cm, yshift=-0.5cm] {$\boldsymbol{\delta}^{(1)}$};

        % Gradient extraction (curved to avoid overlaps)
        \draw[grad] (ZL.north) to[bend right=45]
            node[midway, right, font=\tiny, text=cbGreen!75!black, align=left, xshift=2pt]
            {$\nabla_{\mathbf{W}^{(L)}}$\\$\nabla_{\mathbf{b}^{(L)}}$} (WL.south);
        \draw[grad] (Z1.north) to[bend right=45]
            node[midway, right, font=\tiny, text=cbGreen!75!black, align=left, xshift=2pt]
            {$\nabla_{\mathbf{W}^{(1)}}$\\$\nabla_{\mathbf{b}^{(1)}}$} (W1.south);
    \end{tikzpicture}%
    }
    % Avoid dense inline math in captions; it wraps poorly in EPUB renderers.
    \caption[Computational graph for backpropagation (reverse-mode AD)]{Computational graph for a feedforward network. Backpropagation is reverse\hyp{}mode AD: the forward sweep caches intermediate values, and the reverse sweep propagates deltas while accumulating weight/bias gradients from those cached values.}
    \label{fig:backprop-computational-graph}
\end{figure}


\begin{tcolorbox}[summarybox, title={Debugging and gradient-check checklist}]
\begin{itemize}
    \item \textbf{Overfit a tiny batch:} Drive the loss near zero on a handful of samples; if you cannot, the issue is almost always code or setup, not generalization.
    \item \textbf{Finite differences on a tiny net:} Fix seeds, perturb one parameter, and compare analytic vs.\ numerical gradients. Do this before running long experiments.
    \item \textbf{Track per-layer gradient norms:} \(\|\nabla \mathbf{W}^{(l)}\|\) that vanish (all zeros) or explode are early warnings.
    \item \textbf{Assert shapes and broadcasts:} Verify \(\mathbf{Z}^{(l)}, \mathbf{A}^{(l)}, \boldsymbol{\delta}^{(l)}\) shapes and bias broadcasting explicitly.
    \item \textbf{Sanity baselines:} For a one-layer linear model, backprop should match the closed-form gradient you already know.
\end{itemize}
\end{tcolorbox}

\subsection{Backpropagation Algorithm: Brief Numerical Check}
\label{sec:backprop_backpropagation_algorithm_brief_numerical_check}

For a quick sanity check, take a tiny 2--2--1 network with sigmoid output and cross\hyp{}entropy loss. Using
\begin{align*}
\mathbf{W}^{(1)}&=\begin{bmatrix}0.5&-0.3\\0.8&0.2\end{bmatrix},\quad \mathbf{b}^{(1)}=[0.1,-0.2],\\
\mathbf{W}^{(2)}&=\begin{bmatrix}0.7\\-0.4\end{bmatrix},\quad \mathbf{b}^{(2)}=0.05,\\
\mathbf{x}&=[0.6,-1.2],\quad t=1,
\end{align*}
the forward pass yields
\[
z^{(1)}=[-0.56,-0.62],\quad
a^{(1)}=[0.3635,0.3498],\quad
z^{(2)}=0.1646,\quad
a^{(2)}=0.5411,
\]
with loss \(\mathcal{L}\approx 0.6142\). The cross\hyp{}entropy output error is \(\delta^{(2)}=a^{(2)}-t=-0.4590\). Backpropagating gives
\[
\delta^{(1)}=[-0.0743,0.0418],\quad
\nabla_{\mathbf{W}^{(2)}}=[-0.1669,-0.1605]^\top,\quad
\nabla_{\mathbf{b}^{(2)}}=-0.4590,
\]
and
\[
\nabla_{\mathbf{W}^{(1)}}=
\begin{bmatrix}
-0.0446&0.0251\\
0.0892&-0.0501
\end{bmatrix},\quad
\nabla_{\mathbf{b}^{(1)}}=[-0.0743,0.0418].
\]
Finite-difference checks on the same network match to numerical precision, validating the implementation.

\paragraph{Aside: squared-error loss (alternative)}

The remainder of this subsection sketches the classic squared-error backprop derivation as a separate reminder; it is \emph{not} a continuation of the cross\hyp{}entropy numerical check above.

The error at the output neuron is:
\begin{equation}
    e = y - t,
\label{eq:auto_backprop_b55302fe32}
\end{equation}
and the squared error is:
\begin{equation}
    \mathcal{L}_{\text{SE}} = \frac{1}{2} e^2.
\label{eq:auto_backprop_7c0067d4dd}
\end{equation}

\paragraph{Backward Propagation of Error}

Define the error term \(\delta_j\) for each neuron \( j \) as:
\begin{equation}
    \delta_j = e_j \sigma'(net_j),
\label{eq:auto_backprop_88f14b071c}
\end{equation}
where \( e_j \) is the error at neuron \( j \), and
\[
\sigma'(z) = \sigma(z)(1 - \sigma(z)).
\]
is the derivative of the sigmoid function.

For the output neuron:
\[
\delta_{\text{out}} = (y_{\text{out}} - t) \, y_{\text{out}} (1 - y_{\text{out}}).
\]

For hidden neurons, the error term is computed by backpropagating the weighted sum of the downstream error terms:
\begin{equation}
    \delta_j = y_j (1 - y_j) \sum_{k} w_{kj} \, \delta_k,
\label{eq:auto_backprop_1f3342dea6}
\end{equation}
where the sum is over neurons \( k \) in the next layer and \( w_{kj} \) denotes the weight from neuron \( j \) to neuron \( k \).

\paragraph{Weight update rule (with momentum).}
Once you have \(\delta\)'s, turning them into weight updates is straightforward. For a weight \(w_{ij}^{(l)}\) that connects unit \(i\) in layer \(l-1\) to unit \(j\) in layer \(l\), the gradient is proportional to ``input times local error'': \(a_i^{(l-1)}\delta_j^{(l)}\). With momentum, one common update is
\begin{equation}
    \Delta w_{ij}^{(l)}(n) = -\eta \, a_i^{(l-1)}(n)\,\delta_j^{(l)}(n) + \gamma \Delta w_{ij}^{(l)}(n-1),
    \label{eq:weight_update}
\end{equation}
followed by \(w_{ij}^{(l)}(n)=w_{ij}^{(l)}(n-1)+\Delta w_{ij}^{(l)}(n)\). Here \(\eta\) is the step size and \(\gamma\in[0,1)\) is the momentum coefficient. (For the first hidden layer, \(a_i^{(0)}\) is just the input feature \(x_i\).)

\paragraph{A practical reading of \(\eta\) and \(\gamma\).}
The learning rate sets the basic step scale; momentum averages recent gradient directions so updates do not zig\hyp{}zag as much across narrow valleys. If training oscillates, reduce \(\eta\); if it is painfully slow along a consistent direction, momentum can help.

\paragraph{Step-by-Step Example}

\begin{enumerate}
    \item \textbf{Initialization:} Draw weights \( w_{ij}\sim \mathcal{N}(0,\sigma^2)\) with \(\sigma\) set by He/Xavier rules; set biases to zero and \(\Delta w_{ij}(0) = 0\).
    \item \textbf{Feedforward:} Compute \( net_j \) and \( y_j \) for all neurons.
    \item \textbf{Compute output error:} Calculate \( \delta_{out} \).
    \item \textbf{Backpropagate error:} Compute \(\delta_j\) for hidden neurons.
    \item \textbf{Update weights:} Use equation \eqref{eq:weight_update} to update all weights.
    \item \textbf{Repeat:} Iterate over all training patterns until error \( E \) is below threshold or maximum epochs reached.
\end{enumerate}

\begin{tcolorbox}[summarybox, title={Mini\hyp{}batch backprop with explicit regularization}]
\textbf{Inputs:} mini\hyp{}batch \(\{(\mathbf{x}_b,\mathbf{t}_b)\}_{b=1}^B\), learning rate \(\eta\), L2 coefficient \(\lambda\), dropout keep probability \(q=1-p\).
\begin{enumerate}[leftmargin=*]
    \item \textbf{Forward pass:} propagate activations layer by layer. If you use dropout in a hidden layer, draw a mask \(\mathbf{m}\sim \operatorname{Bernoulli}(q)\), apply \(\tilde{\mathbf{a}}=\mathbf{m}\odot \mathbf{a}/q\), and cache \(\mathbf{m}\) for the backward step.
    \item \textbf{Backward pass:} compute \(\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L}\) using cached activations (and cached dropout masks) so dropped units contribute zero gradient.
    \item \textbf{Update block (per layer):}
    \[
        \mathbf{g}_\ell = \frac{1}{B}\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L} + \lambda \mathbf{W}^{(\ell)}, \qquad
        \mathbf{W}^{(\ell)} \leftarrow \mathbf{W}^{(\ell)} - \eta\, \mathbf{g}_\ell.
    \]
    Biases skip the weight-decay term. With Adam or SGD+momentum, \(\mathbf{g}_\ell\) is what you feed into the optimizer update so regularization stays explicit.
\end{enumerate}
\end{tcolorbox}

\paragraph{Remarks}

\begin{itemize}
    \item Monitor the training error over epochs; a plateau may indicate the need to adjust learning rate or introduce regularization.
    \item Shuffle training patterns between epochs when using SGD to avoid cyclic behaviors.
    \item Always track validation error to detect overfitting and decide when to stop training.
\end{itemize}

\subsection{Training Procedure and Epochs in Multi-Layer Perceptrons}
\label{sec:backprop_training_procedure_and_epochs_in_multi_layer_perceptrons}

An epoch is one complete pass over the training set. In practice you almost never update one example at a time; you shuffle the data and iterate over mini\hyp{}batches so the gradient estimates are both noisy enough to explore and cheap enough to compute.

\begin{enumerate}
    \item Shuffle the training examples (or shuffle within class strata for imbalanced problems).
    \item For each mini\hyp{}batch: run a forward pass, compute the loss, run a backward pass, and update parameters.
    \item At the end of the epoch: evaluate on a validation split, checkpoint the best model, and apply an early\hyp{}stopping rule if needed.
\end{enumerate}

After each epoch, look at both training and validation curves. A training loss that keeps falling while validation stalls is your cue to regularize, stop, or change capacity.

\paragraph{Common habits that prevent wasted runs.}
\begin{itemize}
    \item Start by overfitting a tiny batch and running a gradient check (the checklist box earlier in the chapter).
    \item Log the random seed and the full optimizer recipe; without it, ``it did not train'' is not a reproducible diagnosis.
    \item If you change one thing (activation, initialization, step size), keep everything else fixed so you can attribute cause and effect.
\end{itemize}

\subsection{Role and Design of Hidden Layers}
\label{sec:backprop_role_and_design_of_hidden_layers}

Hidden layers are where an MLP earns its flexibility. They do not just add parameters; they add intermediate representations, so different units can specialize and a later layer can combine those specializations.

\paragraph{Design questions you actually have to answer.}
\begin{itemize}
    \item \textbf{Depth vs.\ width:} do you want a few wide layers or many narrow layers?
    \item \textbf{Capacity vs.\ data:} how much flexibility can your dataset support before you overfit?
    \item \textbf{Activation choice:} will gradients flow (ReLU family), or will they saturate (sigmoid/tanh) for the scale of your pre\hyp{}activations?
\end{itemize}

\paragraph{A simple, defensible starting point.}
\begin{itemize}
    \item Pick one or two hidden layers and a moderate width, then let validation performance tell you whether you need more capacity.
    \item Choose an activation that keeps gradients alive (ReLU or a leaky ReLU are common defaults in practice).
    \item Use early stopping and weight decay as your first line of defense against overfitting.
\end{itemize}

\paragraph{Trade-offs to keep in mind.}
\begin{itemize}
    \item \textbf{Too much capacity:} training loss drops quickly, validation does not; the model memorizes details you did not intend it to learn.
    \item \textbf{Too little capacity:} both training and validation plateau early; the model cannot represent the function you are asking for.
\end{itemize}

\subsection{Case Study: Learning the Function \texorpdfstring{\( y = x \sin x \)}{y = x sin x}}
\label{sec:backprop_case_study_learning_the_function_y_x_x_y_x_sin_x}

Consider the problem of training an MLP to approximate the function
\[
y = x \sin x.
\]

\paragraph{Setup.}
\begin{itemize}
    \item Generate a dataset of input-output pairs \(\{(x_i, y_i)\}\) where \(y_i = x_i \sin x_i\).
    \item Use this dataset to train an MLP regressor.
    \item Evaluate the network's ability to generalize by testing on inputs not seen during training.
\end{itemize}

\paragraph{What to look for.}
\begin{itemize}
    \item \textbf{Capacity vs.\ fit:} if the model is too small, the best\hyp{}fit curve will miss structure; if it is too large for your data density, it will fit noise or interpolation artifacts.
    \item \textbf{Activation effects:} saturated activations can make learning look ``stuck'' even when the model has enough parameters.
    \item \textbf{Data density:} uniform sampling over \([-3\pi,3\pi]\) reveals whether your model interpolates smoothly between points or produces oscillatory artifacts.
\end{itemize}

\paragraph{Practical notes.}
\begin{itemize}
    \item This is a regression problem, not a classification problem.
    \item The target is nonlinear and oscillatory; underfitting and overfitting are both easy to see in a plot.
    \item If you report performance, report it as a reproducible experiment (seed, split, optimizer recipe). Avoid quoting a single ``typical'' number unless you can regenerate it.
\end{itemize}

\subsection{Applications of Multi-Layer Perceptrons}
\label{sec:backprop_applications_of_multi_layer_perceptrons}

MLPs are often used as general-purpose function approximators, either as standalone models or as learnable blocks inside larger systems. Common examples include:

\begin{itemize}
    \item \textbf{Signal processing:} Noise reduction, filtering, and feature extraction.
    \item \textbf{Weather forecasting:} Modeling complex atmospheric patterns.
    \item \textbf{Data compression:} Dimensionality reduction and encoding.
    \item \textbf{Pattern recognition:} Handwriting recognition, face detection.
    \item \textbf{Financial market prediction:} Time series forecasting and anomaly detection.
    \item \textbf{Image recognition:} Object detection and classification.
    \item \textbf{Voice recognition:} Speech-to-text and speaker identification.
\end{itemize}

\paragraph{Summary.}
The details change across domains, but the pattern is the same: a model that composes nonlinear units becomes a flexible mapping, and backpropagation is what makes that mapping trainable.

\subsection{Limitations of Multi-Layer Perceptrons}
\label{sec:backprop_limitations_of_multi_layer_perceptrons}

Despite their versatility, MLPs have several limitations that practitioners must be aware of:

\begin{itemize}
\item \textbf{Convergence to local minima:} Due to the non-convex nature of the loss surface, training may converge to different local minima depending on the initial weights.
    \item \textbf{Sensitivity to initialization:} Different random initializations can lead to significantly different outcomes.
    \item \textbf{Hyperparameter tuning:} Learning rates, momentum, and regularization require careful tuning for stable convergence.
\end{itemize}

\subsection{Conclusion of Multi-Layer Perceptron Derivations}
\label{sec:backprop_conclusion_of_multi_layer_perceptron_derivations}

At this point we can summarize backpropagation as an algorithmic recipe: a forward pass that produces and caches intermediate quantities, followed by a backward pass that reuses those caches to produce gradients for every layer.

\paragraph{Backpropagation algorithm recap.}
For a network with \(L\) layers, it is often convenient to write the forward pass in matrix form:
\[
\mathbf{Z}^{(l)} = \mathbf{A}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{1}(\mathbf{b}^{(l)})^\top,\quad \mathbf{A}^{(l)} = \phi^{(l)}(\mathbf{Z}^{(l)}),
\]
where \(\mathbf{W}^{(l)}\) and \(\mathbf{b}^{(l)}\) are the weights and biases of layer \( l \), \(\mathbf{A}^{(l-1)}\) is the previous layer activation (rows are samples), and \(\phi^{(l)}\) is the activation function.

The error term at layer \( l \) is defined as:
\[
\boldsymbol{\delta}^{(l)} = \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(l)}}.
\]

Using the chain rule, the error terms propagate backward as:
\begin{align}
\boldsymbol{\delta}^{(L)} &= \nabla_{\mathbf{A}^{(L)}} \mathcal{L} \odot \phi^{(L)\prime}(\mathbf{Z}^{(L)}), \\
\boldsymbol{\delta}^{(l-1)} &= \left(\boldsymbol{\delta}^{(l)}(\mathbf{W}^{(l)})^\top\right) \odot \phi^{(l-1)\prime}(\mathbf{Z}^{(l-1)}), \quad l = L, \ldots, 2,
    \label{eq:auto:lecture_4_part_i:2}
\end{align}
where \(\odot\) denotes element-wise multiplication and \(\phi^{(l)\prime}\) is the derivative of the activation function at layer \( l \).

The gradients of the loss with respect to the parameters are then:
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} &= (\mathbf{A}^{(l-1)})^\top \boldsymbol{\delta}^{(l)}, \label{eq:grad_W}\\
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}} &= \mathbf{1}^\top \boldsymbol{\delta}^{(l)}. \label{eq:grad_b}
\end{align}

These gradients are used in gradient-based optimization methods (e.g., stochastic gradient descent) to update the parameters and minimize the loss. \Cref{fig:lec4_backprop_flow} complements the algebra by showing how cached activations (blue) line up with the backward error signals (orange) in a simple two-layer network.

\begin{figure}[h]
    \centering
    \ifdefined\HCode
    % EPUB/HTML: use a simplified version of the original "wide" diagram, but with
    % fewer colors and without tiny per-edge weight labels (which rasterize poorly).
    \begin{tikzpicture}[
        >=Stealth,
        font=\small\sffamily,
        node/.style={circle, draw=black!50, line width=0.9pt, minimum size=12mm, fill=white},
        fwd/.style={->, line width=1.4pt, draw=cbBlue!80!black},
        bwd/.style={->, line width=1.4pt, draw=black!55, dashed},
        lbl/.style={font=\footnotesize\sffamily, text=black!70},
        background rectangle/.style={fill=white}, show background rectangle
    ]

        % Forward nodes (inputs -> hidden -> output)
        % Shift right to leave a clean legend margin on the left.
        \node[node] (x1) at (0.8,0.9) {$x_1$};
        \node[node] (x2) at (0.8,-0.9) {$x_2$};
        \node[node] (h1) at (2.6,0.9) {$a_1^{(1)}$};
        \node[node] (h2) at (2.6,-0.9) {$a_2^{(1)}$};
        \node[node] (y)  at (5.2,0) {$a^{(2)}$};

        % Forward connections (no per-edge weights in EPUB)
        \draw[fwd] (x1) -- (h1);
        \draw[fwd] (x1) -- (h2);
        \draw[fwd] (x2) -- (h1);
        \draw[fwd] (x2) -- (h2);
        \draw[fwd] (h1) -- (y);
        \draw[fwd] (h2) -- (y);

        % Backward/error signals (labels placed away from nodes to avoid overlap)
        \node[lbl, anchor=west] (d2) at ($(y)+(0.9,1.05)$) {$\boldsymbol{\delta}^{(2)}$};
        \node[lbl, anchor=west] (d1) at ($(y)+(0.9,-1.3)$) {$\boldsymbol{\delta}^{(1)}_1$};
        \node[lbl, anchor=west] (d0) at ($(y)+(0.9,-2.1)$) {$\boldsymbol{\delta}^{(1)}_2$};

        \draw[bwd] ($(y)+(0,0.55)$) -- (d2);
        \draw[bwd] ($(y)+(-0.1,-0.2)$).. controls (5.0,-0.85) and (4.6,-1.1).. (d1);
        \draw[bwd] ($(y)+(-0.1,-0.2)$).. controls (5.0,-1.25) and (4.6,-1.65).. (d0);

        % No in-figure legend for EPUB: small labels tend to overlap after rasterization.
        % The caption explains the color/linestyle mapping instead.
    \end{tikzpicture}
    \else
    \begin{tikzpicture}[
        >=Stealth,
        font=\small\sffamily,
        node distance=1.55cm and 2.0cm,
        var/.style={circle, draw=gray!60, line width=0.7pt, minimum size=10mm, fill=cbBlue!12, inner sep=0pt},
        hnode/.style={circle, draw=gray!60, line width=0.7pt, minimum size=10mm, fill=cbGreen!12, inner sep=0pt},
        outnode/.style={circle, draw=gray!60, line width=0.7pt, minimum size=10.5mm, fill=cbPink!15, inner sep=0pt},
        fwd/.style={->, line width=1.0pt, draw=cbBlue!80!black},
        bwd/.style={->, line width=1.0pt, draw=cbOrange!85!black, dashed},
        wlab/.style={font=\scriptsize, text=cbBlue!80!black},
        dlab/.style={font=\scriptsize, text=cbOrange!85!black}
    ]
        \node[var] (x1) {\(x_1\)};
        \node[var, below=0.9cm of x1] (x2) {\(x_2\)};
        \node[hnode, right=2.1cm of x1] (h1) {\(a_1^{(1)}\)};
        \node[hnode, below=0.9cm of h1] (h2) {\(a_2^{(1)}\)};
        \node[outnode, right=2.2cm of h1] (y) {\(a^{(2)}\)};

        \draw[fwd] (x1) -- node[above, sloped, wlab]{\(w_{11}^{(1)}\)} (h1);
        \draw[fwd] (x1) -- node[above, sloped, wlab]{\(w_{21}^{(1)}\)} (h2);
        \draw[fwd] (x2) -- node[below, sloped, wlab]{\(w_{12}^{(1)}\)} (h1);
        \draw[fwd] (x2) -- node[below, sloped, wlab]{\(w_{22}^{(1)}\)} (h2);
        \draw[fwd] (h1) -- node[above, wlab]{\(w_{1}^{(2)}\)} (y);
        \draw[fwd] (h2) -- node[below, wlab]{\(w_{2}^{(2)}\)} (y);

        \draw[bwd] (y) -- ++(0,1.2) node[right, xshift=0.1cm, dlab]{\(\delta^{(2)}\)};
        \draw[bwd] (h1) -- ++(0,-1.2) node[right, xshift=0.1cm, dlab]{\(\delta_1^{(1)}\)};
        \draw[bwd] (h2) -- ++(0,-1.2) node[right, xshift=0.1cm, dlab]{\(\delta_2^{(1)}\)};

        % Keep legend labels *inside* the diagram bounds; otherwise rasterizers can clip them.
        \path (current bounding box.north west) ++(2mm,-2mm)
            node[anchor=north west, font=\scriptsize, text=cbBlue!80!black] {forward};
        \path (current bounding box.north west) ++(2mm,-7mm)
            node[anchor=north west, font=\scriptsize, text=cbOrange!85!black] {backward};
    \end{tikzpicture}
    \fi
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Forward (blue) and backward (orange) flows for a two-layer MLP. Cached activations and layerwise deltas travel along these arrows; backward signals use next-layer weights and activation derivatives.}
    \label{fig:lec4_backprop_flow}
\end{figure}


\paragraph{Example Execution}

Before you scale this to larger models, run one tiny network end-to-end and check each ingredient: compute the forward values, compute the output \(\delta\), push it one layer back, and confirm one gradient numerically. The explicit numbers in \Cref{sec:backprop_backpropagation_algorithm_brief_numerical_check} can serve as a reference trace when you debug your own implementation.

\paragraph{Remarks on Convergence and Practical Considerations}

Backpropagation gives the \emph{right} gradients; whether those gradients turn into learning depends on a handful of design and optimization choices. When training stalls or becomes unstable, the usual levers are:
\begin{itemize}
    \item \textbf{Initialization scale:} keep pre\hyp{}activations in a regime where derivatives are not all near zero (or wildly large).
    \item \textbf{Activation choice:} saturation (sigmoid/tanh) and dead regions (ReLU) show up directly as weak gradients.
    \item \textbf{Step size and schedule:} too large diverges, too small crawls; schedules and momentum smooth the path.
    \item \textbf{Regularization:} weight decay and dropout trade training fit for generalization.
    \item \textbf{Optimizer details:} momentum/Adam change the effective step direction and can rescue poorly conditioned problems.
\end{itemize}

We return to these practicalities in the later deep\hyp{}learning chapters; for now, it helps to see the canonical activation choices in one place.

\paragraph{Comparing canonical nonlinearities}

With the MLP and backpropagation machinery in place, it is useful to compare the most common nonlinearities side-by-side. \Cref{fig:lec4-activations} overlays the step, sigmoid, tanh, and ReLU curves so saturation regions and derivative behavior are visually apparent in one view.

\begin{figure}[t]
    \centering
\begin{tikzpicture}[background rectangle/.style={fill=white}, show background rectangle]
        \begin{groupplot}[
            group style={group size=2 by 2, horizontal sep=1.2cm, vertical sep=1.0cm},
            width=0.42\linewidth,
            height=0.30\linewidth,
            axis lines=middle,
            xmin=-3, xmax=3,
            samples=200,
            tick label style={font=\scriptsize},
            label style={font=\scriptsize},
            title style={font=\scriptsize, align=center},
            axis background/.style={fill=white},
        ]
        \nextgroupplot[
            title={Step},
            ymin=-0.1, ymax=1.1,
            ytick={0,1},
        ]
            \addplot[cbBlue, thick, domain=-3:0] {0};
            \addplot[cbBlue, thick, domain=0:3] {1};
            \addplot[cbBlue, dashed, domain=-3:3] {0};
        \nextgroupplot[
            title={Sigmoid},
            xmin=-4, xmax=4,
            ymin=-0.1, ymax=1.1,
            ytick={0,0.5,1},
        ]
            \addplot[cbOrange, thick, domain=-4:4] {1/(1+exp(-x))};
            \addplot[cbOrange, dashed, domain=-4:4] {1/(1+exp(-x))*(1-1/(1+exp(-x)))};
        \nextgroupplot[
            title={tanh},
            ymin=-1.1, ymax=1.1,
            ytick={-1,0,1},
        ]
            \addplot[cbGreen, thick, domain=-3:3] {tanh(x)};
            \addplot[cbGreen, dashed, domain=-3:3] {1 - tanh(x)^2};
        \nextgroupplot[
            title={ReLU},
            ymin=-0.2, ymax=2.2,
            ytick={0,1,2},
        ]
            \addplot[cbPink, thick, domain=-3:0] {0};
            \addplot[cbPink, thick, domain=0:3] {x};
            \addplot[cbPink, dashed, domain=-3:0] {0};
            \addplot[cbPink, dashed, domain=0:3] {1};
        \end{groupplot}
    \ensuretikzbackgroundlayers
\end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Canonical activation functions on a common axis. Solid curves show the activation; dashed curves show its derivative.}
    \label{fig:lec4-activations}
\end{figure}


For reference, \(\sigma'(z)=\sigma(z)\bigl(1-\sigma(z)\bigr)\), \(\tanh'(z)=1-\tanh^2(z)\), and the ReLU derivative is \(0\) for negative inputs and \(1\) for positive inputs (take \(0\) at the origin).

\paragraph{Trade-offs}
While some activation functions are inspired by biological neurons, others are chosen for mathematical convenience and training efficiency. Sigmoid and tanh saturate at large magnitude inputs, which slows gradients in deep networks. ReLU avoids saturation on the positive side but can produce ``dying ReLUs'' when biases push units negative and the gradients become zero; if many units stall, use He initialization, reduce the learning rate, or swap to a leaky ReLU with a small negative slope (e.g., 0.01).
At this point the core backpropagation story for MLPs is complete. The remainder of the chapter collects practical stability habits and a short set of takeaways you can use when training starts behaving badly.

\clearpage

    \begin{tcolorbox}[summarybox, title={Key takeaways}]
    \textbf{If you remember only a few things}
    \begin{itemize}
        \item Backpropagation is the chain rule organized: cache \(\mathbf{Z}^{(l)}\) and \(\mathbf{A}^{(l)}\), compute \(\boldsymbol{\delta}^{(L)}\) at the output, then propagate \(\boldsymbol{\delta}\) backward with \((\mathbf{W}^{(l)})^\top\) and \(f'(\mathbf{Z}^{(l)})\).
        \item The \(\delta\)'s are not mysterious: they are sensitivities (\(\partial \mathcal{L}/\partial \mathbf{Z}^{(l)}\)) that you reuse to form every weight and bias gradient.
        \item For softmax + cross\hyp{}entropy, the output-layer error simplifies to \((\hat{\mathbf{Y}}-\mathbf{Y})/B\), which is why that pairing is so popular in code.
        \item When training misbehaves, debug like an engineer: overfit a tiny batch, run a finite-difference check, then inspect per-layer gradient norms and shapes.
    \end{itemize}
    \medskip
    \textbf{Common pitfalls}
    \begin{itemize}
        \item Dropping a transpose: \((\mathbf{W}^{(l)})^\top\) is easy to miss and produces gradients that look ``reasonable'' but train the wrong model.
        \item Mixing up pre\hyp{}activations and activations: \(f'(\mathbf{Z})\) depends on what you cache.
        \item Losing the batch scaling: decide whether you average over the batch (\(/B\)) and do it consistently.
        \item Silent broadcasting mistakes: bias terms and per-example deltas can accidentally align in NumPy/PyTorch and hide a bug.
    \end{itemize}
    \end{tcolorbox}

    \begin{tcolorbox}[summarybox, title={Practical early stopping and checkpointing}]
    \begin{itemize}
        \item Keep a validation split separate from the training mini\hyp{}batches, and record \(L_{\text{val}}^{(e)}\) after each epoch.
        \item Stop when \(L_{\text{val}}\) has not improved for \(k\) consecutive epochs (a patience of \(k\in[5,10]\) is a common starting point). If the curve is noisy, require a small minimum improvement before you reset patience.
        \item Always checkpoint the parameters that achieved the best validation score and restore them before testing. If training is noisy, averaging the last few checkpoints can stabilize performance.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Derivation closure: implement, cache, fail-fast}]
\begin{itemize}
    \item \textbf{Implement:} write one clean layer routine \((\mathbf{A}^{(l-1)}, \mathbf{W}^{(l)}, \mathbf{b}^{(l)}) \mapsto (\mathbf{Z}^{(l)}, \mathbf{A}^{(l)})\), then reuse it for every layer.
    \item \textbf{Cache:} store what backprop needs later (at minimum \(\mathbf{A}^{(l-1)}\) and \(\mathbf{Z}^{(l)}\)). Do not recompute forward quantities during the backward pass.
    \item \textbf{Fail fast:} gradient-check one tiny network and overfit one tiny batch before you trust results on a real dataset.
    \item \textbf{Make runs comparable:} record seeds, optimizer, schedule, and stopping rule. The reporting template in \Cref{app:repro_standards} helps keep comparisons honest.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item \textbf{A full trace on paper:} pick a tiny network and write out the forward values and \(\delta\)'s by hand, then match them against a short script.
    \item \textbf{Finite differences:} implement a gradient check for one layer and report the worst relative error; track down the first mismatch.
    \item \textbf{Activation choice:} run the same task with sigmoid/tanh/ReLU and compare gradient norms by layer; relate what you see to \Cref{fig:lec4-activations}.
    \item \textbf{Early stopping:} train until overfitting is visible, then add early stopping and report how the best checkpoint changes.
\end{itemize}
    \end{tcolorbox}
    \medskip
    \noindent\textbf{If you are skipping ahead.} Keep one practical habit: do a gradient check on a tiny case before scaling up. That discipline mirrors residual checks in \Cref{chap:symbolic} and prevents most wasted training runs in later deep chapters.

    \medskip
    \paragraph{Where we head next.} \Cref{chap:rbf} introduces radial basis function networks, an alternative nonlinear route where hidden features are mostly fixed and only the output layer is solved linearly. This provides a clean contrast to end-to-end backpropagation.

    \nocite{Rumelhart1986, Haykin2009}

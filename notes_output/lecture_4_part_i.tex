% Chapter 7
\section{Backpropagation Learning in Multi-Layer Perceptrons}\label{chap:backprop}
\graphicspath{{assets/lec4/}}

Building on the two\hyp{}neuron derivation in \Cref{chap:mlp}, we scale the same idea to an \(L\)\hyp{}layer network. Conceptually it is still the chain rule; what changes is the accounting. We compute layerwise error signals (the \(\delta\)'s), reuse them to obtain every weight and bias gradient, and organize the work into a backward sweep you can implement and debug. The goal is practical: you should be able to trace one forward pass, write down one backward pass, and know exactly which stored intermediate values each gradient depends on. By the end of this chapter, the backpropagation algorithm should feel like a concrete procedure rather than a black box.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
\begin{itemize}
    \item Derive the layerwise \(\boldsymbol{\delta}\) recursion for an \(L\)\hyp{}layer MLP and use it to write gradients for \(\mathbf{W}^{(l)}\) and \(\mathbf{b}^{(l)}\).
    \item Keep a consistent ``shape ledger'' for activations, pre\hyp{}activations, and error signals so vectorized code matches the algebra.
    \item Run one tiny numeric trace and one finite\hyp{}difference gradient check to confirm your implementation before scaling up.
    \item Recognize the two common output\hyp{}layer patterns: squared error (general) and softmax + cross\hyp{}entropy (simplified \(\delta\)).
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Treat the network as a computation you can trace. Cache what the forward pass computes, then sweep backward once and reuse the same local error signals to update every parameter.
\end{tcolorbox}

\subsection{Context and Motivation}
\label{sec:backprop_context_and_motivation}

Multi\hyp{}layer networks raise a specific question: \emph{How do we update the weights across multiple layers when the only explicit error signal is at the output?} In a single\hyp{}layer perceptron, the output error touches the weights directly; in a deep network, a change in one layer propagates through subsequent layers and alters the output in a nonlinear, intertwined way.

Shallow networks (one hidden layer) already move beyond linear separability, but more complex tasks demand deeper hierarchies of features. The multi\hyp{}layer perceptron stacks these layers to learn richer decision boundaries, and backpropagation is the mechanism that makes that depth trainable.

\begin{tcolorbox}[summarybox, title={Intuition: backprop as a controlled ripple effect}]
Changing one early weight can only affect the loss by moving the intermediate quantities it feeds into. Backpropagation does not introduce a new idea; it is the chain rule written in an efficient order: run the forward pass once, cache the intermediates you will need, then flow sensitivities backward so each layer gets its own local error signal. Identities like \(\sigma'(p)=y(1-y)\) (from \Cref{chap:mlp}) are valuable because they let you compute a derivative from cached forward values.
\end{tcolorbox}

\paragraph{Implementation lens.}
A practical MLP rarely updates one coordinate at a time; gradients are treated as full vectors so every weight moves coherently. Backpropagation is what turns a multilayer diagram into a trainable model: it reuses local error signals so you can compute \emph{all} gradients efficiently, making it practical to learn hidden representations (not just tune a final linear layer) while keeping the same validation\(\rightarrow\)audit discipline (learning curves, early stopping, and slice checks---performance broken down by meaningful subgroups/conditions).

\subsection{Problem Setup}
\label{sec:backprop_problem_setup}

Consider a multi-layer perceptron with layers indexed by $l = 0, 1, \ldots, L$, where $l=0$ is the input layer and $L$ is the output layer. Each layer $l$ contains neurons indexed by $i$, and the output of neuron $i$ in layer $l$ is denoted by $a_i^{(l)}$. The input to this neuron before activation is denoted by $z_i^{(l)}$. The weights connecting neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$ are denoted by $w_{ij}^{(l)}$.

The forward pass through the network is given by:
\begin{align}
    z_j^{(l)} &= \sum_i a_i^{(l-1)} w_{ij}^{(l)} + b_j^{(l)}, \label{eq:forward_z} \\
    a_j^{(l)} &= f\big(z_j^{(l)}\big), \label{eq:forward_a}
\end{align}
where $b_j^{(l)}$ is the bias term for neuron $j$ in layer $l$, and $f(\cdot)$ is the activation function, typically nonlinear (e.g., sigmoid, ReLU). Equation~\eqref{eq:forward_z} makes it explicit that we sum over every incoming neuron $i$ in layer $l-1$ to form the affine pre-activation $z_j^{(l)}$.

\subsection{Loss and Objective}
\label{sec:backprop_loss_and_objective}

To keep the algebra uncluttered (and aligned with \Cref{chap:mlp}), we use a simple squared\hyp{}error objective. Let the network output be \(\mathbf{a}^{(L)}\) and let \(\mathbf{t}\) be the target. For classification, \(\mathbf{t}\) is often one\hyp{}hot (a vector with a 1 at the correct class index and 0 elsewhere). For a single example, a standard choice is the half-squared error:
\begin{equation}
    \mathcal{L} = \frac{1}{2} \sum_k \left( t_k - a_k^{(L)} \right)^2. \label{eq:error_function}
\end{equation}
It is often helpful to name the \emph{error} explicitly. Define the componentwise output error as
\(\,e_k := a_k^{(L)} - t_k\,\) and the error vector \(\mathbf{e}:=\mathbf{a}^{(L)}-\mathbf{t}\). Then
\(\mathcal{L}=\tfrac12\sum_k e_k^2=\tfrac12\|\mathbf{e}\|_2^2\), and the derivative you need to start the backward pass is
\(\partial \mathcal{L}/\partial a_k^{(L)} = e_k\) (squared error removes the sign inside \(\mathcal{L}\), but the sign reappears in the derivative).
When you train with a mini\hyp{}batch, you typically use the \emph{mean} loss over the batch; this introduces a factor \(1/B\) in the gradients, which you can treat as part of the effective step size.
The goal of learning is to adjust the weights \(\{w_{ij}^{(l)}\}\) to minimize \(\mathcal{L}\). Later in this chapter, we briefly note how common alternatives (notably cross\hyp{}entropy with sigmoid/softmax outputs) simplify the output\hyp{}layer error term; the backprop recursion itself does not change.

\paragraph{Challenges in weight updates}
\label{sec:backprop_challenges_in_weight_updates}

With more than one layer, the error you measure at the output does not tell each earlier weight what to do directly. Each weight affects the loss only through a chain of intermediate quantities: it shifts a pre\hyp{}activation, changes an activation, and that change fans out through downstream units.

One way to phrase the bottleneck is \emph{credit assignment}. If a small change in \(w_{ij}^{(l)}\) would make the loss smaller, we want that weight to move in that direction; if it would make the loss larger, we want it to move the other way. The chain rule gives both the sign and the magnitude, but only if we compute the right intermediate sensitivities.

Backpropagation is the organization that makes those sensitivities reusable: it defines a local error signal (the \(\delta\)'s), computes them once from the output back toward the input, and then turns them into gradients for every weight and bias.

\subsection{Notation for Layers and Neurons}
\label{sec:backprop_notation_for_layers_and_neurons}

To formalize this, we introduce the following notation:
\begin{itemize}
    \item $l$: layer index, with $l=0$ representing the input layer, and $l=L$ the output layer.
    \item $i$: neuron index in layer $l-1$.
    \item $j$: neuron index in layer $l$.
    \item $k$: neuron index in layer $L$ (output layer).
    \item $a_i^{(l)}$: activation of neuron $i$ in layer $l$.
    \item $z_j^{(l)}$: weighted input to neuron $j$ in layer $l$.
    \item $w_{ij}^{(l)}$: weight from neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$.
    \item $b_j^{(l)}$: bias of neuron $j$ in layer $l$.
    \item $f(\cdot)$: activation function.
\end{itemize}
These definitions carry directly into the forward-pass recap below, where we chain the affine map and nonlinearity across layers.
\paragraph{Notation handoff.}
Across this chapter, \(a\) denotes activations and \(z\) denotes pre-activations; this pairing is reused in later deep-learning chapters. If you jump into chapters out of order, keep \Cref{app:notation_collisions} nearby for symbol overloads.

\subsection{Forward Pass Recap}
\label{sec:backprop_forward_pass_recap}

The forward pass computes activations layer by layer:
\begin{align}
    z_j^{(l)} &= \sum_i a_i^{(l-1)} w_{ij}^{(l)} + b_j^{(l)}, \\
	    a_j^{(l)} &= f\big(z_j^{(l)}\big).
	    \label{eq:auto:lecture_4_part_i:1}
	\end{align}
	
	The output layer activations \(a_k^{(L)}\) are compared to the targets \(t_k\) to form a loss (e.g., \Cref{eq:error_function}), and backpropagation propagates that output error backward through the layers to compute every weight gradient efficiently.
	
	\begin{tcolorbox}[summarybox, title={Mini example (aligned with this chapter): MSE backprop on a two-layer MLP}]
\footnotesize
\begin{verbatim}
# Shapes: X in R^{B x d}, W1 in R^{d x h}, W2 in R^{h x 1}
# Activations: H = f(Z1), y = f(Z2)  (use a smooth f)
def step_mse(X, t, params, eta):
    W1, b1, W2, b2 = params
    B = X.shape[0]
    # Forward pass
    Z1 = X @ W1 + b1
    H  = f(Z1)
    Z2 = H @ W2 + b2
    y  = f(Z2)
    # Loss (mean squared error over batch)
    P = 0.5 * ((y - t)**2).mean()
    # Backward pass (delta = dP/dZ)
    delta2 = (y - t) * fprime(Z2) / B
    grad_W2 = H.T @ delta2
    grad_b2 = delta2.sum(axis=0)
    delta1 = (delta2 @ W2.T) * fprime(Z1)
    grad_W1 = X.T @ delta1
    grad_b1 = delta1.sum(axis=0)
    # GD step
    return (W1 - eta * grad_W1, b1 - eta * grad_b1,
            W2 - eta * grad_W2, b2 - eta * grad_b2)
\end{verbatim}
\normalsize
The elementwise product \texttt{*} mirrors the Hadamard (element\hyp{}by\hyp{}element) product notation from \Crefrange{eq:forward_z}{eq:forward_a}. The key technical point is the \texttt{fprime} factors: this is exactly where hard thresholds break the learning signal.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Shape ledger for an $L$-layer MLP (batch size $B$)}]
\noindent\textbf{If your gradients look ``plausible'' but training fails, check shapes first.} For each layer:
\begin{itemize}
    \item \(\mathbf{A}^{(l-1)}\in\mathbb{R}^{B\times n_{l-1}},\; \mathbf{Z}^{(l)},\boldsymbol{\delta}^{(l)}\in\mathbb{R}^{B\times n_l}\)
    \item \(\mathbf{W}^{(l)}\in\mathbb{R}^{n_{l-1}\times n_l},\; \mathbf{b}^{(l)}\in\mathbb{R}^{n_l}\)
    \item \(\partial L/\partial \mathbf{W}^{(l)} = (\mathbf{A}^{(l-1)})^\top \boldsymbol{\delta}^{(l)} / B \in \mathbb{R}^{n_{l-1}\times n_l}\)
    \item \(\partial L/\partial \mathbf{b}^{(l)} = \mathrm{batch\_mean}(\boldsymbol{\delta}^{(l)}) \in \mathbb{R}^{n_l}\)
\end{itemize}
Layers share this structure; convolutional/sequence models reuse the same calculus with different indexing and shape conventions.
\end{tcolorbox}

\subsection{Backpropagation: Recursive Computation of Error Terms}
\label{sec:backprop_backpropagation_recursive_computation_of_error_terms}

Our goal is the gradient of the loss with respect to each weight \(w_{ij}^{(l)}\) connecting layer \(l\) to layer \(l+1\), i.e., the weight from neuron \(i\) in layer \(l\) to neuron \(j\) in layer \(l+1\).

We will continue with the squared\hyp{}error loss from \Cref{chap:mlp}:
\begin{equation}
\mathcal{L} = \frac{1}{2} \sum_{k} (t_k - a_k^{(L)})^2.
\label{eq:auto_backprop_4c440b0502}
\end{equation}
where \( t_k \) is the target output and \( a_k^{(L)} \) is the activation of output neuron \( k \). Other losses change only a few local derivatives (most notably at the output layer), but the backprop recursion and overall structure are the same.

To update the weights using gradient descent, we need to compute
\[
\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}.
\]

\paragraph{Chain rule decomposition}

By the chain rule, we have
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \frac{\partial \mathcal{L}}{\partial z_j^{(l+1)}} \cdot \frac{\partial z_j^{(l+1)}}{\partial w_{ij}^{(l)}}.
\label{eq:chain_rule_weight}
\end{equation}
where \( z_j^{(l+1)} \) is the weighted input to neuron \( j \) in layer \( l+1 \):
\[
z_j^{(l+1)} = \sum_i a_i^{(l)} w_{ij}^{(l)} + b_j^{(l+1)}.
\]
Here \( a_i^{(l)} \) is the activation of neuron \( i \) in layer \( l \), and \( b_j^{(l+1)} \) the bias term.

Since \( z_j^{(l+1)} \) is linear in \( w_{ij}^{(l)} \), we have
\[
\frac{\partial z_j^{(l+1)}}{\partial w_{ij}^{(l)}} = a_i^{(l)}.
\]

Thus,
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \delta_j^{(l+1)} a_i^{(l)},
\label{eq:weight_gradient}
\end{equation}
where we define the \emph{error term}
\[
\delta_j^{(l+1)}:= \frac{\partial \mathcal{L}}{\partial z_j^{(l+1)}}.
\]
Collecting the \(\delta_j^{(l+1)}\) for all neurons in layer \(l+1\) forms a vector \(\boldsymbol{\delta}^{(l+1)}\) with the same dimension as \(z^{(l+1)}\), ensuring the gradient \(\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}\) has the same shape as the weight matrix.

\paragraph{Interpretation of \(\delta_j^{(l+1)}\)}

The term \(\delta_j^{(l+1)}\) measures how sensitive the loss is to changes in the pre\hyp{}activation \(z_j^{(l+1)}\). Our task reduces to computing these \(\delta\) terms for all neurons in the network.

\subsubsection{Output layer error terms}
\label{sec:backprop_output_layer_error_terms_sub}

For the output layer \( L \), the activation of neuron \( k \) is
\[
a_k^{(L)} = f\big(z_k^{(L)}\big),
\]
where \(f(\cdot)\) is the activation function.

The error term for output neuron \( k \) is
\begin{align}
\delta_k^{(L)} &= \frac{\partial \mathcal{L}}{\partial z_k^{(L)}} \\
&= \frac{\partial \mathcal{L}}{\partial a_k^{(L)}} \frac{\partial a_k^{(L)}}{\partial z_k^{(L)}} \\
&= \big(a_k^{(L)} - t_k\big) \, f'\big(z_k^{(L)}\big),
\label{eq:delta_output}
\end{align}
where \(f'\) denotes the derivative of the activation function evaluated element-wise. With squared error, this is the
entire story: \(\partial \mathcal{L}/\partial a_k^{(L)} = a_k^{(L)}-t_k\), and the chain rule contributes the extra
\(f'(z_k^{(L)})\) factor.

For cross\hyp{}entropy paired with a sigmoid or softmax output, the derivative of the log-likelihood removes that extra
activation-derivative factor at the output, leaving \(\delta_k^{(L)} = a_k^{(L)}-t_k\) (with \(\delta\) still defined as
\(\partial \mathcal{L}/\partial z\)). This simplification is one reason those loss/output pairs are so common: you avoid
carrying an additional \(f'(z)\) term in the output\hyp{}layer learning signal.

\subsubsection{Hidden layer error terms}
\label{sec:backprop_hidden_layer_error_terms_sub}

For a hidden neuron \( j \) in layer \( l \), the error term \(\delta_j^{(l)}\) depends on the error terms of the neurons in the next layer \( l+1 \) to which it connects. Using the chain rule,
\begin{align}
\delta_j^{(l)} &= \frac{\partial \mathcal{L}}{\partial z_j^{(l)}} \\
&= \sum_{k} \frac{\partial \mathcal{L}}{\partial z_k^{(l+1)}} \frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}} \\
&= \sum_{k} \delta_k^{(l+1)} \frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}}.
\label{eq:delta_hidden_chain}
\end{align}

Since
\[
z_k^{(l+1)} = \sum_m a_m^{(l)}\, w_{mk}^{(l)} + b_k^{(l+1)},
\]
and \( a_j^{(l)} = f\big(z_j^{(l)}\big) \), we have
\[
\frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}} = w_{jk}^{(l)} \, f'\big(z_j^{(l)}\big).
\]

Substituting into \eqref{eq:delta_hidden_chain} yields
\begin{equation}
\delta_j^{(l)} = f'\big(z_j^{(l)}\big) \sum_{k} w_{jk}^{(l)} \delta_k^{(l+1)}.
\label{eq:delta_hidden}
\end{equation}

For sigmoid activations \(f\), the derivative simplifies to \(f'(z_j^{(l)}) = a_j^{(l)} (1 - a_j^{(l)})\); other activations require substituting their respective derivatives in \eqref{eq:delta_hidden}.

\paragraph{Summary: Backpropagation recursion}
Backpropagation is reverse\hyp{}mode differentiation on the network graph. A forward pass stores intermediate values (pre\hyp{}activations and activations); a backward pass reuses those stored values to compute all gradients efficiently. Frameworks (PyTorch/JAX/TF) automate the mechanics, but the manual story is what lets you sanity-check shapes, signs, and scaling when training does something surprising.

The step-by-step output-layer derivation and the hidden-layer recursion are already established in \Cref{sec:backprop_output_layer_error_terms_sub,sec:backprop_hidden_layer_error_terms_sub}; we now focus on how those gradients are accumulated in batch and stochastic settings.

\subsection{Batch and Stochastic Gradient Descent}
\label{sec:backprop_batch_and_stochastic_gradient_descent}

Given a training set of \( N \) examples \(\{(\mathbf{x}^{(n)}, \mathbf{t}^{(n)})\}_{n=1}^N\), the weight updates can be computed in different ways:

\begin{itemize}
    \item \textbf{Batch gradient descent:} Compute the gradient over the entire dataset and update weights once per epoch (shown here for a single output unit; the multi\hyp{}layer case uses the same idea with layerwise \(\delta\)'s and cached activations):
    \[
        \Delta w = -\frac{\eta}{N} \sum_{n=1}^N \delta^{(n)} \mathbf{x}^{(n)}.
    \]

    \item \textbf{Stochastic gradient descent (SGD):} Update weights after each training example using the instantaneous gradient \(-\eta \, \delta^{(n)} \mathbf{x}^{(n)}\). Although the updates are noisy, SGD often converges faster in practice and can escape shallow local minima.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Optimizer and stability notes}]
Start with plain SGD so you can tell whether your gradients are correct, then add stability aids one at a time (momentum, then an adaptive method like Adam/AdamW; see \Cref{chap:cnn}). For regularization, connect this to the L2 discussion from \Cref{chap:supervised}: in plain SGD, ``weight decay'' corresponds to shrinking weights toward zero; with AdamW, that shrinkage is applied as a decoupled decay step rather than being folded into the gradient. For deep or ill-conditioned nets, gradient clipping can prevent explosions. For classification, cross\hyp{}entropy with a numerically stable softmax (implement it via log\hyp{}sum\hyp{}exp or ``subtract the max logit'') is a common default recipe (\Cref{chap:cnn}).
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Implementation pattern (modern practice): softmax + cross-entropy}]
\footnotesize
\begin{verbatim}
# Shapes: X in R^{B x d}, Y in R^{B x c} (one-hot)
#         W1 in R^{d x h}, W2 in R^{h x c}
# Note: stable softmax: subtract row max.
	def step_ce_softmax(X, Y, params, eta, wd=1e-4, p_drop=0.1):
	    W1, b1, W2, b2 = params
	    B = X.shape[0]
    # Forward pass
    Z1 = X @ W1 + b1
    H1 = relu(Z1)
    mask1 = (np.random.rand(*H1.shape) > p_drop).astype(H1.dtype)
    # Inverted dropout
    H1 = H1 * mask1 / (1 - p_drop)
    Z2 = H1 @ W2 + b2
    Yhat = softmax(Z2)
    # Backward pass: for softmax + CE, delta2 = Yhat - Y
    delta2 = (Yhat - Y) / B
    grad_W2 = H1.T @ delta2 + wd * W2
    grad_b2 = delta2.sum(axis=0)
    delta1 = (delta2 @ W2.T) * relu_deriv(Z1)
    # Dropout backprop: reuse mask + scale
    delta1 = delta1 * mask1 / (1 - p_drop)
    grad_W1 = X.T @ delta1 + wd * W1
    grad_b1 = delta1.sum(axis=0)
    # SGD step
    return (W1 - eta * grad_W1, b1 - eta * grad_b1,
            W2 - eta * grad_W2, b2 - eta * grad_b2)
\end{verbatim}
\normalsize
This snippet uses the same \(\delta\) recursion as the squared\hyp{}error derivation; the main change is the output layer, where softmax + cross\hyp{}entropy yields \(\delta^{(L)}=\hat{\mathbf{Y}}-\mathbf{Y}\) (up to batch scaling). The extra details (dropout, weight decay) are included only to make the pattern look like real code.
\end{tcolorbox}


\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[
        >={Stealth[round, length=2.5mm, width=1.75mm]},
        node distance=1.2cm and 1.5cm,
        font=\small\sffamily,
        base/.style={draw, line width=0.9pt, rounded corners=4pt, align=center},
        var/.style={base, circle, fill=white, draw=gray!40, minimum size=12mm, inner sep=1pt},
        block/.style={base, rectangle, fill=cbBlue!18, draw=cbBlue!75!black, minimum height=12mm, minimum width=18mm},
        act/.style={base, rectangle, fill=cbBlue!8, draw=cbBlue!75!black, minimum height=12mm, minimum width=18mm},
        param/.style={base, rectangle, fill=cbGreen!18, draw=cbGreen!75!black, minimum height=10mm, minimum width=26mm},
        loss/.style={base, rectangle, fill=cbPink!12, draw=cbPink!80!black, minimum height=12mm, minimum width=18mm},
        fwd/.style={->, line width=1.3pt, draw=gray!55},
        bwd/.style={->, line width=1.1pt, draw=cbOrange!85!black, dashed, rounded corners=10pt},
        grad/.style={->, line width=1.1pt, draw=cbGreen!75!black, dashed}
    ]
        \node[var] (x) {$\mathbf{x}$};

        \node[block, right=of x] (Z1) {$\mathbf{Z}^{(1)}$};
        \node[param, above=1.5cm of Z1] (W1) {$\mathbf{W}^{(1)},\, \mathbf{b}^{(1)}$};
        \node[act, right=0.8cm of Z1] (A1) {$\mathbf{A}^{(1)}$\\[-2pt]{\scriptsize $f(\cdot)$}};

        \node[right=0.8cm of A1, font=\large\bfseries, text=gray!40] (dots) {$\cdots$};

        \node[block, right=0.8cm of dots] (ZL) {$\mathbf{Z}^{(L)}$};
        \node[param, above=1.5cm of ZL] (WL) {$\mathbf{W}^{(L)},\, \mathbf{b}^{(L)}$};
        \node[act, right=0.8cm of ZL] (AL) {$\mathbf{A}^{(L)}$\\[-2pt]{\scriptsize $\sigma(\cdot)$}};

        \node[loss, right=1.2cm of AL] (L) {$\mathcal{L}$};
        \node[var, above=1.5cm of L] (t) {$t$};

        % Cache box (manual corners; no fit/backgrounds/shadows libraries needed)
        % Raise the cache-box top so the legend sits above the parameter nodes without overlap.
        \coordinate (boxTop) at ($(W1.north)+(0,2.2cm)$);
        \coordinate (cacheNW) at ($(Z1.west |- boxTop)+(-0.6,0)$);
        \coordinate (cacheSE) at ($(AL.south east)+(0.6,-0.55)$);
        % Use fill opacity so the cache box doesn't obscure the nodes (drawn after nodes).
        \draw[draw=gray!20, dashed, rounded corners=12pt, fill=gray!5, fill opacity=0.35, draw opacity=1]
            (cacheNW) rectangle (cacheSE);
        \node[anchor=north east, font=\scriptsize\itshape, text=gray!60, inner sep=4pt]
            at (cacheSE) {Forward cache};

        % Legend (top-left inside cache box)
        \node[anchor=north west, font=\scriptsize\sffamily, fill=white, draw=gray!20, rounded corners=3pt, inner sep=5pt]
            at ($(cacheNW)+(0.25,-0.25)$) {%
            \begin{tabular}{@{}l@{\,\,}l@{}}
            \textcolor{gray!55}{\rule{6mm}{1.5pt}} & Forward \\
            \textcolor{cbOrange!85!black}{\rule[2pt]{6mm}{1.5pt}} & Backprop ($\delta$) \\
            \textcolor{cbGreen!75!black}{\rule[2pt]{6mm}{1.5pt}} & Grads ($\nabla$) \\
            \end{tabular}
        };

        % Forward pass
        \draw[fwd] (x) -- (Z1);
        \draw[fwd] (W1) -- (Z1);
        \draw[fwd] (Z1) -- (A1);
        \draw[fwd] (A1) -- (dots);
        \draw[fwd] (dots) -- (ZL);
        \draw[fwd] (WL) -- (ZL);
        \draw[fwd] (ZL) -- (AL);
        \draw[fwd] (AL) -- (L);
        \draw[fwd] (t) -- (L);

        % Backward pass
        \draw[bwd] (L.south) |- ++(0,-1.3) -|
            node[pos=0.25, below, font=\scriptsize] {$\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{(L)}}$} (AL.south);
        \draw[bwd] (AL.south) |- ++(0,-1.3) -|
            node[pos=0.25, below, font=\scriptsize] {$\boldsymbol{\delta}^{(L)}$} (ZL.south);
        \draw[bwd] (ZL.south) |- ++(0,-1.3) -|
            node[pos=0.5, below, font=\scriptsize, fill=white, inner sep=3pt] {chain rule via $W^\top, f'$} (Z1.south);
        \node[font=\scriptsize, text=cbOrange!85!black, below=0.1cm of Z1.south, xshift=0.4cm, yshift=-0.5cm] {$\boldsymbol{\delta}^{(1)}$};

        % Gradient extraction (curved to avoid overlaps)
        \draw[grad] (ZL.north) to[bend right=45]
            node[midway, right, font=\tiny, text=cbGreen!75!black, align=left, xshift=2pt]
            {$\nabla_{\mathbf{W}^{(L)}}$\\$\nabla_{\mathbf{b}^{(L)}}$} (WL.south);
        \draw[grad] (Z1.north) to[bend right=45]
            node[midway, right, font=\tiny, text=cbGreen!75!black, align=left, xshift=2pt]
            {$\nabla_{\mathbf{W}^{(1)}}$\\$\nabla_{\mathbf{b}^{(1)}}$} (W1.south);
    \end{tikzpicture}%
    }
    % Avoid dense inline math in captions; it wraps poorly in EPUB renderers.
    \caption[Computational graph for backpropagation (reverse-mode AD)]{Computational graph for a feedforward network. Backpropagation is reverse\hyp{}mode AD: the forward sweep caches intermediate values, and the reverse sweep propagates deltas while accumulating weight/bias gradients from those cached values.}
    \label{fig:backprop-computational-graph}
\end{figure}


\begin{tcolorbox}[summarybox, title={Debugging and gradient-check checklist}]
\begin{itemize}
    \item \textbf{Overfit a tiny batch:} Drive the loss near zero on a handful of samples; if you cannot, the issue is almost always code or setup, not generalization.
    \item \textbf{Finite differences on a tiny net:} Fix seeds, perturb one parameter, and compare analytic vs.\ numerical gradients. Do this before running long experiments.
    \item \textbf{Track per-layer gradient norms:} \(\|\nabla \mathbf{W}^{(l)}\|\) that vanish (all zeros) or explode are early warnings.
    \item \textbf{Assert shapes and broadcasts:} Verify \(\mathbf{Z}^{(l)}, \mathbf{A}^{(l)}, \boldsymbol{\delta}^{(l)}\) shapes and bias broadcasting explicitly.
    \item \textbf{Sanity baselines:} For a one-layer linear model, backprop should match the closed-form gradient you already know.
\end{itemize}
\end{tcolorbox}

\subsection{Backpropagation Algorithm: Brief Numerical Check}
\label{sec:backprop_backpropagation_algorithm_brief_numerical_check}

For a quick sanity check, take a tiny 2--2--1 network with sigmoid output and cross\hyp{}entropy loss. Using
\begin{align*}
\mathbf{W}^{(1)}&=\begin{bmatrix}0.5&-0.3\\0.8&0.2\end{bmatrix},\quad \mathbf{b}^{(1)}=[0.1,-0.2],\\
\mathbf{W}^{(2)}&=\begin{bmatrix}0.7\\-0.4\end{bmatrix},\quad \mathbf{b}^{(2)}=0.05,\\
\mathbf{x}&=[0.6,-1.2],\quad t=1,
\end{align*}
the forward pass yields
\[
z^{(1)}=[-0.56,-0.62],\quad
a^{(1)}=[0.3635,0.3498],\quad
z^{(2)}=0.1646,\quad
a^{(2)}=0.5411,
\]
with loss \(\mathcal{L}\approx 0.6142\). The cross\hyp{}entropy output error is \(\delta^{(2)}=a^{(2)}-t=-0.4590\). Backpropagating gives
\[
\delta^{(1)}=[-0.0743,0.0418],\quad
\nabla_{\mathbf{W}^{(2)}}=[-0.1669,-0.1605]^\top,\quad
\nabla_{\mathbf{b}^{(2)}}=-0.4590,
\]
and
\[
\nabla_{\mathbf{W}^{(1)}}=
\begin{bmatrix}
-0.0446&0.0251\\
0.0892&-0.0501
\end{bmatrix},\quad
\nabla_{\mathbf{b}^{(1)}}=[-0.0743,0.0418].
\]
Finite-difference checks on the same network match to numerical precision, validating the implementation.

\paragraph{Aside: squared-error loss (alternative)}

The remainder of this subsection sketches the classic squared-error backprop derivation as a separate reminder; it is \emph{not} a continuation of the cross\hyp{}entropy numerical check above.

For one output unit with activation \(a=\sigma(z)\) and target \(t\), define the scalar error \(e=a-t\) and squared error
\(\mathcal{L}_{\text{SE}}=\tfrac12 e^2\). The output-layer error signal (still defined as \(\delta=\partial\mathcal{L}/\partial z\)) is
\[
\delta^{(L)} = (a-t)\,\sigma'(z), \qquad \sigma'(z)=\sigma(z)\bigl(1-\sigma(z)\bigr).
\]

For a hidden unit \(j\) in layer \(l\), the same chain-rule logic gives
\[
\delta_j^{(l)} = f'(z_j^{(l)}) \sum_{k} w_{jk}^{(l)}\,\delta_k^{(l+1)},
\]
i.e., ``local slope times the weighted sum of downstream error signals,'' with \(w_{jk}^{(l)}\) the weight from unit \(j\) (layer \(l\)) to unit \(k\) (layer \(l+1\)).

\paragraph{Weight update rule (with momentum).}
Once you have \(\delta\)'s, turning them into weight updates is straightforward. For a weight \(w_{ij}^{(l)}\) that connects unit \(i\) in layer \(l-1\) to unit \(j\) in layer \(l\), the gradient is proportional to ``input times local error'': \(a_i^{(l-1)}\delta_j^{(l)}\). With momentum, one common update is
\begin{equation}
    \Delta w_{ij}^{(l)}(n) = -\eta \, a_i^{(l-1)}(n)\,\delta_j^{(l)}(n) + \gamma \Delta w_{ij}^{(l)}(n-1),
    \label{eq:weight_update}
\end{equation}
followed by \(w_{ij}^{(l)}(n)=w_{ij}^{(l)}(n-1)+\Delta w_{ij}^{(l)}(n)\). Here \(\eta\) is the step size and \(\gamma\in[0,1)\) is the momentum coefficient. (For the first hidden layer, \(a_i^{(0)}\) is just the input feature \(x_i\).)
The index \(n\) denotes the update step (for example, the current training example in SGD, or the current mini\hyp{}batch update).

\paragraph{A practical reading of \(\eta\) and \(\gamma\).}
The learning rate sets the basic step scale; momentum averages recent gradient directions so updates do not zig\hyp{}zag as much across narrow valleys. If training oscillates, reduce \(\eta\); if it is painfully slow along a consistent direction, momentum can help.

\paragraph{One update step (what happens in one mini-batch)}

\begin{enumerate}
    \item \textbf{Initialize:} choose an initialization scale (Xavier is a common default for sigmoid/tanh; He is a common default for ReLU-family activations) and set biases (often zero).
    \item \textbf{Forward pass:} compute and cache \(z^{(l)}\) and \(a^{(l)}\) for each layer.
    \item \textbf{Loss + output signal:} compute \(\mathcal{L}\) and \(\delta^{(L)}=\partial\mathcal{L}/\partial z^{(L)}\).
    \item \textbf{Backward pass:} propagate \(\delta^{(l)}\) from \(l=L-1\) down to the first hidden layer.
    \item \textbf{Gradients:} form \(\nabla_{\mathbf{W}^{(l)}}\mathcal{L}\) and \(\nabla_{\mathbf{b}^{(l)}}\mathcal{L}\) from cached activations and \(\delta\)'s.
    \item \textbf{Update:} apply your optimizer step (SGD, momentum, Adam) and repeat for the next mini\hyp{}batch.
\end{enumerate}

\begin{tcolorbox}[summarybox, title={Mini\hyp{}batch backprop with explicit regularization}]
\textbf{Inputs:} mini\hyp{}batch \(\{(\mathbf{x}_b,\mathbf{t}_b)\}_{b=1}^B\), learning rate \(\eta\), L2 coefficient \(\lambda\), dropout keep probability \(q=1-p\).
\begin{enumerate}[leftmargin=*]
    \item \textbf{Forward pass:} propagate activations layer by layer. If you use dropout in a hidden layer, draw a mask \(\mathbf{m}\sim \operatorname{Bernoulli}(q)\), apply \(\tilde{\mathbf{a}}=\mathbf{m}\odot \mathbf{a}/q\), and cache \(\mathbf{m}\) for the backward step.
    \item \textbf{Backward pass:} compute \(\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L}\) using cached activations (and cached dropout masks) so dropped units contribute zero gradient.
    \item \textbf{Update block (per layer):}
    \[
        \mathbf{g}_\ell = \frac{1}{B}\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L} + \lambda \mathbf{W}^{(\ell)}, \qquad
        \mathbf{W}^{(\ell)} \leftarrow \mathbf{W}^{(\ell)} - \eta\, \mathbf{g}_\ell.
    \]
    Biases skip the weight-decay term. With Adam or SGD+momentum, \(\mathbf{g}_\ell\) is what you feed into the optimizer update so regularization stays explicit.
\end{enumerate}
\end{tcolorbox}

\paragraph{Remarks}

Track both training and validation curves: a flat training loss usually points to an optimization issue (step size, activation saturation, initialization), while a widening train/validation gap points to capacity or regularization. Shuffle each epoch, checkpoint the best validation model, and let an early-stopping rule end runs before you overfit. In essence, the objective is to build confidence that your design decisions generalize out-of-sample; these curves give you a simple way to describe that generalization quantitatively.

\subsection{Training Procedure and Epochs in Multi-Layer Perceptrons}
\label{sec:backprop_training_procedure_and_epochs_in_multi_layer_perceptrons}

An epoch is one complete pass over the training set. In practice you almost never update one example at a time; you shuffle the data and iterate over mini\hyp{}batches so the gradient estimates are both noisy enough to explore and cheap enough to compute.

\begin{enumerate}
    \item Shuffle the training examples (or shuffle within class strata for imbalanced problems).
    \item For each mini\hyp{}batch: run a forward pass, compute the loss, run a backward pass, and update parameters.
    \item At the end of the epoch: evaluate on a validation split, checkpoint the best model, and apply an early\hyp{}stopping rule if needed.
\end{enumerate}

After each epoch, look at both training and validation curves. A training loss that keeps falling while validation stalls is your cue to regularize, stop, or change capacity.

\paragraph{Common habits that prevent wasted runs.}
\begin{itemize}
    \item Start by overfitting a tiny batch and running a gradient check (the checklist box earlier in the chapter).
    \item Log the random seed and the full optimizer recipe; without it, ``it did not train'' is not a reproducible diagnosis.
    \item If you change one thing (activation, initialization, step size), keep everything else fixed so you can attribute cause and effect.
\end{itemize}

\subsection{Role and Design of Hidden Layers}
\label{sec:backprop_role_and_design_of_hidden_layers}

Hidden layers are where an MLP earns its flexibility. They do not just add parameters; they add intermediate representations, so different units can specialize and a later layer can combine those specializations.

\paragraph{Design questions you actually have to answer.}
\begin{itemize}
    \item \textbf{Depth vs.\ width:} do you want a few wide layers or many narrow layers?
    \item \textbf{Capacity vs.\ data:} how much flexibility can your dataset support before you overfit?
    \item \textbf{Activation choice:} will gradients flow (ReLU family), or will they saturate (sigmoid/tanh) for the scale of your pre\hyp{}activations?
\end{itemize}

\paragraph{A simple, defensible starting point.}
\begin{itemize}
    \item Pick one or two hidden layers and a moderate width, then let validation performance tell you whether you need more capacity.
    \item Choose an activation that keeps gradients alive (ReLU or a leaky ReLU are common defaults in practice).
    \item Use early stopping and weight decay as your first line of defense against overfitting.
\end{itemize}

\paragraph{Trade-offs to keep in mind.}
\begin{itemize}
    \item \textbf{Too much capacity:} training loss drops quickly, validation does not; the model memorizes details you did not intend it to learn.
    \item \textbf{Too little capacity:} both training and validation plateau early; the model cannot represent the function you are asking for.
\end{itemize}

\subsection{Case Study: Learning the Function \texorpdfstring{\( y = x \sin x \)}{y = x sin x}}
\label{sec:backprop_case_study_learning_the_function_y_x_x_y_x_sin_x}

Consider the problem of training an MLP to approximate the function
\[
y = x \sin x.
\]

\paragraph{Setup.}
\begin{itemize}
    \item Generate a dataset of input-output pairs \(\{(x_i, y_i)\}\) where \(y_i = x_i \sin x_i\).
    \item Use this dataset to train an MLP regressor.
    \item Evaluate the network's ability to generalize by testing on inputs not seen during training.
\end{itemize}

\paragraph{What to look for.}
\begin{itemize}
    \item \textbf{Capacity vs.\ fit:} if the model is too small, the best\hyp{}fit curve will miss structure; if it is too large for your data density, it will fit noise or interpolation artifacts.
    \item \textbf{Activation effects:} saturated activations can make learning look ``stuck'' even when the model has enough parameters.
    \item \textbf{Data density:} uniform sampling over \([-3\pi,3\pi]\) reveals whether your model interpolates smoothly between points or produces oscillatory artifacts.
\end{itemize}

\paragraph{Practical notes.}
\begin{itemize}
    \item This is a regression problem, not a classification problem.
    \item The target is nonlinear and oscillatory; underfitting and overfitting are both easy to see in a plot.
    \item If you report performance, report it as a reproducible experiment (seed, split, optimizer recipe). Avoid quoting a single ``typical'' number unless you can regenerate it.
\end{itemize}

\subsection{Applications of Multi-Layer Perceptrons}
\label{sec:backprop_applications_of_multi_layer_perceptrons}

MLPs show up in two roles. Sometimes they are the whole model, as in small function-approximation and classification tasks. More often they are trainable blocks inside larger systems: the same affine\(\rightarrow\)nonlinearity pattern appears in convolutional networks, recurrent networks, and attention models, and we will revisit this connection later. In all cases, the training signal still arrives as a loss at the output, and backpropagation is what assigns that loss to each parameter through the stored intermediate values from the forward pass.

\subsection{Limitations of Multi-Layer Perceptrons}
\label{sec:backprop_limitations_of_multi_layer_perceptrons}

MLPs are flexible, but they require care to train reliably. The main limitations show up as training instability, sensitivity to design choices, and difficulty diagnosing silent implementation mistakes. Training can be sensitive: small changes to the random initialization, data order, or step-size settings can lead to noticeably different outcomes, especially when the problem is poorly conditioned.

\begin{itemize}
    \item \textbf{Run-to-run variability:} different initializations and data orders can lead to different solutions. Treat seeds and splits as part of the experiment, not as incidental details.
    \item \textbf{Optimization fragility:} step size, activation choice, and initialization scale directly affect gradient flow. When learning stalls, use the gradient norms and the activation-derivative picture in \Cref{fig:lec4-activations} as your first diagnostic.
    \item \textbf{Silent implementation bugs:} shape/broadcasting mistakes can produce gradients that look plausible but are wrong. This is why the shape ledger and finite-difference checks in this chapter are part of the basic implementation discipline.
\end{itemize}

\subsection{Conclusion of Multi-Layer Perceptron Derivations}
\label{sec:backprop_conclusion_of_multi_layer_perceptron_derivations}

At this point you have the full backprop workflow: store computations from the forward pass, compute \(\delta^{(L)}\), propagate \(\delta\)'s backward, and assemble gradients with the shapes kept honest. The rest of the chapter focuses on practical stability (activations, initialization, stopping) and the habits that keep implementations honest.

\paragraph{Backpropagation algorithm recap (matrix form).}
One compact way to write the forward pass is:
\[
\mathbf{Z}^{(l)} = \mathbf{A}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{1}(\mathbf{b}^{(l)})^\top,\quad \mathbf{A}^{(l)} = f^{(l)}(\mathbf{Z}^{(l)}),
\]
where \(\mathbf{W}^{(l)}\) and \(\mathbf{b}^{(l)}\) are the weights and biases of layer \( l \), \(\mathbf{A}^{(l-1)}\) is the previous layer activation (rows are samples), \(\mathbf{1}\in\mathbb{R}^{B}\) is an all-ones vector that broadcasts the bias across the batch, and \(f^{(l)}\) is the (possibly layer-specific) activation function.

Define the error signal at layer \(l\) as:
\[
\boldsymbol{\delta}^{(l)} = \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(l)}}.
\]

Then the chain rule gives a reusable backward recursion:
\begin{align}
\boldsymbol{\delta}^{(L)} &= \nabla_{\mathbf{A}^{(L)}} \mathcal{L} \odot f^{(L)\prime}(\mathbf{Z}^{(L)}), \\
\boldsymbol{\delta}^{(l-1)} &= \left(\boldsymbol{\delta}^{(l)}(\mathbf{W}^{(l)})^\top\right) \odot f^{(l-1)\prime}(\mathbf{Z}^{(l-1)}), \quad l = L, \ldots, 2,
		    \label{eq:auto:lecture_4_part_i:2}
\end{align}
where \(\odot\) denotes element-wise multiplication and \(f^{(l)\prime}\) is the derivative of the activation function at layer \( l \).

Finally, turn those \(\delta\)'s into parameter gradients:
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} &= (\mathbf{A}^{(l-1)})^\top \boldsymbol{\delta}^{(l)}, \label{eq:grad_W}\\
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}} &= \mathbf{1}^\top \boldsymbol{\delta}^{(l)}. \label{eq:grad_b}
\end{align}
If your loss is defined as a mean over the batch, divide these batch sums by \(B\); the shape ledger earlier in the chapter uses that mean convention.

These equations are the whole story: cache forward values, propagate \(\delta\)'s backward, then assemble gradients. \Cref{fig:lec4_backprop_flow} complements the algebra by showing how cached activations (forward arrows) line up with backward error signals in a simple two-layer network.

\begin{figure}[h]
    \centering
    \ifdefined\HCode
    % EPUB/HTML: use a simplified version of the original "wide" diagram, but with
    % fewer colors and without tiny per-edge weight labels (which rasterize poorly).
    \begin{tikzpicture}[
        >=Stealth,
        font=\small\sffamily,
        node/.style={circle, draw=black!50, line width=0.9pt, minimum size=12mm, fill=white},
        fwd/.style={->, line width=1.4pt, draw=cbBlue!80!black},
        bwd/.style={->, line width=1.4pt, draw=black!55, dashed},
        lbl/.style={font=\footnotesize\sffamily, text=black!70},
        background rectangle/.style={fill=white}, show background rectangle
    ]

        % Forward nodes (inputs -> hidden -> output)
        % Shift right to leave a clean legend margin on the left.
        \node[node] (x1) at (0.8,0.9) {$x_1$};
        \node[node] (x2) at (0.8,-0.9) {$x_2$};
        \node[node] (h1) at (2.6,0.9) {$a_1^{(1)}$};
        \node[node] (h2) at (2.6,-0.9) {$a_2^{(1)}$};
        \node[node] (y)  at (5.2,0) {$a^{(2)}$};

        % Forward connections (no per-edge weights in EPUB)
        \draw[fwd] (x1) -- (h1);
        \draw[fwd] (x1) -- (h2);
        \draw[fwd] (x2) -- (h1);
        \draw[fwd] (x2) -- (h2);
        \draw[fwd] (h1) -- (y);
        \draw[fwd] (h2) -- (y);

        % Backward/error signals (labels placed away from nodes to avoid overlap)
        \node[lbl, anchor=west] (d2) at ($(y)+(0.9,1.05)$) {$\boldsymbol{\delta}^{(2)}$};
        \node[lbl, anchor=west] (d1) at ($(y)+(0.9,-1.3)$) {$\boldsymbol{\delta}^{(1)}_1$};
        \node[lbl, anchor=west] (d0) at ($(y)+(0.9,-2.1)$) {$\boldsymbol{\delta}^{(1)}_2$};

        \draw[bwd] ($(y)+(0,0.55)$) -- (d2);
        \draw[bwd] ($(y)+(-0.1,-0.2)$).. controls (5.0,-0.85) and (4.6,-1.1).. (d1);
        \draw[bwd] ($(y)+(-0.1,-0.2)$).. controls (5.0,-1.25) and (4.6,-1.65).. (d0);

        % No in-figure legend for EPUB: small labels tend to overlap after rasterization.
        % The caption explains the color/linestyle mapping instead.
    \end{tikzpicture}
    \else
    \begin{tikzpicture}[
        >=Stealth,
        font=\small\sffamily,
        node distance=1.55cm and 2.0cm,
        var/.style={circle, draw=gray!60, line width=0.7pt, minimum size=10mm, fill=cbBlue!12, inner sep=0pt},
        hnode/.style={circle, draw=gray!60, line width=0.7pt, minimum size=10mm, fill=cbGreen!12, inner sep=0pt},
        outnode/.style={circle, draw=gray!60, line width=0.7pt, minimum size=10.5mm, fill=cbPink!15, inner sep=0pt},
        fwd/.style={->, line width=1.0pt, draw=cbBlue!80!black},
        bwd/.style={->, line width=1.0pt, draw=cbOrange!85!black, dashed},
        wlab/.style={font=\scriptsize, text=cbBlue!80!black},
        dlab/.style={font=\scriptsize, text=cbOrange!85!black}
    ]
        \node[var] (x1) {\(x_1\)};
        \node[var, below=0.9cm of x1] (x2) {\(x_2\)};
        \node[hnode, right=2.1cm of x1] (h1) {\(a_1^{(1)}\)};
        \node[hnode, below=0.9cm of h1] (h2) {\(a_2^{(1)}\)};
        \node[outnode, right=2.2cm of h1] (y) {\(a^{(2)}\)};

        \draw[fwd] (x1) -- node[above, sloped, wlab]{\(w_{11}^{(1)}\)} (h1);
        \draw[fwd] (x1) -- node[above, sloped, wlab]{\(w_{21}^{(1)}\)} (h2);
        \draw[fwd] (x2) -- node[below, sloped, wlab]{\(w_{12}^{(1)}\)} (h1);
        \draw[fwd] (x2) -- node[below, sloped, wlab]{\(w_{22}^{(1)}\)} (h2);
        \draw[fwd] (h1) -- node[above, wlab]{\(w_{1}^{(2)}\)} (y);
        \draw[fwd] (h2) -- node[below, wlab]{\(w_{2}^{(2)}\)} (y);

        \draw[bwd] (y) -- ++(0,1.2) node[right, xshift=0.1cm, dlab]{\(\delta^{(2)}\)};
        \draw[bwd] (h1) -- ++(0,-1.2) node[right, xshift=0.1cm, dlab]{\(\delta_1^{(1)}\)};
        \draw[bwd] (h2) -- ++(0,-1.2) node[right, xshift=0.1cm, dlab]{\(\delta_2^{(1)}\)};

        % Keep legend labels *inside* the diagram bounds; otherwise rasterizers can clip them.
        \path (current bounding box.north west) ++(2mm,-2mm)
            node[anchor=north west, font=\scriptsize, text=cbBlue!80!black] {forward};
        \path (current bounding box.north west) ++(2mm,-7mm)
            node[anchor=north west, font=\scriptsize, text=cbOrange!85!black] {backward};
    \end{tikzpicture}
    \fi
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Forward (blue) and backward (orange) flows for a two-layer MLP. Cached activations and layerwise deltas travel along these arrows; backward signals use next-layer weights and activation derivatives.}
    \label{fig:lec4_backprop_flow}
\end{figure}


\paragraph{Example Execution}

Before you scale this to larger models, run one tiny network end-to-end and check each ingredient: compute the forward values, compute the output \(\delta\), push it one layer back, and confirm one gradient numerically. The explicit numbers in \Cref{sec:backprop_backpropagation_algorithm_brief_numerical_check} can serve as a reference trace when you debug your own implementation.

\paragraph{Remarks on Convergence and Practical Considerations}

Backpropagation gives the \emph{right} gradients; whether those gradients turn into learning depends on a handful of design and optimization choices. When training stalls or becomes unstable, the usual levers are:
\begin{itemize}
    \item \textbf{Initialization scale:} keep pre\hyp{}activations in a regime where derivatives are not all near zero (or wildly large).
    \item \textbf{Activation choice:} saturation (sigmoid/tanh) and dead regions (ReLU) show up directly as weak gradients.
    \item \textbf{Step size and schedule:} too large diverges, too small crawls; schedules and momentum smooth the path.
    \item \textbf{Regularization:} weight decay and dropout trade training fit for generalization.
    \item \textbf{Optimizer details:} momentum/Adam change the effective step direction and can rescue poorly conditioned problems.
\end{itemize}

We return to these practicalities in the later deep\hyp{}learning chapters; for now, it helps to see the canonical activation choices in one place.

\paragraph{Comparing canonical nonlinearities}

With the MLP and backpropagation machinery in place, it is useful to compare the most common nonlinearities side-by-side. \Cref{fig:lec4-activations} overlays the step, sigmoid, tanh, and ReLU curves so saturation regions and derivative behavior are visually apparent in one view.

\begin{figure}[t]
    \centering
\begin{tikzpicture}[background rectangle/.style={fill=white}, show background rectangle]
        \begin{groupplot}[
            group style={group size=2 by 2, horizontal sep=1.2cm, vertical sep=1.0cm},
            width=0.42\linewidth,
            height=0.30\linewidth,
            axis lines=middle,
            xmin=-3, xmax=3,
            samples=200,
            tick label style={font=\scriptsize},
            label style={font=\scriptsize},
            title style={font=\scriptsize, align=center},
            axis background/.style={fill=white},
        ]
        \nextgroupplot[
            title={Step},
            ymin=-0.1, ymax=1.1,
            ytick={0,1},
        ]
            \addplot[cbBlue, thick, domain=-3:0] {0};
            \addplot[cbBlue, thick, domain=0:3] {1};
            \addplot[cbBlue, dashed, domain=-3:3] {0};
        \nextgroupplot[
            title={Sigmoid},
            xmin=-4, xmax=4,
            ymin=-0.1, ymax=1.1,
            ytick={0,0.5,1},
        ]
            \addplot[cbOrange, thick, domain=-4:4] {1/(1+exp(-x))};
            \addplot[cbOrange, dashed, domain=-4:4] {1/(1+exp(-x))*(1-1/(1+exp(-x)))};
        \nextgroupplot[
            title={tanh},
            ymin=-1.1, ymax=1.1,
            ytick={-1,0,1},
        ]
            \addplot[cbGreen, thick, domain=-3:3] {tanh(x)};
            \addplot[cbGreen, dashed, domain=-3:3] {1 - tanh(x)^2};
        \nextgroupplot[
            title={ReLU},
            ymin=-0.2, ymax=2.2,
            ytick={0,1,2},
        ]
            \addplot[cbPink, thick, domain=-3:0] {0};
            \addplot[cbPink, thick, domain=0:3] {x};
            \addplot[cbPink, dashed, domain=-3:0] {0};
            \addplot[cbPink, dashed, domain=0:3] {1};
        \end{groupplot}
    \ensuretikzbackgroundlayers
\end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Canonical activation functions on a common axis. Solid curves show the activation; dashed curves show its derivative.}
    \label{fig:lec4-activations}
\end{figure}


For reference, \(\sigma'(z)=\sigma(z)\bigl(1-\sigma(z)\bigr)\), \(\tanh'(z)=1-\tanh^2(z)\), and the ReLU derivative is \(0\) for negative inputs and \(1\) for positive inputs (take \(0\) at the origin).

\paragraph{Trade-offs}
While some activation functions are inspired by biological neurons, others are chosen for mathematical convenience and training efficiency. Sigmoid and tanh saturate at large magnitude inputs, which slows gradients in deep networks. ReLU avoids saturation on the positive side but can produce ``dying ReLUs'' when biases push units negative and the gradients become zero; if many units stall, use He initialization, reduce the learning rate, or swap to a leaky ReLU with a small negative slope (e.g., 0.01).
At this point the core backpropagation story for MLPs is complete. The remainder of the chapter collects practical stability habits and a short set of takeaways you can use when training starts behaving badly.

\clearpage

	    \begin{tcolorbox}[summarybox, title={Key takeaways}]
	    \textbf{If you remember only a few things}
	    \begin{itemize}
	        \item Backpropagation is the chain rule organized: cache forward values, propagate \(\delta\)'s backward using \Cref{eq:auto:lecture_4_part_i:2}, then assemble gradients with \Cref{eq:grad_W,eq:grad_b}.
	        \item The \(\delta\)'s are not mysterious: they are sensitivities (\(\partial \mathcal{L}/\partial \mathbf{Z}^{(l)}\)) that you reuse to update every parameter.
	        \item For softmax + cross\hyp{}entropy, the output-layer error simplifies to \((\hat{\mathbf{Y}}-\mathbf{Y})/B\); we verify that identity numerically and use it in the modern-practice code pattern earlier in the chapter.
	        \item When training misbehaves, debug like an engineer: overfit a tiny batch, run a finite-difference check, then inspect per-layer gradient norms and shapes.
	    \end{itemize}
    \medskip
    \textbf{Common pitfalls}
    \begin{itemize}
        \item Dropping a transpose: \((\mathbf{W}^{(l)})^\top\) is easy to miss and produces gradients that look ``reasonable'' but train the wrong model.
        \item Mixing up pre\hyp{}activations and activations: \(f'(\mathbf{Z})\) depends on what you cache.
        \item Losing the batch scaling: decide whether you average over the batch (\(/B\)) and do it consistently.
        \item Silent broadcasting mistakes: bias terms and per-example deltas can accidentally align in NumPy/PyTorch and hide a bug.
    \end{itemize}
    \end{tcolorbox}

    \begin{tcolorbox}[summarybox, title={Practical early stopping and checkpointing}]
    \begin{itemize}
        \item Keep a validation split separate from the training mini\hyp{}batches, and record \(L_{\text{val}}^{(e)}\) after each epoch.
        \item Stop when \(L_{\text{val}}\) has not improved for \(k\) consecutive epochs (a patience of \(k\in[5,10]\) is a common starting point). If the curve is noisy, require a small minimum improvement before you reset patience.
        \item Always checkpoint the parameters that achieved the best validation score and restore them before testing. If training is noisy, averaging the last few checkpoints can stabilize performance.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Derivation closure: implement, cache, fail-fast}]
\begin{itemize}
    \item \textbf{Implement:} write one clean layer routine \((\mathbf{A}^{(l-1)}, \mathbf{W}^{(l)}, \mathbf{b}^{(l)}) \mapsto (\mathbf{Z}^{(l)}, \mathbf{A}^{(l)})\), then reuse it for every layer.
    \item \textbf{Cache:} store what backprop needs later (at minimum \(\mathbf{A}^{(l-1)}\) and \(\mathbf{Z}^{(l)}\)). Do not recompute forward quantities during the backward pass.
    \item \textbf{Fail fast:} gradient-check one tiny network and overfit one tiny batch before you trust results on a real dataset.
    \item \textbf{Make runs comparable:} record seeds, optimizer, schedule, and stopping rule. The reporting template in \Cref{app:repro_standards} helps keep comparisons honest.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item \textbf{A full trace on paper:} pick a tiny network and write out the forward values and \(\delta\)'s by hand, then match them against a short script.
    \item \textbf{Finite differences:} implement a gradient check for one layer and report the worst relative error; track down the first mismatch.
    \item \textbf{Activation choice:} run the same task with sigmoid/tanh/ReLU and compare gradient norms by layer; relate what you see to \Cref{fig:lec4-activations}.
    \item \textbf{Early stopping:} train until overfitting is visible, then add early stopping and report how the best checkpoint changes.
\end{itemize}
    \end{tcolorbox}
    \medskip
    \noindent\textbf{If you are jumping chapters.} Keep one habit: run a tiny gradient check before scaling up.

    \medskip
    \paragraph{Where we head next.} \Cref{chap:rbf} introduces radial basis function networks, an alternative nonlinear route where many hidden features are fixed and the output layer can be solved more directly. This provides a clean contrast to end-to-end backpropagation and a different training decomposition to compare against.

    \nocite{Rumelhart1986, Haykin2009}

\section*{Key Takeaways}
\addcontentsline{toc}{section}{Key Takeaways}
\begin{sloppypar}
{\raggedright
\begin{description}[leftmargin=3.6cm,labelwidth=3.2cm,labelsep=0.3cm,style=nextline]
\item[\Cref{chap:intro}] \textit{About This Book} explains how to read the book, points to the notation/figure conventions, and motivates the four-level taxonomy for systems thinking across chapters.
\item[\Cref{chap:symbolic}] \textit{Symbolic Integration and Problem-Solving Strategies} shows how safe substitutions, heuristic branches, and numeric fallbacks cooperate in a transformation tree, giving a procedural view of algebraic problem solving.
\item[\Cref{chap:supervised}] \textit{Supervised Learning Foundations} develops ERM/MLE/MAP, contrasts loss families, and grounds diagnostics such as learning curves, calibration, and proper scoring rules.
\item[\Cref{chap:logistic}] \textit{Classification and Logistic Regression} reuses the \Cref{chap:supervised} toolkit to build probabilistic classifiers, emphasize ROC/PR analysis, and reason about class imbalance, calibration, and optimization choices (Newton vs.\ first-order).
\item[\Cref{chap:perceptron}] \textit{Introduction to Neural Networks} casts the perceptron as a thresholded linear classifier (vs.\ logistic as the smooth probabilistic counterpart), proves convergence guarantees, and catalogues practical pitfalls such as poor feature scaling or non-separable data.
\item[\Cref{chap:mlp}] \textit{MLP Foundations} formalizes forward/backward passes with matrix-calculus identities, highlights caching/normalization for numerical stability, and frames bias--variance behavior for deep linear stacks.
\item[\Cref{chap:backprop}] \textit{Backpropagation in Practice} turns the derivatives into SGD/mini\hyp{}batch pseudocode, adds early\hyp{}stopping heuristics, and compares optimization tweaks (momentum, adaptive schedules) against the diagnostics from \Cref{chap:supervised}.
\item[\Cref{chap:rbf}] \textit{Radial Basis Function Networks} interprets RBFs as local ``bubbles,'' covers center/width selection (including practical \(\sigma\) rules), contrasts primal vs.\ dual training formulations, and connects the finite-basis view to kernel methods (e.g., kernel ridge regression and SVMs with RBF kernels).
\item[\Cref{chap:som}] \textit{Self-Organizing Maps} explains neighborhood competition/cooperation phases, quality measures (quantization/topographic error), and visualization tricks for prototype-based embedding.
\item[\Cref{chap:hopfield}] \textit{Hopfield and Energy-Based Memories} derives discrete/continuous dynamics, capacity bounds, and asynchronous vs.\ synchronous update strategies for associative recall.
\item[\Cref{chap:cnn}] \textit{Convolutional Neural Networks and Deep Training Tools} details convolution/cross-correlation, pooling, receptive-field growth, and the engineering defaults behind modern CNN blocks and training loops.
\item[\Cref{chap:rnn}] \textit{Recurrent Neural Networks} develops BPTT, gating strategies, and conditioning tricks (teacher forcing, scheduled sampling) for sequential modeling while connecting to the diagnostics from earlier chapters.
\item[\Cref{chap:transformers}] \textit{Transformers and Attention} consolidates scaled dot-product attention, multi-head blocks, encoder/decoder stacks, long-context strategies (RoPE/ALiBi, FlashAttention, KV caches), and PEFT techniques.
\item[\Cref{chap:nlp}] \textit{NLP Pipelines and Responsible Deployment} links static/contextual embeddings to downstream tasks, adds bias/calibration checklists, and closes with a deployment-readiness assessment.
\item[\Cref{chap:softcomp}] \textit{Soft Computing Orientation} positions fuzzy logic, neurocomputing, probabilistic reasoning, and evolutionary search as complementary tools and introduces the running thermostat example used in \Crefrange{chap:fuzzysets}{chap:fuzzyinference}.
\item[\Cref{chap:fuzzysets}] \textit{Fuzzy Sets and Membership Functions} defines linguistic variables, membership design patterns, set operations, and inclusion metrics that quantify vagueness and overlap.
\item[\Cref{chap:fuzzyrelations}] \textit{Fuzzy Relations and the Extension Principle} covers Cartesian products, projections, and composition operators (max--min, algebraic, \L{}ukasiewicz) that transfer fuzzy information across universes.
\item[\Cref{chap:fuzzyinference}] \textit{Fuzzy Inference Systems} assembles complete Mamdani and Sugeno pipelines (aggregation, implication, defuzzification) and studies practical operator choices, scaling, and thermostat/autofocus examples.
\item[\Cref{chap:evo}] \textit{Evolutionary and Population-Based Search} surveys canonical GAs, GP, CMA-ES, and Differential Evolution, emphasizing constraint handling, budget-aware population sizing, and integration with the rest of the toolkit.
\end{description}
}
\end{sloppypar}

\begin{tcolorbox}[summarybox,title={Four-level taxonomy in practice}]
\textbf{Level 1 (reactive systems):} Feedback loops and associative memories that respond instantly, e.g., Hopfield dynamics in \Cref{chap:hopfield} or the thermostat-style fuzzy controllers introduced across \Crefrange{chap:fuzzysets}{chap:fuzzyinference}.\\
\textbf{Level 2 (deliberative planners):} Rule-based systems that reason over an internal linguistic state before acting; see the fuzzy relation and inference machinery of \Crefrange{chap:fuzzysets}{chap:fuzzyinference}, where conditions aggregate before a crisp recommendation is issued.\\
\textbf{Level 3 (adaptive learners):} Data-driven models that update parameters from data, spanning the ERM toolkit (\Crefrange{chap:supervised}{chap:logistic}), perceptrons/MLPs/RBFs/CNNs (\Crefrange{chap:perceptron}{chap:rbf} and \Cref{chap:cnn}), sequence models and Transformers (\Crefrange{chap:rnn}{chap:transformers}), SOMs (\Cref{chap:som}), and population heuristics (\Cref{chap:evo}).\\
\textbf{Level 4 (meta-cognitive agents):} Algorithms that reason about their own learning loops: calibration and uncertainty estimation (\Crefrange{chap:supervised}{chap:logistic}), training diagnostics and early-stop policies (\Cref{chap:backprop}), alignment/PEFT tooling for Transformers (\Cref{chap:transformers}), and self-adaptive evolutionary strategies (\Cref{chap:evo}). These illustrate early steps toward systems that refine their own policies.
\end{tcolorbox}

\begin{figure}[!hb]
    \centering
    % Auto-fit the taxonomy diagram to the text width to avoid clipping
    \begin{adjustbox}{max width=\linewidth,center}
    \begin{tikzpicture}[x=2.6cm,y=1.35cm]
        % Column labels (levels)
        \foreach \L/\X in {1/1,2/2,3/3,4/4}{
            \node[font=\footnotesize] at (\X,6.6) {Level \L};
        }

        % Row labels (model nature)
        \foreach \N/\Y in {
            {Prob./kernel}/6,
            {Connectionist}/5,
            {Unsupervised / competitive}/4,
            {Fuzzy rule-based}/3,
            {Evolutionary / search}/2,
            {Energy-based}/1} {
            \node[anchor=east,font=\footnotesize] at (0.05,\Y) {\N};
        }

        % Light grid
        \foreach \y in {1,...,6}{
            \draw[gray!20] (0.4,\y) -- (4.6,\y);
        }
        \foreach \x in {1,...,4}{
            \draw[gray!20] (\x,0.7) -- (\x,6.3);
        }

        % Style for model cells
        \tikzset{model/.style={draw,rounded corners=1pt,minimum width=2.4cm,minimum height=0.9cm,font=\scriptsize,align=center}}

        % Legend (clean block centered below the grid, with generous spacing)
        \node[draw,rounded corners=2pt,fill=white,inner sep=6pt] at (2.5,-1.15) (legend) {%
            \begin{tikzpicture}[x=0.6cm,y=0.6cm]
                \matrix[matrix of nodes,
                        nodes={anchor=west,font=\scriptsize},
                        row sep=3pt,
                        column sep=8pt] {
                    \node[font=\scriptsize\bfseries,anchor=west] {Legend:}; &
                    \node[draw,rounded corners=1pt,fill=cbBlue!20,draw=cbBlue,minimum width=0.55cm,minimum height=0.3cm] {}; & supervised; &
                    \node[draw,rounded corners=1pt,fill=cbGreen!20,draw=cbGreen,minimum width=0.55cm,minimum height=0.3cm] {}; & self-/unsupervised; \\
                    & \node[draw,rounded corners=1pt,fill=cbOrange!20,draw=cbOrange,minimum width=0.55cm,minimum height=0.3cm] {}; & rules / hybrid; &
                    \node[draw,rounded corners=1pt,fill=cbPink!20,draw=cbPink,minimum width=0.55cm,minimum height=0.3cm] {}; & search / associative; \\
                };
            \end{tikzpicture}
        };

        % Model families (aligned to columns/rows)
        % Probabilistic / kernel row (y=6)
        \node[model,fill=cbBlue!20,draw=cbBlue] at (2.0,6.0) {Linear /\\ logistic};
        \node[model,fill=cbBlue!20,draw=cbBlue] at (3.0,6.0) {Kernel SVMs /\\ RBF nets};

        % Connectionist row (y=5)
        \node[model,fill=cbBlue!20,draw=cbBlue]   at (2.0,5.0) {MLPs / CNNs};
        \node[model,fill=cbGreen!20,draw=cbGreen] at (3.0,5.0) {RNNs /\\ seq.\ models};
        \node[model,fill=cbGreen!20,draw=cbGreen] at (4.0,5.0) {Transformers /\\ LMs};

        % Unsupervised / competitive row (y=4)
        \node[model,fill=cbGreen!20,draw=cbGreen] at (2.5,4.0) {SOMs};

        % Fuzzy rule-based row (y=3)
        \node[model,fill=cbOrange!20,draw=cbOrange] at (1.8,3.0) {Fuzzy\\ controllers};
        \node[model,fill=cbOrange!20,draw=cbOrange] at (2.6,3.0) {Fuzzy\\ inference};

        % Evolutionary / search row (y=2)
        \node[model,fill=cbPink!20,draw=cbPink] at (2.5,2.0) {GA / GP};

        % Energy-based row (y=1)
        \node[model,fill=cbPink!20,draw=cbPink] at (2.5,1.0) {Hopfield /\\ energy models};
    \end{tikzpicture}
    \end{adjustbox}
    \caption[Map of model families]{Schematic: Color-coded map of model families across agent level, model nature, and learning signal.}
    \label{fig:big-picture-map}
\end{figure}

\begin{table}[b]
\centering
\captionsetup{width=.92\linewidth}
\caption{Schematic: Big-picture view of model families across the taxonomy and learning paradigms. Each entry represents a family introduced in the book; supervision labels indicate the dominant training signal rather than strict exclusivity.}
\label{tab:big-picture-models}
\begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X c c >{\raggedright\arraybackslash}X@{}}
\toprule
Model family & Level & Nature & Learning signal \\
\midrule
Linear / logistic regression & 2--3 & probabilistic & supervised \\
Kernel SVMs and RBF networks & 2--3 & probabilistic / kernel & supervised \\
MLPs / CNNs & 3 & connectionist & supervised \\
RNNs / sequence models & 3 & connectionist & supervised / self-supervised \\
Transformers / attention LMs & 3--4 & connectionist & self-supervised \\
Self-organizing maps (SOMs) & 2--3 & unsupervised / competitive & unsupervised \\
Fuzzy controllers and inference systems & 1--2 & fuzzy rule-based & supervised / expert rules \\
Genetic algorithms and GP & 1--3 & evolutionary & search / fitness-driven \\
Hopfield and modern Hopfield variants & 1--3 & energy-based & associative / unsupervised \\
\bottomrule
\end{tabularx}
\end{table}

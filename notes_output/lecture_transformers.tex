% Chapter 14
\section{Transformers: Attention-Based Sequence Modeling}\label{chap:transformers}

\Cref{chap:rnn} made the sequence problem explicit: stateful computation plus gradients that must flow across time. \Cref{chap:nlp} then supplied the representation layer that makes those sequence objectives practical. This chapter keeps those representations but loosens the ``one step at a time'' constraint: instead of marching through time, we let each position look around and pull in what it needs through attention.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
After this chapter, you should be able to:
\begin{itemize}
  \item Explain the encoder--decoder bottleneck and how attention fixes it in sequence-to-sequence problems.
  \item Write scaled dot-product attention and multi-head attention, and interpret them as weighted averages.
  \item Distinguish self-attention from cross-attention and describe where each appears in a Transformer.
  \item Explain positional encodings and masking (padding/causal) in training and decoding.
  \item Describe a Transformer block (residual paths, layer norm, FFN) and common objectives (MLM/CLM) and families (BERT/GPT/encoder--decoder).
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Make information flow explicit. Attention is a controlled mixing operation; masks enforce which interactions are allowed; residual paths and normalization keep optimization stable as models deepen and context windows grow.
\end{tcolorbox}

\begin{tcolorbox}[perspectivebox, title={Author's note: models, world models, and language models (an opinionated lens)}]
A \emph{model} predicts outcomes from captured information. In the chapters so far, that ``outcome'' ranged
from a class label, to a time-step forecast, to a retrieved memory pattern.

A \emph{world model}, as I use the phrase here, is aspirational: it is the idea of a general-purpose internal
model that can represent situations and dynamics well enough to support planning, counterplanning, and
``what-if'' reasoning. Whether we can build such systems reliably is part of the larger research program.

Language models are a simplified and very constrained training interface to this aspiration: at training time
the next action is a token predicted from previous tokens. My opinion is that this setup can \emph{suggest}
a kind of internal world structure (because the output must remain internally and externally consistent to be
useful), but it does not \emph{guarantee} that a faithful world model has formed. Fluency is not proof of
understanding; it is a behavior you still have to audit.
\end{tcolorbox}

\subsection{From encoder--decoder bottlenecks to attention}
\label{sec:transformers_why_transformers_after_rnns}
Sequence-to-sequence (seq2seq) problems have a natural story: read an input sequence and produce an output sequence. Translation is the canonical example. In the classical encoder--decoder picture, the encoder reads the source sentence and produces a representation; the decoder then uses that representation to generate the target sentence token by token.

We will keep the term \emph{token} from \Cref{chap:nlp}: a token is a discrete symbol index that gets mapped to a vector. In text it might be a word or subword; in other sequence problems it can be any event ID you embed into a feature vector.

The practical bottleneck is the fixed-vector squeeze. If the decoder is only given one summary vector, it is forced to carry \emph{everything} about the source through time, even when the next output token only depends on a small part of the input. Attention is the engineering fix: at each decoding step, compute a weighted average of the encoder states and let that mixture act as the context for the next prediction. In other words, the decoder is allowed to look back at the encoder's ``memory'' and ask which source positions matter \emph{right now}.

Transformers \citep{Vaswani2017} take that idea seriously and push it further. They remove recurrence and make ``look around and mix'' the core operation inside a layer. This allows parallel computation across positions and makes long-range interactions a first-class design choice rather than a side effect of how well information survives through many recurrent steps.

\paragraph{Seq2seq with attention (cross-attention).}
In an encoder--decoder RNN, the encoder produces a sequence of hidden states \(\{\mathbf{h}_j\}_{j=1}^{S}\) from the input \(\{\mathbf{x}_j\}\), and the decoder produces a sequence of states \(\{\mathbf{s}_t\}_{t=1}^{T}\) while generating the output \(\{y_t\}\). A convenient probabilistic view of translation is
\begin{equation}
    p(\mathbf{y}\mid \mathbf{x})
    = \prod_{t=1}^{T} p\!\left(y_t \mid y_{<t}, \mathbf{x}\right).
    \label{eq:transformers_seq2seq_factorization}
\end{equation}
The ``one-vector bottleneck'' appears when the decoder is forced to rely on a single summary of the entire input. Attention replaces that with a \emph{per-step context}:
\begin{align}
    e_{t j} &= a(\mathbf{s}_{t-1}, \mathbf{h}_j), \label{eq:transformers_attn_score_seq2seq}\\
    \alpha_{t j} &= \operatorname{softmax}_j(e_{t j}), \label{eq:transformers_attn_weights_seq2seq}\\
    \mathbf{c}_t &= \sum_{j=1}^{S} \alpha_{t j}\,\mathbf{h}_j. \label{eq:transformers_context_seq2seq}
\end{align}
Here \(a(\cdot,\cdot)\) is any differentiable scoring function (a dot product, a bilinear form, or a small MLP) and \(\mathbf{c}_t\) is the context the decoder uses when predicting \(y_t\). That is the core move: different output positions can ``look back'' at different parts of the source. This is the bridge from encoder--decoder RNNs to Transformers: the attention math stays; what changes is that Transformers build the states \(\mathbf{h}_j\) and \(\mathbf{s}_t\) without recurrence.

\subsection{Scaled Dot-Product Attention}
\label{sec:transformers_scaled_dot_product_attention}
\begin{tcolorbox}[summarybox, title={Author's note: attention is a weighted average}]
I like to read attention as a weighted average over candidate pieces of information. A query asks a question (what do I need?), keys advertise what each candidate is about (what do I contain?), and values carry the content that gets mixed. Similarity scores between queries and keys become nonnegative weights that sum to one; the output is the weighted sum of the value vectors.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={A tiny analogy: keys, values, and weighted retrieval}]
Think of a tiny ``database'' with keys \(\{70,80\}\) and values \(\{1000,1500\}\). If your query is exactly \(70\), you retrieve \(1000\). If your query is \(75\), there is no exact match, so you can do a soft retrieval: score each key by a similarity such as
\[
s_i = \frac{1}{|k_i-q|+\epsilon},
\]
then normalize those scores so they sum to one and use them as weights on the values. Here \(|70-75|=|80-75|=5\), so \(s_1\approx s_2\approx 0.2\), the normalized weights are \((0.5,0.5)\), and the weighted average gives \(0.5\cdot 1000 + 0.5\cdot 1500 = 1250\).
\par\smallskip
This is only an intuition pump: Transformers do not use scalar keys like ``70.'' They learn vector keys and queries and score them in a learned feature space. But the weighted-average mechanism is exactly the same.
\end{tcolorbox}

At the per-position level, the computation is
\[
\mathbf{z}_i = \sum_{j} \alpha_{ij}\,\mathbf{v}_j,\qquad
\alpha_{ij} \ge 0,\quad \sum_j \alpha_{ij} = 1,
\]
Given query, key, value matrices \(\mathbf{Q} \in \mathbb{R}^{n_q \times d_k}\), \(\mathbf{K} \in \mathbb{R}^{n_k \times d_k}\), and \(\mathbf{V} \in \mathbb{R}^{n_k \times d_v}\), the basic attention operation is
\begin{equation}
    \operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
    = \operatorname{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right) \mathbf{V}.
    \label{eq:transformers_scaled_dot_product_attention}
\end{equation}
Here \(n_q\) is the sequence length of the queries and \(n_k\) the sequence length of keys/values. We keep the same sequence-first convention used in \Cref{chap:rnn,chap:nlp}: rows index time positions (a ``token dimension'') and columns index features, while batch elements are processed independently. The \(1/\sqrt{d_k}\) factor stabilizes gradients by keeping logits in a reasonable range.

\paragraph{Where do \(Q,K,V\) come from?}
Start from token vectors stacked into a matrix \(\mathbf{X}\in\mathbb{R}^{n\times d_{\text{model}}}\) (one row per position). For a \emph{single head}, learned projections produce
\begin{equation}
    \mathbf{Q}=\mathbf{X}\mathbf{W}^Q,\qquad
    \mathbf{K}=\mathbf{X}\mathbf{W}^K,\qquad
    \mathbf{V}=\mathbf{X}\mathbf{W}^V,
    \label{eq:transformers_qkv_projections}
\end{equation}
with \(\mathbf{W}^Q,\mathbf{W}^K\in\mathbb{R}^{d_{\text{model}}\times d_k}\) and \(\mathbf{W}^V\in\mathbb{R}^{d_{\text{model}}\times d_v}\).
In self-attention, \(\mathbf{X}\) is the same sequence for all three. In cross-attention (seq2seq), \(\mathbf{Q}\) typically comes from decoder states while \(\mathbf{K},\mathbf{V}\) come from encoder states; the math is unchanged. Multi-head attention simply runs this projection-and-attend pattern several times in parallel with separate \(\mathbf{W}_i^Q,\mathbf{W}_i^K,\mathbf{W}_i^V\).

\begin{tcolorbox}[summarybox, title={Worked example: 2-token causal self-attention (one head)}]
Let \(d_k=d_v=2\) and \(Q=K=V=\mathbf{I}_2\). Without masking, the scaled scores are
\[
S=\frac{QK^\top}{\sqrt{d_k}}=\frac{1}{\sqrt{2}}\mathbf{I}_2.
\]
A \emph{causal mask} forbids looking into the future: for query index \(i\), mask out all keys with index \(j>i\) by setting those logits to \(-\infty\) (so they vanish after the softmax).

Row 1 can only attend to itself, so \(\alpha_{1\cdot}=[1,0]\). Row 2 sees logits \([0,\,1/\sqrt{2}]\), so
\[
\begin{aligned}
u &= e^{1/\sqrt{2}},\\
\alpha_{2\cdot} &= \operatorname{softmax}\!\left([0,\,1/\sqrt{2}]\right)
= \left[\frac{1}{1+u},\;\frac{u}{1+u}\right]\\
&\approx[0.330238,\;0.669762].
\end{aligned}
\]
Because \(V=\mathbf{I}_2\), the attention output equals the weight matrix: \(\operatorname{Attn}(Q,K,V)=\alpha\).
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Shape ledger}]
We treat mini\hyp{}batches as \(\mathbf{X}\in\mathbb{R}^{B\times n \times d_{\text{model}}}\) (batch, sequence, features). After the linear projections each head carries \(\mathbf{Q},\mathbf{K}\in\mathbb{R}^{B\times h \times n \times d_k}\), \(\mathbf{V}\in\mathbb{R}^{B\times h \times n \times d_v}\), and the attention weights live in \(\mathbb{R}^{B\times h \times n \times n}\). Reading dimensions in this order avoids confusion when mixing frameworks; \(h\cdot d_k = d_{\text{model}}\) (often \(d_v=d_k\)). FFN inner widths typically 2--4$\times d_{\text{model}}$.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Complexity and memory}]
Naive attention is \(O(n^2 d_{\text{model}})\) compute and \(O(n^2)\) memory per head/layer for the attention map; this dominates long sequences. FlashAttention reduces activation I/O but keeps the quadratic arithmetic; sparse/linear variants reduce the \(n^2\) factor by trading exactness for structure (see Longformer/BigBird/Reformer/Performer/Linformer). Causal/padding masks do not change complexity, only which entries participate.
\end{tcolorbox}

\subsection{Self-attention vs.\ cross-attention}
\label{sec:transformers_self_attention_vs_cross_attention}
The same equations serve two distinct roles.
\emph{Self-attention} means \(Q,K,V\) come from the same sequence. This is how a token in a sentence can incorporate information from other tokens in that sentence. In decoder-only generation, self-attention is typically \emph{causal}: the mask enforces that position \(t\) can only use positions \(\le t\).
Self-attention by itself does not know what ``before'' and ``after'' mean: without positional information the operation is permutation-equivariant. That is why we explicitly add positional encodings in \Cref{sec:transformers_positional_information}.

\emph{Cross-attention} is the seq2seq bridge. Here the queries come from the decoder states, but the keys and values come from the encoder outputs. In translation terms: each output position asks a question (query) and then pulls a weighted average over the source-side memory (keys/values) to decide what to emit next.

\subsection{Multi-Head Attention (MHA)}
\label{sec:transformers_multi_head_attention_mha}
One attention head gives you one similarity space. Multi-head attention gives you several in parallel: each head learns its own projections and can focus on different relations at the same time (local vs.\ global cues, syntactic vs.\ semantic signals, or different parts of an image-like grid). You should not read heads as guaranteed ``modules'' with fixed roles; the point is capacity and parallel views under the same weighted-average mechanism.

Multiple heads attend in parallel after learned linear projections:
\begin{align}
    \mathrm{head}_i &= \operatorname{Attn}(\mathbf{X}\mathbf{W}_i^Q,\, \mathbf{X}\mathbf{W}_i^K,\, \mathbf{X}\mathbf{W}_i^V),\\
    \operatorname{MHA}(\mathbf{X}) &= [\mathrm{head}_1;\ldots;\mathrm{head}_h]\, \mathbf{W}^O,
    \label{eq:transformers_mha}
\end{align}
with \(\mathbf{W}_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}\), \(\mathbf{W}_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\), \(\mathbf{W}_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\), and output projection \(\mathbf{W}^O\in\mathbb{R}^{(h d_v)\times d_{\text{model}}}\).
For cross-attention, the formula is the same but keys/values come from encoder states: \(\operatorname{Attn}(\mathbf{X}_{\text{dec}}\mathbf{W}_i^Q,\,\mathbf{X}_{\text{enc}}\mathbf{W}_i^K,\,\mathbf{X}_{\text{enc}}\mathbf{W}_i^V)\).
\Cref{fig:lec13_transformer_block} bundles scaled dot-product attention, multi-head concatenation, and the residual pre-LN block so the entire signal path is visible at a glance.

\begin{figure}[h]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[
        >={Latex[length=4pt, width=3pt]},
        font=\small\sffamily,
        % Panels
        panel/.style={draw=gray!40, rounded corners=5pt, fill=gray!3, line width=0.6pt},
        panel_label/.style={anchor=north west, font=\bfseries\footnotesize, text=gray!80, inner sep=4pt},
        % Functional blocks
        func/.style={draw=cbBlue!80, fill=cbBlue!10, rounded corners=2pt, line width=0.7pt, minimum height=1.8em, align=center, inner sep=4pt},
        % Operation blocks
        proc/.style={draw=cbGreen!80, fill=cbGreen!10, rounded corners=2pt, line width=0.7pt, minimum height=1.8em, align=center},
        % Warning/mask blocks
        maskblock/.style={draw=cbOrange!80, fill=cbOrange!10, rounded corners=2pt, line width=0.7pt, minimum height=1.6em, align=center},
        % Merge/add/concat
        sum/.style={circle, draw=cbPink!80, fill=cbPink!10, line width=0.7pt, inner sep=1pt, minimum size=1.2em},
        merge/.style={draw=cbPink!80, fill=cbPink!10, rounded corners=2pt, line width=0.7pt, minimum height=1.8em, align=center},
        % Wires + annotation
        wire/.style={->, draw=gray!70, line width=0.8pt, rounded corners=3pt},
        skip/.style={->, draw=gray!55, line width=0.8pt, rounded corners=6pt},
        annot/.style={font=\scriptsize, text=gray!60, align=left}
    ]
        % ==========================
        % Panel (a): Scaled Dot-Product
        % ==========================
        \node[panel, minimum width=4.2cm, minimum height=6.2cm] (p1) at (0,0) {};
        \node[panel_label] at (p1.north west) {(a) Scaled Dot-Product};

        \node[func, minimum width=0.8cm] (q) at ([yshift=-1.2cm, xshift=-1.2cm]p1.north) {\(\mathbf{Q}\)};
        \node[func, minimum width=0.8cm] (k) at ([yshift=-1.2cm]p1.north) {\(\mathbf{K}\)};
        \node[func, minimum width=0.8cm] (v) at ([yshift=-1.2cm, xshift=1.2cm]p1.north) {\(\mathbf{V}\)};

        \node[proc, minimum width=2.4cm] (matmul1) at ([yshift=-1.0cm]k.south) {MatMul};
        \node[annot, right] at (matmul1.east) {\(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\)};

        \node[maskblock, minimum width=2.4cm] (masknode) at ([yshift=-0.7cm]matmul1.south) {Mask (opt.)};
        \node[proc, minimum width=2.4cm] (softmax) at ([yshift=-0.7cm]masknode.south) {Softmax};
        \node[proc, minimum width=2.4cm] (matmul2) at ([yshift=-0.9cm]softmax.south) {MatMul};
        \node[annot, right] at (matmul2.east) {Weighted\\Sum};

        \draw[wire] (q.south) -- ++(0,-0.3) -| ([xshift=-4pt]matmul1.north);
        \draw[wire] (k.south) -- (matmul1.north);
        \draw[wire] (matmul1.south) -- (masknode.north);
        \draw[wire] (masknode.south) -- (softmax.north);
        \draw[wire] (softmax.south) -- (matmul2.north);
        % Route V -> MatMul2 with an "outside" elbow so it avoids the right-side annotations.
        \coordinate (vRouteX) at ($(p1.east)+(0.55,0)$);
        \draw[wire] (v.east) -- (vRouteX |- v.east) |- (matmul2.north east);
        \draw[wire] (matmul2.south) -- ++(0,-0.4);

        % ==========================
        % Panel (b): Multi-Head
        % ==========================
        \node[panel, minimum width=4.2cm, minimum height=6.2cm] (p2) at (5,0) {};
        \node[panel_label] at (p2.north west) {(b) Multi-Head};

        \node (input_b) at ([yshift=-0.8cm]p2.north) {};
        \node[func, fill=white, draw=gray!30, minimum width=2.8cm, yshift=2pt, xshift=2pt] at ([yshift=-0.6cm]input_b.south) {};
        \node[func, minimum width=2.8cm] (proj) at ([yshift=-0.6cm]input_b.south) {Projections\\\(W_i^Q, W_i^K, W_i^V\)};

        \node[proc, fill=white, draw=gray!30, minimum width=2.2cm, yshift=2pt, xshift=2pt] at ([yshift=-0.9cm]proj.south) {};
        \node[proc, minimum width=2.2cm] (head) at ([yshift=-0.9cm]proj.south) {Attention\\Head \(i\)};
        \node[annot, right] at (head.east) {\(\times h\)};

        \node[merge, minimum width=2.8cm] (concat) at ([yshift=-0.8cm]head.south) {Concat};
        \node[func, minimum width=2.8cm] (out_proj) at ([yshift=-0.8cm]concat.south) {Output \(W^O\)};

        \draw[wire] (proj.south) -- (head.north);
        \draw[wire] (head.south) -- (concat.north);
        \draw[wire] (concat.south) -- (out_proj.north);
        \draw[wire] (out_proj.south) -- ++(0,-0.4);

        % ==========================
        % Panel (c): Pre-LN Encoder
        % ==========================
        \node[panel, minimum width=4.2cm, minimum height=6.2cm] (p3) at (10,0) {};
        \node[panel_label] at (p3.north west) {(c) Pre-LN Block};

        \node[coordinate] (in_c) at ([yshift=-1.0cm]p3.north) {};

        \node[func, minimum width=2.4cm] (ln1_real) at ([yshift=-0.6cm]in_c) {LayerNorm};
        \node[proc, minimum width=2.4cm] (mha_real) at ([yshift=-0.6cm]ln1_real.south) {Multi-Head Attn};
        \node[sum] (add1_real) at ([yshift=-0.5cm]mha_real.south) {+};

        \node[func, minimum width=2.4cm] (ln2_real) at ([yshift=-0.6cm]add1_real.south) {LayerNorm};
        \node[proc, minimum width=2.4cm] (ffn_real) at ([yshift=-0.6cm]ln2_real.south) {FFN (GELU)};
        \node[sum] (add2_real) at ([yshift=-0.5cm]ffn_real.south) {+};

        \draw[wire] (in_c) -- (ln1_real.north);
        \draw[wire] (ln1_real.south) -- (mha_real.north);
        \draw[wire] (mha_real.south) -- (add1_real.north);
        \draw[wire] (add1_real.south) -- (ln2_real.north);
        \draw[wire] (ln2_real.south) -- (ffn_real.north);
        \draw[wire] (ffn_real.south) -- (add2_real.north);
        \draw[wire] (add2_real.south) -- ++(0,-0.4);

        % Residual skips (explicit bypass into each residual add)
        \draw[skip] ([yshift=0.2cm]ln1_real.north) -- ++(-1.6,0) |- node[pos=0.2, above, font=\scriptsize, text=gray!55]{residual} (add1_real.west);
        \draw[skip] ([yshift=0.2cm]ln2_real.north) -- ++(-1.6,0) |- node[pos=0.2, above, font=\scriptsize, text=gray!55]{residual} (add2_real.west);

        \node[draw=gray!50, dotted, fill=white, rounded corners, font=\tiny, align=left, inner sep=2pt] (postln) at ([xshift=1.3cm]add2_real.east) {Post-LN:\\Norm here};
        \draw[->, dotted, gray!60] (postln.west) -- (add2_real.east);
    \end{tikzpicture}}
    % Avoid dense inline math in captions; it wraps poorly in EPUB renderers.
    \caption{Reference schematic for the Transformer. Left: scaled dot-product attention. Center: multi-head concatenation with an output projection. Right: pre-LN encoder block combining attention, FFN, and residual connections; a post-LN variant simply moves each LayerNorm after its residual add (dotted alternative, not shown).}
    \label{fig:lec13_transformer_block}
\end{figure}
\FloatBarrier

\Cref{fig:lec13_micro_figures} provides a quick visual summary of positional encoding, KV cache reuse, and LoRA adapters.

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[
        font=\small\sffamily,
        enc/.style={thick, smooth},
        panel/.style={draw=gray!55, rounded corners=4pt, thick, fill=gray!6, inner sep=6pt, minimum width=4.6cm, minimum height=3.6cm},
        box/.style={draw=gray!60, rounded corners=3pt, thick, fill=gray!5, inner sep=6pt},
        subbox/.style={draw=gray!60, rounded corners=3pt, thick, fill=cbOrange!20, inner sep=4pt},
        mat/.style={draw=gray!60, rounded corners=3pt, thick, fill=cbBlue!12, minimum width=2cm, minimum height=1cm},
        smallmat/.style={draw=gray!60, rounded corners=3pt, thick, fill=cbGreen!25, minimum width=1.4cm, minimum height=0.7cm}
    ]

    % Left panel: positional encodings
    \begin{scope}[xshift=0cm]
        \node[panel] (p1) {};
        \begin{axis}[
            at={(p1.center)},
            anchor=center,
            width=3.9cm,
            height=2.8cm,
            axis lines=none,
            xmin=0, xmax=2*pi,
            ymin=-1.2, ymax=1.2,
            clip=false
        ]
            \addplot[enc, cbBlue, samples=200, domain=0:2*pi] {sin(deg(x))};
            \addplot[enc, cbOrange, samples=200, domain=0:2*pi] {sin(deg(2*x))};
        \end{axis}
        \node[anchor=south, font=\small] at ([yshift=12pt]p1.south) {Positional encodings};
        \node[anchor=north, font=\small] at ([yshift=2pt]p1.south) {sinusoidal / RoPE};
        \node[anchor=north west, font=\footnotesize] at ([xshift=4pt, yshift=-4pt]p1.north west) {(a)};
    \end{scope}

    % Middle panel: Decoder + KV Cache
    \begin{scope}[xshift=5.8cm]
        \node[panel] (p2) {};
        \node[box, minimum width=2.6cm, font=\small] (dec) at (p2.center|-0,0.65) {Decoder block};
        \node[subbox, minimum width=2.6cm, font=\small] (kv) at (p2.center|-0,-0.65) {K/V cache};
        \draw[->, line width=0.9pt] (dec) -- (kv);
        \node[anchor=north west, font=\footnotesize] at ([xshift=4pt, yshift=-4pt]p2.north west) {(b)};
    \end{scope}

    % Right panel: LoRA adapters
    \begin{scope}[xshift=11.6cm]
        \node[panel] (p3) {};
        % horizontal BA feeding W
        \node[mat, font=\small] (W) at (p3.center|-0,1.35) {$\mathbf{W}$};
        \node[smallmat, font=\small] (B) at ([xshift=-1.7cm]p3.center|-0,-0.15) {$\mathbf{B}$};
        \node[smallmat, font=\small] (A) at ([xshift=1.7cm]p3.center|-0,-0.15) {$\mathbf{A}$};
        \draw[->, line width=0.9pt] (B.north) -- ($(W.south)!0.55!(W.south west)$);
        \draw[->, line width=0.9pt] (A.north) -- ($(W.south)!0.55!(W.south east)$);
        \node[font=\scriptsize, anchor=south] at ([yshift=2pt]W.south) {rank $r$};
        \node[font=\small] at ([yshift=-12pt]p3.south) {LoRA adapters};
        \node[anchor=north west, font=\footnotesize] at ([xshift=4pt, yshift=-4pt]p3.north west) {(c)};
    \end{scope}

    % Cross-panel connectivity arrows
    % Token flowing from positional encodings into the decoder block
    \draw[->, line width=0.9pt, draw=gray!65]
        ([xshift=0.25cm]p1.east) --
        ([xshift=-0.25cm]p2.west)
        node[midway, above, font=\small] {token};
    % Reuse of cached K/V feeding the following decoding step
    \draw[->, line width=0.9pt, draw=gray!65]
        ([xshift=0.25cm]kv.east) --
        ([xshift=-0.25cm]p3.west|-kv.east)
        node[midway, above, font=\small] {reuse};

    \end{tikzpicture}
    }%
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Transformer micro-views. Left: positional encodings (sinusoidal/rotary) add order information. Center: KV cache stores past keys/values so decoding a new token reuses prior context. Right: LoRA inserts low-rank adapters (B A) on top of a frozen weight matrix W for parameter-efficient tuning.}
    \label{fig:lec13_micro_figures}
\end{figure}
\FloatBarrier

\subsection{Positional Information}
\label{sec:transformers_positional_information}
Transformers lack recurrence, so order has to be injected explicitly. A simple baseline (from \citet{Vaswani2017}) is a sinusoidal positional encoding: for position \(\mathrm{pos}\) and feature index \(i\),
\begin{align}
    \mathrm{PE}(\mathrm{pos},\,2i)   &= \sin\!\left(\frac{\mathrm{pos}}{10000^{2i/d_{\text{model}}}}\right),\\
    \mathrm{PE}(\mathrm{pos},\,2i+1) &= \cos\!\left(\frac{\mathrm{pos}}{10000^{2i/d_{\text{model}}}}\right).
    \label{eq:transformers_sinusoidal_pe}
\end{align}
You add this vector (or a learned alternative) to the token embedding at each position. The engineering point is not the specific constant; it is that different dimensions oscillate at different frequencies, so nearby positions look similar in some coordinates and far positions look different in others.

In modern practice you will also see learned positional embeddings, relative-position schemes, and rotary position embeddings (RoPE). The chapter keeps sinusoidal PE as the clean reference, and we treat other variants as drop-in replacements that mainly change how well models extrapolate to longer contexts or shifting windows.

\subsection{Objectives, masks, and model families}
\label{sec:transformers_masks_and_training_objectives}
Two masks show up so often that it is worth naming them early.
A \emph{causal mask} forbids attention to future positions; this is what makes next-token generation well-defined.
A \emph{padding mask} forbids attention to padded positions inserted only to make batches rectangular.
Both are easy to get wrong: a single missing mask can leak information from the future or silently change what the model is allowed to use.
See \Cref{fig:lec13_masks} for a concrete picture of both patterns (queries on rows, keys on columns).

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
            \begin{groupplot}[
                group style={group size=2 by 1, horizontal sep=1.2cm},
                width=0.42\linewidth,
                height=0.36\linewidth,
                view={0}{90},
            xmin=-0.5, xmax=4.5,
            ymin=-0.5, ymax=4.5,
            xtick={0,...,4},
            ytick={0,...,4},
            xticklabels={t\(_0\), t\(_1\), t\(_2\), t\(_3\), t\(_4\)},
            yticklabels={t\(_0\), t\(_1\), t\(_2\), t\(_3\), t\(_4\)},
                xticklabel style={font=\scriptsize},
                yticklabel style={font=\scriptsize},
                colorbar,
                colorbar style={height=3.0cm},
                colormap/viridis,
                point meta min=0, point meta max=1,
                nodes near coords,
                nodes near coords align={center},
                every node near coord/.append style={
                    font=\scriptsize,
                    fill=white,
                    fill opacity=0.75,
                    text opacity=1,
                    inner sep=1.0pt
                }
            ]
            \nextgroupplot[title={Padding mask}]
                \addplot[matrix plot*, mesh/cols=5, mesh/rows=5, point meta=explicit] table [meta=z] {
                    x y z
                    0 0 1
                    1 0 1
                    2 0 1
                    3 0 0
                    4 0 0
                    0 1 1
                    1 1 1
                    2 1 1
                    3 1 0
                    4 1 0
                    0 2 1
                    1 2 1
                    2 2 1
                    3 2 0
                    4 2 0
                    0 3 1
                    1 3 1
                    2 3 1
                    3 3 0
                    4 3 0
                    0 4 1
                    1 4 1
                    2 4 1
                    3 4 0
                    4 4 0
                };
                \node[font=\scriptsize, anchor=north west] at (axis cs:-0.4,4.4) {keep};
                \node[font=\scriptsize, anchor=south east] at (axis cs:3.4,0.0) {mask};
            \nextgroupplot[title={Causal mask}]
                \addplot[matrix plot*, mesh/cols=5, mesh/rows=5, point meta=explicit] table [meta=z] {
                    x y z
                    0 0 1
                    1 0 0
                    2 0 0
                    3 0 0
                    4 0 0
                    0 1 1
                    1 1 1
                    2 1 0
                    3 1 0
                    4 1 0
                    0 2 1
                    1 2 1
                    2 2 1
                    3 2 0
                    4 2 0
                    0 3 1
                    1 3 1
                    2 3 1
                    3 3 1
                    4 3 0
                    0 4 1
                    1 4 1
                    2 4 1
                    3 4 1
                    4 4 1
                };
                \node[font=\scriptsize, anchor=north east] at (axis cs:4.4,4.4) {future masked};
        \end{groupplot}
    \end{tikzpicture}
    \caption{Attention masks as heatmaps (queries on rows, keys on columns). Left: padding mask zeroes out attention to padded positions of a shorter sequence in a packed batch. Right: causal mask enforces autoregressive flow by blocking attention to future tokens.}
    \label{fig:lec13_masks}
\end{figure}
\FloatBarrier

The training objective is usually a cross-entropy (CE) loss, i.e., a negative log-likelihood (NLL) of the correct label under the model's predicted distribution. In sequence modeling, you typically sum (or average) that loss across time positions and across batch elements.

We will return to the main Transformer families (encoder-only vs.\ decoder-only vs.\ encoder--decoder) after we write down the block structure in \Cref{sec:transformers_encoder_decoder_stacks_and_stabilizers}. The core idea is simple: families differ mostly by which masks they apply and which prediction problem they are trained on, not by a different attention mechanism.

\subsection{Encoder/Decoder Stacks and Stabilizers}
\label{sec:transformers_encoder_decoder_stacks_and_stabilizers}
Each block uses residual connections and layer normalization:
\begin{align}
\mathbf{H}' &= \operatorname{LayerNorm}(\mathbf{H} + \operatorname{MHA}(\mathbf{H},\mathbf{H},\mathbf{H})),\\
\mathbf{H}_{\text{out}} &= \operatorname{LayerNorm}(\mathbf{H}' + \operatorname{FFN}(\mathbf{H}')).
    \label{eq:auto:lecture_transformers:2}
\end{align}
The feed-forward sublayer (FFN) is position-wise, typically two linear layers with a nonlinearity (e.g., GELU). Dropout and label smoothing are common.

\begin{tcolorbox}[summarybox, breakable, title={Implementation snapshot: block pseudocode, training defaults, and one step}]
\noindent\textbf{Block pseudocode (pre-LN).}
\begin{verbatim}
function Block(H):
    # Pre-normalize inputs (pre-LN stabilizes deep stacks)
    H_norm = LayerNorm(H)
    attn = MHA(H_norm, H_norm, H_norm)
    H = H + Dropout(attn)
    H_norm = LayerNorm(H)
    ff = FFN(H_norm)
    return H + Dropout(ff)
\end{verbatim}
Decoder blocks add causal masks and cross-attention with encoder states. Pre-LN (shown here) is now common because it keeps gradients well behaved for very deep stacks; post-LN (original Transformer) is still used in smaller models.
\medskip
\noindent\textbf{Training defaults (decoder-only, practical baseline).}
AdamW with cosine decay and 1--3\% warmup; LR \(\sim 10^{-3}\) for small models, \(1\text{--}2\times 10^{-4}\) for mid-size. Weight decay \(\approx 0.01\) (exclude biases/LayerNorm gains). Attention/MLP dropout \(\approx 0.1\); clip global norm to 1.0. Mixed precision (FP16/BF16) plus gradient checkpointing for long contexts; tie input embeddings to the LM head; use causal masks for CLM and padding masks for packed batches.

\medskip
\noindent\textbf{One training step (decoder-only, causal mask).}
\begin{verbatim}
x = tokenizer(batch_text)                # [B, T]
mask = causal_mask(x)                    # [B, 1, T, T]
h = embed(x) + pos(x)                    # [B, T, d_model]
for block in blocks:
    h = block(h, mask)                   # pre-LN MHA + FFN
logits = lm_head(h)                      # [B, T, vocab]
loss = CE(logits[:, :-1], x[:, 1:])      # next-token
loss.backward()
clip_grad_norm_(model.parameters(), 1.0)
opt.step(); opt.zero_grad()
\end{verbatim}
At inference, reuse cached K/V states (see \Cref{fig:lec13_micro_figures} and \Cref{sec:transformers_advanced_attention_and_efficiency_notes_snapshot}).
\end{tcolorbox}
\paragraph{Code--math dictionary.} In code blocks, \texttt{x} is the token-index tensor (input IDs), \texttt{h} is the hidden-state array \(\mathbf{H}\), \texttt{mask} is the attention mask, and \texttt{embed(x)} denotes an embedding lookup into the learned table \(\mathbf{W}\) (written algebraically as a row-selection, e.g., \(\mathbf{W}[w_t]\), in \Cref{chap:nlp}).

\subsection{BERT vs.\ GPT vs.\ encoder--decoder}
\label{sec:transformers_model_families}
Once you have attention blocks, most of the ``model family'' distinctions come from two choices: (i) which directions a position is allowed to attend to, and (ii) what prediction problem you train on.
\begin{itemize}
    \item \textbf{Encoder-only (BERT-style):} no causal mask; each token can use information from both left and right. Training commonly hides some input tokens and asks the model to predict the missing pieces (masked language modeling, MLM). A pooled vector (often a prepended \texttt{[CLS]} token) is then used as a sentence representation for classification.
    \item \textbf{Decoder-only (GPT-style):} causal mask; position \(t\) can only attend to positions \(\le t\). Training is next-token prediction (causal language modeling, CLM). Inference is the same loop run forward: predict the next token, append it, and repeat.
    \item \textbf{Encoder--decoder (seq2seq):} the encoder reads the source; the decoder generates the target with causal self-attention plus cross-attention into the encoder outputs. This is the cleanest fit for translation: each output position pulls a weighted average over source-side memory and then commits to one more token.
\end{itemize}
Some BERT variants also used ``next sentence prediction'' (NSP) as an auxiliary task historically; it is not essential for understanding the core encoder-only idea.

\subsection{Long Contexts and Efficient Attention}
\label{sec:transformers_long_contexts_and_efficient_attention}
Memory and compute scale quadratically with sequence length. Practical systems therefore mix several tricks:
\begin{itemize}
    \item \textbf{Sparse or local attention} (e.g., Longformer, BigBird) to limit each query to a sliding or block-sparse neighborhood.
    \item \textbf{Low-rank/kernelized approximations} and recurrent chunking (Performer, Transformer-XL) so that computation/storage grows roughly linearly in context length.
    \item \textbf{I/O-aware kernels} such as FlashAttention that stream tiles through SRAM so the \(O(n^2)\) attention computation remains exact while memory stays manageable.
\end{itemize}
Relative/rotary position schemes and KV caching are summarized in \Cref{sec:transformers_advanced_attention_and_efficiency_notes_snapshot} and \Cref{fig:lec13_micro_figures}.

\subsection{Fine-Tuning and Parameter-Efficient Adaptation}
\label{sec:transformers_fine_tuning_and_parameter_efficient_adaptation}
Pre-training gives you a general-purpose language model; fine-tuning adapts that model to a task, domain, or interaction style. \emph{Full fine-tuning} updates all weights, which can yield the best performance when you have enough high-quality data and a stable objective, but it is also the easiest to overfit or to accidentally ``forget'' useful general behavior.

\emph{Parameter-efficient} methods (LoRA, adapters, prefix/prompt tuning, and related variants) inject small trainable modules while freezing most of the base model, enabling rapid adaptation with lower memory and more predictable changes. Practically, PEFT is attractive when you want many task-specific variants of a shared base model, or when you need to keep the base weights fixed for deployment and auditing.

\paragraph{Audit hooks for adaptation.}
Regardless of whether you update all weights or only a small adapter, treat fine-tuning like any other ERM pipeline: keep a held-out evaluation set that matches the deployment slice, monitor calibration and failure modes (not just loss), and log the exact base checkpoint, tokenizer, and data snapshot so results are reproducible. When fine-tuning for instruction-following or conversational behavior, add explicit tests for regressions (refusals, hallucinations on factual probes, and brittleness to prompt variants) rather than relying on a single aggregate score.

\subsection{Decoding and Evaluation}
\label{sec:transformers_decoding_and_evaluation}
Training produces a distribution over the next token; decoding turns that distribution into an actual sequence. For decoder-only models, decoding runs autoregressively: predict \(p_\theta(x_{t+1}\mid x_{1:t})\), choose a token, append it, and repeat. This is also where the KV cache matters (see \Cref{fig:lec13_micro_figures}): you reuse past keys/values so generating one more token does not require recomputing attention over the entire prefix from scratch.

Greedy decoding takes the argmax at each step; it is fast and often strong for short, factual completions, but it can get stuck in repetitive loops. Beam search keeps multiple partial hypotheses; it can improve likelihood but sometimes harms perceived quality in open-ended generation. Sampling (top-\(k\), top-\(p\) nucleus) trades certainty for diversity; temperature controls how sharp or flat the distribution feels.

For evaluation, perplexity summarizes next-token performance for language modeling (see \Cref{chap:nlp}), but it does not tell you whether generations are useful, safe, or faithful. For downstream classification, prefer metrics that match the deployment slice (e.g., AUPRC for imbalanced problems) and keep decoding settings logged alongside checkpoints so results are reproducible.

\subsection{Audit and failure modes (short list)}
\label{sec:transformers_audit_and_failure_modes}
\begin{tcolorbox}[summarybox, title={Audit and failure modes (engineering view)}]
\begin{itemize}
    \item \textbf{Masking bugs:} missing/incorrect causal masks can leak future tokens; missing padding masks can let padding dominate attention in batched training.
    \item \textbf{Train/inference mismatch:} teacher forcing during training does not automatically tell you how errors compound during decoding; test the decoding strategy you plan to ship.
    \item \textbf{Long-context degradation:} attention enables long-range access, but quality can still decay with length; measure how performance changes as you increase context.
    \item \textbf{Calibration vs.\ correctness:} high probability is not a guarantee of correctness; audit reliability on slices and stress tests, not just average loss.
    \item \textbf{Reproducibility:} tokenizer choices, data filters, and decoding hyperparameters can swing results; log them as part of the experiment.
\end{itemize}
\end{tcolorbox}

\subsection{Alignment (Brief)}
\label{sec:transformers_alignment_brief}
Post-training \emph{alignment} shapes model behavior to match human preferences, safety constraints, and interaction norms. In broad terms, alignment objectives do not change the Transformer mechanics; they change what you reward during optimization (and therefore what behaviors are reinforced).

RLHF optimizes a policy against a learned reward model (with careful regularization to avoid drifting too far from the base model). Preference-based objectives such as DPO, KTO, or ORPO optimize directly from ranked pairs without a full reinforcement-learning loop.

\paragraph{Alignment is not a proof of correctness.}
Alignment can improve helpfulness and reduce obvious failures, but it can also introduce new ones (reward hacking, over-refusal, brittleness to prompt phrasing, or degraded performance off-distribution). Treat it as an engineering stage with explicit test suites and logging: evaluate on held-out tasks, check calibration and refusal behavior, and track changes relative to the pre-alignment model.

\subsection{Advanced attention and efficiency notes (practitioner snapshot)}
\label{sec:transformers_advanced_attention_and_efficiency_notes_snapshot}
\begin{itemize}
    \item \textbf{Relative/rotary positions.} RoPE \citep{Su2021RoPE} and ALiBi \citep{Press2022ALiBi} replace absolute sinusoidal embeddings with rotation/bias terms so extrapolating to longer sequences no longer requires re-fitting positional lookups; the trade-off is that absolute tables keep fixed anchors for classification tokens while rotary/relative schemes favour extrapolation and smoothly sliding windows.
    \item \textbf{KV-cache management.} Decoder-only inference stores per-layer key/value tensors; chunked caching, paged attention, and sliding windows keep memory linear in context length. Speculative decoding and assisted decoding reuse a lightweight draft model to propose tokens that the full model verifies before committing.
    \item \textbf{Efficient kernels.} FlashAttention \citep{Dao2022FlashAttention} computes attention blocks in streaming tiles to keep activations in SRAM. Long-context variants mix windowed attention, recurrent memory, or low-rank adapters; state-space models such as Mamba \citep{Gu2023Mamba} provide linear-time alternatives that back-propagate through implicitly defined kernels.
    \item \textbf{Mixture-of-experts and routing.} Sparse MoE layers \citep{Shazeer2017MoE} add conditional capacity; router z-losses, capacity factors, and load-balancing losses are essential to avoid expert collapse.
    \item \textbf{Test-time scaling.} Curriculum-based sampling (nucleus, temperature annealing), classifier-free guidance, and beam-search variants all tune the accuracy/latency frontier; plan to log decoding hyperparameters alongside checkpoints so experiments are reproducible.
\end{itemize}
Parameter-efficient tuning methods are covered in \Cref{sec:transformers_fine_tuning_and_parameter_efficient_adaptation}.

\subsection{RNNs vs. Transformers: When and Why}
\label{sec:transformers_rnns_vs_transformers_when_and_why}
\begin{center}
\begin{tabular}{@{}>{\raggedright\arraybackslash}p{0.24\linewidth} >{\raggedright\arraybackslash}p{0.34\linewidth} >{\raggedright\arraybackslash}p{0.34\linewidth}@{}}
\toprule
 & \textbf{RNN/LSTM/GRU} & \textbf{Transformer} \\
\midrule
Parallelism & Limited (sequential) & High (tokens in parallel) \\
Long context & Challenging (vanishing) & Natural; quadratic cost \\
Inductive bias & Order, recurrence & Content-based attention \\
Best for & Small data, streaming & Large data, global deps \\
Equivariance & N/A & Permutation-equivariant until positions (cf. conv translation equivariance in \Cref{chap:cnn}) \\
\bottomrule
\end{tabular}
\end{center}

\begin{tcolorbox}[summarybox, title={Key takeaways}]
\noindent\textbf{Terminology.} Masked-LM and next-token LM are \emph{self-supervised} (targets derived from input), not unsupervised.
\par\smallskip
\begin{itemize}
    \item Attention replaces recurrence with content-based mixing, enabling highly parallel training but introducing quadratic cost in sequence length.
    \item Practical stability depends on details (pre-LN vs.\ post-LN, optimizer choices, masking, and careful decoding/evaluation).
    \item Architecture choices (encoder/decoder, positions, caching) are not cosmetic: they determine what the model can reuse at inference time.
\end{itemize}

\medskip
\noindent\textbf{What to be able to do.}
\begin{itemize}
    \item Compute masked attention for a short sequence and explain why the mask enforces causality.
    \item Explain pre-LN vs.\ post-LN and why residual paths influence optimization stability.
    \item Describe KV caching and how it changes inference-time cost compared to training-time cost.
\end{itemize}

\noindent\textbf{Common pitfalls to watch for.}
\begin{itemize}
    \item Incorrect masking (future leakage) or inconsistent tokenization between training and evaluation.
    \item Reporting speed or memory without stating context length, batch size, and caching assumptions.
    \item Treating decoding strategy as an afterthought; greedy, beam, and sampling regimes change observed quality.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item Hand-compute a 2-token attention step with masking; verify against a short script.
    \item Implement a single-block decoder-only transformer (embed + pos + pre-LN MHA + FFN) and train on a tiny character corpus; report perplexity.
    \item Compare naive attention vs.\ FlashAttention on \(n \in \{256, 1024, 4096\}\); log peak memory and tokens/s.
    \item Fine-tune a base model with RoPE vs.\ ALiBi and evaluate extrapolation to 2\(\times\) the training context.
    \item Implement DPO on a small preference dataset; report win-rates versus the SFT baseline.
\end{itemize}

\medskip
\noindent\textbf{If you are skipping ahead.} After this chapter, the book pivots away from neural sequence models, so treat this chapter as the last stop for masking discipline and evaluation hygiene. If you need the embedding objectives and bias/deployment checklist, see \Cref{chap:nlp}.
\end{tcolorbox}

\medskip
\paragraph{Where we head next.} \Cref{chap:softcomp} steps away from neural sequence models and re-enters the broader soft-computing toolkit (fuzzy logic and evolutionary ideas) previewed in \Cref{chap:intro}. Read this chapter as the endpoint of the neural sequence thread: representation objectives from \Cref{chap:nlp} plus masking/calibration discipline from \Crefrange{chap:rnn}{chap:transformers}.

% Chapter 14
\section{Transformers: Attention-Based Sequence Modeling}\label{chap:transformers}

\Cref{chap:rnn} made the sequence problem explicit: stateful computation plus gradients that must flow across time. \Cref{chap:nlp} then supplied the representation layer that makes those sequence objectives practical. This chapter keeps those representations but loosens the ``one step at a time'' constraint: instead of marching through time, we let each position look around and pull in what it needs through attention.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
After this chapter, you should be able to:
\begin{itemize}
  \item Explain the encoder--decoder bottleneck and how attention fixes it in sequence-to-sequence problems.
  \item Write scaled dot-product attention and multi-head attention, and interpret them as weighted averages.
  \item Distinguish self-attention from cross-attention and describe where each appears in a Transformer.
  \item Explain positional encodings and masking (padding/causal) in training and decoding.
  \item Describe a Transformer block (residual paths, layer norm, FFN) and common objectives (MLM/CLM) and families (BERT/GPT/encoder--decoder).
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Make information flow explicit. Attention is a controlled mixing operation; masks enforce which interactions are allowed; residual paths and normalization keep optimization stable as models deepen and context windows grow.
\end{tcolorbox}

\begin{tcolorbox}[perspectivebox, title={Author's note: models, world models, and language models (an opinionated lens)}]
A \emph{model} predicts outcomes from captured information. In the chapters so far, that ``outcome'' ranged
from a class label, to a time-step forecast, to a retrieved memory pattern.

A \emph{world model}, as I use the phrase here, is aspirational: it is the idea of a general-purpose internal
model that can represent situations and dynamics well enough to support planning, counterplanning, and
``what-if'' reasoning. Whether we can build such systems reliably is part of the larger research program.

Language models are a simplified and very constrained training interface to this aspiration: at training time
the next action is a token predicted from previous tokens. My opinion is that this setup can \emph{suggest}
a kind of internal world structure (because the output must remain internally and externally consistent to be
useful), but it does not \emph{guarantee} that a faithful world model has formed. Fluency is not proof of
understanding; it is a behavior you still have to audit.
\end{tcolorbox}

\subsection{From encoder--decoder bottlenecks to attention}
\label{sec:transformers_why_transformers_after_rnns}
Sequence-to-sequence (seq2seq) problems have a natural story: read an input sequence and produce an output sequence. Translation is the canonical example. In the classical encoder--decoder picture, the encoder reads the source sentence and produces a representation; the decoder then uses that representation to generate the target sentence token by token.

We will keep the term \emph{token} from \Cref{chap:nlp}: a token is a discrete symbol index that gets mapped to a vector. In text it might be a word or subword; in other sequence problems it can be any event ID you embed into a feature vector.

The practical bottleneck is the fixed-vector squeeze. If the decoder is only given one summary vector, it is forced to carry \emph{everything} about the source through time, even when the next output token only depends on a small part of the input. Attention is the engineering fix: at each decoding step, compute a weighted average of the encoder states and let that mixture act as the context for the next prediction. In other words, the decoder is allowed to look back at the encoder's ``memory'' and ask which source positions matter \emph{right now}.

Transformers \citep{Vaswani2017} take that idea seriously and push it further. They remove recurrence and make ``look around and mix'' the core operation inside a layer. This allows parallel computation across positions and makes long-range interactions a first-class design choice rather than a side effect of how well information survives through many recurrent steps.

\paragraph{Seq2seq with attention (cross-attention).}
In an encoder--decoder RNN, the encoder produces a sequence of hidden states \(\{\mathbf{h}_j\}_{j=1}^{S}\) from the input \(\{\mathbf{x}_j\}\), and the decoder produces a sequence of states \(\{\mathbf{s}_t\}_{t=1}^{T}\) while generating the output \(\{y_t\}\). A convenient probabilistic view of translation is
\begin{equation}
    p(\mathbf{y}\mid \mathbf{x})
    = \prod_{t=1}^{T} p\!\left(y_t \mid y_{<t}, \mathbf{x}\right).
    \label{eq:transformers_seq2seq_factorization}
\end{equation}
The ``one-vector bottleneck'' appears when the decoder is forced to rely on a single summary of the entire input. Attention replaces that with a \emph{per-step context}:
\begin{align}
    e_{t j} &= a(\mathbf{s}_{t-1}, \mathbf{h}_j), \label{eq:transformers_attn_score_seq2seq}\\
    \alpha_{t j} &= \operatorname{softmax}_j(e_{t j}), \label{eq:transformers_attn_weights_seq2seq}\\
    \mathbf{c}_t &= \sum_{j=1}^{S} \alpha_{t j}\,\mathbf{h}_j. \label{eq:transformers_context_seq2seq}
\end{align}
Here \(\operatorname{softmax}_j\) denotes a softmax over the source positions \(j\), i.e.,
\(\alpha_{t j}=\exp(e_{t j})/\sum_{j'=1}^{S}\exp(e_{t j'})\).
The scoring function \(a(\cdot,\cdot)\) can be a dot product, a bilinear form, or a small MLP, and
\(\mathbf{c}_t\) is the context the decoder uses when predicting \(y_t\).
This style of encoder--decoder attention traces back to early neural machine translation work
\citep{Bahdanau2015}.
That is the core move: different output positions can ``look back'' at different parts of the source.
This is the bridge from encoder--decoder RNNs to Transformers: the attention math stays; what changes is
that Transformers build the states \(\mathbf{h}_j\) and \(\mathbf{s}_t\) without recurrence.

\subsection{Scaled Dot-Product Attention}
\label{sec:transformers_scaled_dot_product_attention}
\begin{tcolorbox}[summarybox, title={Author's note: attention is a weighted average}]
RNNs and gated cells give you a way to carry \emph{memory} forward, but memory is not the same thing as
\emph{relevance}. When you read a page, some words are glue and some words carry the meaning you need
right now; your mind does not treat the whole history as equally important at every step.

I like to read attention as the engineering version of that idea: a weighted average over candidate pieces
of information. A query asks a question (what do I need?), keys advertise what each candidate is about
(what do I contain?), and values carry the content that gets mixed. Similarity scores between queries and
keys become nonnegative weights that sum to one; the output is the weighted sum of the value vectors.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={A tiny analogy: keys, values, and weighted retrieval}]
Think of a tiny ``database'' with keys \(\{70,80\}\) and values \(\{1000,1500\}\). If your query is exactly \(70\), you retrieve \(1000\). If your query is \(75\), there is no exact match, so you can do a soft retrieval: score each key by a similarity such as
\[
s_i = \frac{1}{|k_i-q|+\epsilon},
\]
then normalize those scores so they sum to one and use them as weights on the values. Here \(|70-75|=|80-75|=5\), so \(s_1\approx s_2\approx 0.2\), the normalized weights are \((0.5,0.5)\), and the weighted average gives \(0.5\cdot 1000 + 0.5\cdot 1500 = 1250\).
\par\smallskip
This is only an intuition pump: Transformers do not use scalar keys like ``70.'' They learn vector keys and queries and score them in a learned feature space. But the weighted-average mechanism is exactly the same.
\end{tcolorbox}

At the per-position level, the computation is
\[
\mathbf{z}_i = \sum_{j} \alpha_{ij}\,\mathbf{v}_j,\qquad
\alpha_{ij} \ge 0,\quad \sum_j \alpha_{ij} = 1,
\]
Given query, key, value matrices \(\mathbf{Q} \in \mathbb{R}^{n_q \times d_k}\), \(\mathbf{K} \in \mathbb{R}^{n_k \times d_k}\), and \(\mathbf{V} \in \mathbb{R}^{n_k \times d_v}\), the basic attention operation is
\begin{equation}
    \operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
    = \operatorname{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right) \mathbf{V}.
    \label{eq:transformers_scaled_dot_product_attention}
\end{equation}
Here \(n_q\) is the sequence length of the queries and \(n_k\) the sequence length of keys/values. We keep the same sequence-first convention used in \Cref{chap:rnn,chap:nlp}: rows index time positions (a ``token dimension'') and columns index features, while batch elements are processed independently. The \(1/\sqrt{d_k}\) factor stabilizes gradients by keeping logits in a reasonable range.

It is often helpful to name the attention-weight matrix
\[
\mathbf{A}=\operatorname{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right),
\]
where the softmax is applied \emph{row-wise} (each query position gets a distribution over keys).
Then the output is simply \(\mathbf{Z}=\mathbf{A}\mathbf{V}\). The whole pipeline is differentiable, so the
projection matrices that create \(Q,K,V\) can be learned by backpropagation.

\paragraph{Where do \(Q,K,V\) come from?}
Start from token vectors stacked into a matrix \(\mathbf{X}\in\mathbb{R}^{n\times d_{\text{model}}}\) (one row per position). For a \emph{single head}, learned projections produce
\begin{equation}
    \mathbf{Q}=\mathbf{X}\mathbf{W}^Q,\qquad
    \mathbf{K}=\mathbf{X}\mathbf{W}^K,\qquad
    \mathbf{V}=\mathbf{X}\mathbf{W}^V,
    \label{eq:transformers_qkv_projections}
\end{equation}
with \(\mathbf{W}^Q,\mathbf{W}^K\in\mathbb{R}^{d_{\text{model}}\times d_k}\) and \(\mathbf{W}^V\in\mathbb{R}^{d_{\text{model}}\times d_v}\).
In self-attention, \(\mathbf{X}\) is the same sequence for all three. In cross-attention (seq2seq), \(\mathbf{Q}\) typically comes from decoder states while \(\mathbf{K},\mathbf{V}\) come from encoder states; the math is unchanged. Multi-head attention simply runs this projection-and-attend pattern several times in parallel with separate \(\mathbf{W}_i^Q,\mathbf{W}_i^K,\mathbf{W}_i^V\).

\begin{tcolorbox}[summarybox, title={Worked example: 2-token causal self-attention (one head)}]
Let \(d_k=d_v=2\) and \(Q=K=V=\mathbf{I}_2\). Without masking, the scaled scores are
\[
S=\frac{QK^\top}{\sqrt{d_k}}=\frac{1}{\sqrt{2}}\mathbf{I}_2.
\]
A \emph{causal mask} forbids looking into the future: for query index \(i\), mask out all keys with index \(j>i\) by setting those logits to \(-\infty\) (so they vanish after the softmax).

Row 1 can only attend to itself, so \(\alpha_{1\cdot}=[1,0]\). Row 2 sees logits \([0,\,1/\sqrt{2}]\), so
\[
\begin{aligned}
u &= e^{1/\sqrt{2}},\\
\alpha_{2\cdot} &= \operatorname{softmax}\!\left([0,\,1/\sqrt{2}]\right)
= \left[\frac{1}{1+u},\;\frac{u}{1+u}\right]\\
&\approx[0.330238,\;0.669762].
\end{aligned}
\]
Because \(V=\mathbf{I}_2\), the attention output equals the weight matrix: \(\operatorname{Attn}(Q,K,V)=\alpha\).
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Shape ledger}]
We treat mini\hyp{}batches as \(\mathbf{X}\in\mathbb{R}^{B\times n \times d_{\text{model}}}\) (batch, sequence, features). After the linear projections each head carries \(\mathbf{Q},\mathbf{K}\in\mathbb{R}^{B\times h \times n \times d_k}\), \(\mathbf{V}\in\mathbb{R}^{B\times h \times n \times d_v}\), and the attention weights live in \(\mathbb{R}^{B\times h \times n \times n}\). Reading dimensions in this order avoids confusion when mixing frameworks; \(h\cdot d_k = d_{\text{model}}\) (often \(d_v=d_k\)). FFN inner widths typically 2--4$\times d_{\text{model}}$.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Complexity and memory}]
Naive attention is \(O(n^2 d_{\text{model}})\) compute and \(O(n^2)\) memory per head/layer for the attention map; this dominates long sequences. FlashAttention reduces activation I/O but keeps the quadratic arithmetic; sparse/linear variants reduce the \(n^2\) factor by trading exactness for structure (see Longformer/BigBird/Reformer/Performer/Linformer). Causal/padding masks do not change complexity, only which entries participate.
\end{tcolorbox}

\subsection{Self-attention vs.\ cross-attention}
\label{sec:transformers_self_attention_vs_cross_attention}
The same equations serve two distinct roles.
\emph{Self-attention} means \(Q,K,V\) come from the same sequence. This is the retrieval story turned inward:
each position forms a query and pulls a weighted average over values from the whole sequence, producing a
\emph{contextualized} representation of that token. In decoder-only generation, self-attention is typically
\emph{causal}: the mask enforces that position \(t\) can only use positions \(\le t\).

This is why the same surface form can behave differently depending on what surrounds it. In the phrase
``red flag,'' the useful information is often the idiom (a warning sign), not the literal color red or a physical
flag. In ``red coat,'' red is literal. Self-attention gives the model a mechanism to build token
representations that reflect these different roles by mixing in different neighbors with different weights.
Self-attention by itself does not know what ``before'' and ``after'' mean: without positional information the operation is permutation-equivariant. That is why we explicitly add positional encodings in \Cref{sec:transformers_positional_information}.

\emph{Cross-attention} is the seq2seq bridge. Here the queries come from the decoder states, but the keys and values come from the encoder outputs. In translation terms: each output position asks a question (query) and then pulls a weighted average over the source-side memory (keys/values) to decide what to emit next.

\subsection{Multi-Head Attention (MHA)}
\label{sec:transformers_multi_head_attention_mha}
One attention head gives you one similarity space. Multi-head attention gives you several in parallel: each head learns its own projections and can focus on different relations at the same time (local vs.\ global cues, syntactic vs.\ semantic signals, or different parts of an image-like grid). You should not read heads as guaranteed ``modules'' with fixed roles; the point is capacity and parallel views under the same weighted-average mechanism.

One intuition I find useful in language is that several relations matter at once. Some languages encode gender;
some emphasize agreement; and word order can differ substantially across languages (subject--verb--object versus
other patterns). During translation you may need to track several of these constraints simultaneously while still
resolving local collocations. Multi-head attention gives the model multiple learned ``views'' at the same time:
each head has its own projection matrices and can compute a different attention pattern over the same sequence.

Multiple heads attend in parallel after learned linear projections:
\begin{align}
    \mathrm{head}_i &= \operatorname{Attn}(\mathbf{X}\mathbf{W}_i^Q,\, \mathbf{X}\mathbf{W}_i^K,\, \mathbf{X}\mathbf{W}_i^V),\\
    \operatorname{MHA}(\mathbf{X}) &= [\mathrm{head}_1;\ldots;\mathrm{head}_h]\, \mathbf{W}^O,
    \label{eq:transformers_mha}
\end{align}
with \(\mathbf{W}_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}\), \(\mathbf{W}_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\), \(\mathbf{W}_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\), and output projection \(\mathbf{W}^O\in\mathbb{R}^{(h d_v)\times d_{\text{model}}}\).
For cross-attention, the formula is the same but keys/values come from encoder states: \(\operatorname{Attn}(\mathbf{X}_{\text{dec}}\mathbf{W}_i^Q,\,\mathbf{X}_{\text{enc}}\mathbf{W}_i^K,\,\mathbf{X}_{\text{enc}}\mathbf{W}_i^V)\).
\Cref{fig:lec13_transformer_block} bundles scaled dot-product attention, multi-head concatenation, and the residual pre-LN block so the entire signal path is visible at a glance.

\begin{figure}[h]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[
        >={Latex[length=4pt, width=3pt]},
        font=\small\sffamily,
        % Panels
        panel/.style={draw=gray!40, rounded corners=5pt, fill=gray!3, line width=0.6pt},
        panel_label/.style={anchor=north west, font=\bfseries\footnotesize, text=gray!80, inner sep=4pt},
        % Functional blocks
        func/.style={draw=cbBlue!80, fill=cbBlue!10, rounded corners=2pt, line width=0.7pt, minimum height=1.8em, align=center, inner sep=4pt},
        % Operation blocks
        proc/.style={draw=cbGreen!80, fill=cbGreen!10, rounded corners=2pt, line width=0.7pt, minimum height=1.8em, align=center},
        % Warning/mask blocks
        maskblock/.style={draw=cbOrange!80, fill=cbOrange!10, rounded corners=2pt, line width=0.7pt, minimum height=1.6em, align=center},
        % Merge/add/concat
        sum/.style={circle, draw=cbPink!80, fill=cbPink!10, line width=0.7pt, inner sep=1pt, minimum size=1.2em},
        merge/.style={draw=cbPink!80, fill=cbPink!10, rounded corners=2pt, line width=0.7pt, minimum height=1.8em, align=center},
        % Wires + annotation
        wire/.style={->, draw=gray!70, line width=0.8pt, rounded corners=3pt},
        skip/.style={->, draw=gray!55, line width=0.8pt, rounded corners=6pt},
        annot/.style={font=\scriptsize, text=gray!60, align=left}
    ]
        % ==========================
        % Panel (a): Scaled Dot-Product
        % ==========================
        \node[panel, minimum width=4.2cm, minimum height=6.2cm] (p1) at (0,0) {};
        \node[panel_label] at (p1.north west) {(a) Scaled Dot-Product};

        \node[func, minimum width=0.8cm] (q) at ([yshift=-1.2cm, xshift=-1.2cm]p1.north) {\(\mathbf{Q}\)};
        \node[func, minimum width=0.8cm] (k) at ([yshift=-1.2cm]p1.north) {\(\mathbf{K}\)};
        \node[func, minimum width=0.8cm] (v) at ([yshift=-1.2cm, xshift=1.2cm]p1.north) {\(\mathbf{V}\)};

        \node[proc, minimum width=2.4cm] (matmul1) at ([yshift=-1.0cm]k.south) {MatMul};
        \node[annot, right] at (matmul1.east) {\(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\)};

        \node[maskblock, minimum width=2.4cm] (masknode) at ([yshift=-0.7cm]matmul1.south) {Mask (opt.)};
        \node[proc, minimum width=2.4cm] (softmax) at ([yshift=-0.7cm]masknode.south) {Softmax};
        \node[proc, minimum width=2.4cm] (matmul2) at ([yshift=-0.9cm]softmax.south) {MatMul};
        \node[annot, right] at (matmul2.east) {Weighted\\Sum};

        \draw[wire] (q.south) -- ++(0,-0.3) -| ([xshift=-4pt]matmul1.north);
        \draw[wire] (k.south) -- (matmul1.north);
        \draw[wire] (matmul1.south) -- (masknode.north);
        \draw[wire] (masknode.south) -- (softmax.north);
        \draw[wire] (softmax.south) -- (matmul2.north);
        % Route V -> MatMul2 with an "outside" elbow so it avoids the right-side annotations.
        \coordinate (vRouteX) at ($(p1.east)+(0.55,0)$);
        \draw[wire] (v.east) -- (vRouteX |- v.east) |- (matmul2.north east);
        \draw[wire] (matmul2.south) -- ++(0,-0.4);

        % ==========================
        % Panel (b): Multi-Head
        % ==========================
        \node[panel, minimum width=4.2cm, minimum height=6.2cm] (p2) at (5,0) {};
        \node[panel_label] at (p2.north west) {(b) Multi-Head};

        \node (input_b) at ([yshift=-0.8cm]p2.north) {};
        \node[func, fill=white, draw=gray!30, minimum width=2.8cm, yshift=2pt, xshift=2pt] at ([yshift=-0.6cm]input_b.south) {};
        \node[func, minimum width=2.8cm] (proj) at ([yshift=-0.6cm]input_b.south) {Projections\\\(W_i^Q, W_i^K, W_i^V\)};

        \node[proc, fill=white, draw=gray!30, minimum width=2.2cm, yshift=2pt, xshift=2pt] at ([yshift=-0.9cm]proj.south) {};
        \node[proc, minimum width=2.2cm] (head) at ([yshift=-0.9cm]proj.south) {Attention\\Head \(i\)};
        \node[annot, right] at (head.east) {\(\times h\)};

        \node[merge, minimum width=2.8cm] (concat) at ([yshift=-0.8cm]head.south) {Concat};
        \node[func, minimum width=2.8cm] (out_proj) at ([yshift=-0.8cm]concat.south) {Output \(W^O\)};

        \draw[wire] (proj.south) -- (head.north);
        \draw[wire] (head.south) -- (concat.north);
        \draw[wire] (concat.south) -- (out_proj.north);
        \draw[wire] (out_proj.south) -- ++(0,-0.4);

        % ==========================
        % Panel (c): Pre-LN Encoder
        % ==========================
        \node[panel, minimum width=4.2cm, minimum height=6.2cm] (p3) at (10,0) {};
        \node[panel_label] at (p3.north west) {(c) Pre-LN Block};

        \node[coordinate] (in_c) at ([yshift=-1.0cm]p3.north) {};

        \node[func, minimum width=2.4cm] (ln1_real) at ([yshift=-0.6cm]in_c) {LayerNorm};
        \node[proc, minimum width=2.4cm] (mha_real) at ([yshift=-0.6cm]ln1_real.south) {Multi-Head Attn};
        \node[sum] (add1_real) at ([yshift=-0.5cm]mha_real.south) {+};

        \node[func, minimum width=2.4cm] (ln2_real) at ([yshift=-0.6cm]add1_real.south) {LayerNorm};
        \node[proc, minimum width=2.4cm] (ffn_real) at ([yshift=-0.6cm]ln2_real.south) {FFN (GELU)};
        \node[sum] (add2_real) at ([yshift=-0.5cm]ffn_real.south) {+};

        \draw[wire] (in_c) -- (ln1_real.north);
        \draw[wire] (ln1_real.south) -- (mha_real.north);
        \draw[wire] (mha_real.south) -- (add1_real.north);
        \draw[wire] (add1_real.south) -- (ln2_real.north);
        \draw[wire] (ln2_real.south) -- (ffn_real.north);
        \draw[wire] (ffn_real.south) -- (add2_real.north);
        \draw[wire] (add2_real.south) -- ++(0,-0.4);

        % Residual skips (explicit bypass into each residual add)
        \draw[skip] ([yshift=0.2cm]ln1_real.north) -- ++(-1.6,0) |- node[pos=0.2, above, font=\scriptsize, text=gray!55]{residual} (add1_real.west);
        \draw[skip] ([yshift=0.2cm]ln2_real.north) -- ++(-1.6,0) |- node[pos=0.2, above, font=\scriptsize, text=gray!55]{residual} (add2_real.west);

        \node[draw=gray!50, dotted, fill=white, rounded corners, font=\tiny, align=left, inner sep=2pt] (postln) at ([xshift=1.3cm]add2_real.east) {Post-LN:\\Norm here};
        \draw[->, dotted, gray!60] (postln.west) -- (add2_real.east);
    \end{tikzpicture}}
    % Avoid dense inline math in captions; it wraps poorly in EPUB renderers.
    \caption{Reference schematic for the Transformer. Left: scaled dot-product attention. Center: multi-head concatenation with an output projection. Right: pre-LN encoder block combining attention, FFN, and residual connections; a post-LN variant simply moves each LayerNorm after its residual add (dotted alternative, not shown).}
    \label{fig:lec13_transformer_block}
\end{figure}
\FloatBarrier

\Cref{fig:lec13_micro_figures} provides a quick visual summary of positional encoding, KV cache reuse, and LoRA adapters.

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[
        font=\small\sffamily,
        enc/.style={thick, smooth},
        panel/.style={draw=gray!55, rounded corners=4pt, thick, fill=gray!6, inner sep=6pt, minimum width=4.6cm, minimum height=3.6cm},
        box/.style={draw=gray!60, rounded corners=3pt, thick, fill=gray!5, inner sep=6pt},
        subbox/.style={draw=gray!60, rounded corners=3pt, thick, fill=cbOrange!20, inner sep=4pt},
        mat/.style={draw=gray!60, rounded corners=3pt, thick, fill=cbBlue!12, minimum width=2cm, minimum height=1cm},
        smallmat/.style={draw=gray!60, rounded corners=3pt, thick, fill=cbGreen!25, minimum width=1.4cm, minimum height=0.7cm}
    ]

    % Left panel: positional encodings
    \begin{scope}[xshift=0cm]
        \node[panel] (p1) {};
        \begin{axis}[
            at={(p1.center)},
            anchor=center,
            width=3.9cm,
            height=2.8cm,
            axis lines=none,
            xmin=0, xmax=2*pi,
            ymin=-1.2, ymax=1.2,
            clip=false
        ]
            \addplot[enc, cbBlue, samples=200, domain=0:2*pi] {sin(deg(x))};
            \addplot[enc, cbOrange, samples=200, domain=0:2*pi] {sin(deg(2*x))};
        \end{axis}
        \node[anchor=south, font=\small] at ([yshift=12pt]p1.south) {Positional encodings};
        \node[anchor=north, font=\small] at ([yshift=2pt]p1.south) {sinusoidal / RoPE};
        \node[anchor=north west, font=\footnotesize] at ([xshift=4pt, yshift=-4pt]p1.north west) {(a)};
    \end{scope}

    % Middle panel: Decoder + KV Cache
    \begin{scope}[xshift=5.8cm]
        \node[panel] (p2) {};
        \node[box, minimum width=2.6cm, font=\small] (dec) at (p2.center|-0,0.65) {Decoder block};
        \node[subbox, minimum width=2.6cm, font=\small] (kv) at (p2.center|-0,-0.65) {K/V cache};
        \draw[->, line width=0.9pt] (dec) -- (kv);
        \node[anchor=north west, font=\footnotesize] at ([xshift=4pt, yshift=-4pt]p2.north west) {(b)};
    \end{scope}

    % Right panel: LoRA adapters
    \begin{scope}[xshift=11.6cm]
        \node[panel] (p3) {};
        % horizontal BA feeding W
        \node[mat, font=\small] (W) at (p3.center|-0,1.35) {$\mathbf{W}$};
        \node[smallmat, font=\small] (B) at ([xshift=-1.7cm]p3.center|-0,-0.15) {$\mathbf{B}$};
        \node[smallmat, font=\small] (A) at ([xshift=1.7cm]p3.center|-0,-0.15) {$\mathbf{A}$};
        \draw[->, line width=0.9pt] (B.north) -- ($(W.south)!0.55!(W.south west)$);
        \draw[->, line width=0.9pt] (A.north) -- ($(W.south)!0.55!(W.south east)$);
        \node[font=\scriptsize, anchor=south] at ([yshift=2pt]W.south) {rank $r$};
        \node[font=\small] at ([yshift=-12pt]p3.south) {LoRA adapters};
        \node[anchor=north west, font=\footnotesize] at ([xshift=4pt, yshift=-4pt]p3.north west) {(c)};
    \end{scope}

    % Cross-panel connectivity arrows
    % Token flowing from positional encodings into the decoder block
    \draw[->, line width=0.9pt, draw=gray!65]
        ([xshift=0.25cm]p1.east) --
        ([xshift=-0.25cm]p2.west)
        node[midway, above, font=\small] {token};
    % Reuse of cached K/V feeding the following decoding step
    \draw[->, line width=0.9pt, draw=gray!65]
        ([xshift=0.25cm]kv.east) --
        ([xshift=-0.25cm]p3.west|-kv.east)
        node[midway, above, font=\small] {reuse};

    \end{tikzpicture}
    }%
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Transformer micro-views. Left: positional encodings (sinusoidal/rotary) add order information. Center: KV cache stores past keys/values so decoding a new token reuses prior context. Right: LoRA inserts low-rank adapters (B A) on top of a frozen weight matrix W for parameter-efficient tuning.}
    \label{fig:lec13_micro_figures}
\end{figure}
\FloatBarrier

\subsection{Positional encodings (order without recurrence)}
\label{sec:transformers_positional_information}
Transformers lack recurrence, so order has to be injected explicitly. A simple baseline (from \citet{Vaswani2017}) is a sinusoidal positional encoding: for position \(\mathrm{pos}\) and feature index \(i\),
\begin{align}
    \mathrm{PE}(\mathrm{pos},\,2i)   &= \sin\!\left(\frac{\mathrm{pos}}{10000^{2i/d_{\text{model}}}}\right),\\
    \mathrm{PE}(\mathrm{pos},\,2i+1) &= \cos\!\left(\frac{\mathrm{pos}}{10000^{2i/d_{\text{model}}}}\right).
    \label{eq:transformers_sinusoidal_pe}
\end{align}
You add this vector (or a learned alternative) to the token embedding at each position. The engineering point is not the specific constant; it is that different dimensions oscillate at different frequencies, so nearby positions look similar in some coordinates and far positions look different in others.

A tempting baseline is to represent the position as an ID (or a one-hot vector) and treat it like any other lookup. That \emph{can} work as a table, but it is a poor fit for how attention does its comparisons. One-hot positions are orthogonal: under dot products they only tell you ``same position'' versus ``different position,'' not whether two positions are one step apart or fifty steps apart. A fixed table also has a hard maximum length: if you trained up to \(n=512\) and you now see position 513, you must extend (and usually re-train) the lookup.

Sinusoidal encodings bake in a gentle geometry. For one frequency \(\omega\), the sin/cos pair satisfies
\[
\begin{bmatrix}
\sin(\omega(\mathrm{pos}+\Delta))\\
\cos(\omega(\mathrm{pos}+\Delta))
\end{bmatrix}
=
\begin{bmatrix}
\cos(\omega\Delta) & \sin(\omega\Delta)\\
-\sin(\omega\Delta) & \cos(\omega\Delta)
\end{bmatrix}
\begin{bmatrix}
\sin(\omega\,\mathrm{pos})\\
\cos(\omega\,\mathrm{pos})
\end{bmatrix},
\]
so a fixed offset \(\Delta\) acts like a rotation in that 2D subspace. Across many frequencies, this becomes a multi-scale position code: low-frequency terms vary slowly (coarse, long-range cues) while high-frequency terms vary quickly (fine, local cues). Because attention is built on dot products and similarities, that structure gives the model a usable notion of ``near'' and ``far'' without having to relearn distance from scratch, and it lets you evaluate \(\mathrm{PE}(\mathrm{pos},\cdot)\) beyond the training window.

You can view the input to the first block as
\[
\mathbf{H}_0 = \operatorname{Embed}(\text{tokens}) + \operatorname{PosEnc}(\text{positions}),
\]
where both terms produce vectors in \(\mathbb{R}^{d_{\text{model}}}\) and the sum is taken position-wise.

In modern practice you will also see learned positional embeddings, relative-position schemes, and rotary position embeddings (RoPE). The chapter keeps sinusoidal PE as the clean reference, and we treat other variants as drop-in replacements that mainly change how well models extrapolate to longer contexts or shifting windows.

\subsection{Objectives and masks}
\label{sec:transformers_masks_and_training_objectives}
Two masks show up so often that it is worth naming them early.
A \emph{causal mask} forbids attention to future positions; this is what makes next-token generation well-defined.
A \emph{padding mask} forbids attention to padded positions inserted only to make batches rectangular.
Both are easy to get wrong: a single missing mask can leak information from the future or silently change what the model is allowed to use.
See \Cref{fig:lec13_masks} for a concrete picture of both patterns (queries on rows, keys on columns).

One subtle point is \emph{why} the causal mask is needed during training. At inference time the future does not exist, so a
decoder cannot look ahead even if it wanted to. But during training we often feed the full target sequence (teacher forcing:
provide the ground-truth previous tokens)
so we can compute a loss at every position. Without a causal mask, the decoder would ``cheat'' by attending to future target
tokens. The causal mask enforces the same information constraint at training time that you will have at decoding time.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
            \begin{groupplot}[
                group style={group size=2 by 1, horizontal sep=1.2cm},
                width=0.42\linewidth,
                height=0.36\linewidth,
                view={0}{90},
            xmin=-0.5, xmax=4.5,
            ymin=-0.5, ymax=4.5,
            xtick={0,...,4},
            ytick={0,...,4},
            xticklabels={t\(_0\), t\(_1\), t\(_2\), t\(_3\), t\(_4\)},
            yticklabels={t\(_0\), t\(_1\), t\(_2\), t\(_3\), t\(_4\)},
                xticklabel style={font=\scriptsize},
                yticklabel style={font=\scriptsize},
                colorbar,
                colorbar style={height=3.0cm},
                colormap/viridis,
                point meta min=0, point meta max=1,
                nodes near coords,
                nodes near coords align={center},
                every node near coord/.append style={
                    font=\scriptsize,
                    fill=white,
                    fill opacity=0.75,
                    text opacity=1,
                    inner sep=1.0pt
                }
            ]
            \nextgroupplot[title={Padding mask}]
                \addplot[matrix plot*, mesh/cols=5, mesh/rows=5, point meta=explicit] table [meta=z] {
                    x y z
                    0 0 1
                    1 0 1
                    2 0 1
                    3 0 0
                    4 0 0
                    0 1 1
                    1 1 1
                    2 1 1
                    3 1 0
                    4 1 0
                    0 2 1
                    1 2 1
                    2 2 1
                    3 2 0
                    4 2 0
                    0 3 1
                    1 3 1
                    2 3 1
                    3 3 0
                    4 3 0
                    0 4 1
                    1 4 1
                    2 4 1
                    3 4 0
                    4 4 0
                };
                \node[font=\scriptsize, anchor=north west] at (axis cs:-0.4,4.4) {keep};
                \node[font=\scriptsize, anchor=south east] at (axis cs:3.4,0.0) {mask};
            \nextgroupplot[title={Causal mask}]
                \addplot[matrix plot*, mesh/cols=5, mesh/rows=5, point meta=explicit] table [meta=z] {
                    x y z
                    0 0 1
                    1 0 0
                    2 0 0
                    3 0 0
                    4 0 0
                    0 1 1
                    1 1 1
                    2 1 0
                    3 1 0
                    4 1 0
                    0 2 1
                    1 2 1
                    2 2 1
                    3 2 0
                    4 2 0
                    0 3 1
                    1 3 1
                    2 3 1
                    3 3 1
                    4 3 0
                    0 4 1
                    1 4 1
                    2 4 1
                    3 4 1
                    4 4 1
                };
                \node[font=\scriptsize, anchor=north east] at (axis cs:4.4,4.4) {future masked};
        \end{groupplot}
    \end{tikzpicture}
    \caption{Attention masks as heatmaps (queries on rows, keys on columns). Left: padding mask zeroes out attention to padded positions of a shorter sequence in a packed batch. Right: causal mask enforces autoregressive flow by blocking attention to future tokens.}
    \label{fig:lec13_masks}
\end{figure}
\FloatBarrier

The training objective is usually a cross-entropy (CE) loss, i.e., a negative log-likelihood (NLL) of the correct label under the model's predicted distribution. In sequence modeling, you typically sum (or average) that loss across time positions and across batch elements.

We will return to the main Transformer families (encoder-only vs.\ decoder-only vs.\ encoder--decoder) after we write down the block structure in \Cref{sec:transformers_encoder_decoder_stacks_and_stabilizers}. The core idea is simple: families differ mostly by which masks they apply and which prediction problem they are trained on, not by a different attention mechanism.

\subsection{Transformer blocks and training stabilizers}
\label{sec:transformers_encoder_decoder_stacks_and_stabilizers}
Transformer blocks are built from the same few ingredients repeated many times: attention, a small feed-forward network, and
stability glue (residual connections, normalization, dropout). Residual connections keep a direct path for information and
gradients: the input to a sublayer is added back to its output so the model can refine a representation without destroying it.
LayerNorm stabilizes scale by normalizing each token vector across its feature dimension; unlike BatchNorm it does not depend
on batch statistics, which matters for variable-length sequences and small batches.

In the \emph{pre-LN} layout (common in modern implementations), each sublayer sees a normalized input and then its output is
added back to the residual stream:
\begin{align}
\mathbf{H}_1 &= \mathbf{H} + \operatorname{MHA}(\operatorname{LayerNorm}(\mathbf{H})),\\
\mathbf{H}_{\text{out}} &= \mathbf{H}_1 + \operatorname{FFN}(\operatorname{LayerNorm}(\mathbf{H}_1)).
    \label{eq:transformers_preln_block}
\end{align}
The original Transformer applied LayerNorm after each residual add (post-LN); the attention and FFN computations are the same,
only the placement of normalization changes.

The feed-forward sublayer (FFN) is applied independently to each position (row). In matrix form it is typically two linear
layers with a nonlinearity:
\begin{equation}
    \operatorname{FFN}(\mathbf{H})
    = \sigma(\mathbf{H}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2,
    \label{eq:transformers_ffn}
\end{equation}
where \(\sigma\) is commonly ReLU (original paper) or GELU (many modern stacks), \(\mathbf{W}_1\in\mathbb{R}^{d_{\text{model}}\times d_{\text{ff}}}\),
\(\mathbf{W}_2\in\mathbb{R}^{d_{\text{ff}}\times d_{\text{model}}}\), and the biases are broadcast across positions. Dropout and label smoothing are common.

\begin{tcolorbox}[summarybox, title={Author's note: attention mixes; the FFN refines (two different jobs)}]
Self-attention is where tokens ``talk'' to each other. It is the part of the block that \emph{mixes across positions}: each token forms a query, assigns weights over other tokens' keys, and pulls a weighted average of their values.

The feed-forward sublayer is different. It is applied \emph{independently} to each token: the same small MLP processes every position, taking whatever the attention step mixed in and turning it into a cleaner set of features for the next layer. If you like metaphors, I think of attention as the class discussion and the FFN as the private tutor: the discussion lets you hear from others; the tutor helps you rewrite your notes in a form you can use.
\end{tcolorbox}

\paragraph{Decoder blocks: masked self-attention plus cross-attention.}
Encoder blocks use self-attention (typically with a padding mask) plus an FFN. Decoder blocks add a second attention step: first \emph{masked} self-attention over the partially generated target (causal mask), then cross-attention into the encoder outputs. In that cross-attention, the query comes from the decoder stream while the keys and values come from the encoder stream. It is the same weighted-average mechanism, but now the memory lives on the source side and the questions live on the target side.

\begin{tcolorbox}[summarybox, breakable, title={Implementation snapshot: block pseudocode, training defaults, and one step}]
\noindent\textbf{Block pseudocode (pre-LN).}
\begin{verbatim}
function Block(H, mask=None):
    # Pre-normalize inputs (pre-LN stabilizes deep stacks)
    H_norm = LayerNorm(H)
    attn = MHA(H_norm, H_norm, H_norm, mask)
    H = H + Dropout(attn)
    H_norm = LayerNorm(H)
    ff = FFN(H_norm)
    return H + Dropout(ff)
\end{verbatim}
Decoder blocks add a causal mask on self-attention, and encoder--decoder variants also insert cross-attention (queries from decoder; keys/values from encoder outputs). Pre-LN (shown here) is now common because it keeps gradients well behaved for very deep stacks; post-LN (original Transformer) is still used in smaller models.
\medskip
\noindent\textbf{Training defaults (decoder-only, practical baseline).}
AdamW with cosine decay and 1--3\% warmup; LR \(\sim 10^{-3}\) for small models, \(1\text{--}2\times 10^{-4}\) for mid-size. Weight decay \(\approx 0.01\) (exclude biases/LayerNorm gains). Attention/MLP dropout \(\approx 0.1\); clip global norm to 1.0. Mixed precision (FP16/BF16) plus gradient checkpointing for long contexts; tie input embeddings to the LM head; use causal masks for CLM and padding masks for packed batches.

\medskip
\noindent\textbf{One training step (decoder-only, causal mask).}
\begin{verbatim}
x = tokenizer(batch_text)                # [B, T]
mask = causal_mask(x)                    # [B, 1, T, T]
h = embed(x) + pos(x)                    # [B, T, d_model]
for block in blocks:
    h = block(h, mask)                   # pre-LN MHA + FFN
logits = lm_head(h)                      # [B, T, vocab]
loss = CE(logits[:, :-1], x[:, 1:])      # next-token
loss.backward()
clip_grad_norm_(model.parameters(), 1.0)
opt.step(); opt.zero_grad()
\end{verbatim}
At inference, reuse cached K/V states (see \Cref{fig:lec13_micro_figures} and \Cref{sec:transformers_advanced_attention_and_efficiency_notes_snapshot}).
\end{tcolorbox}
\paragraph{Code--math dictionary.} In code blocks, \texttt{x} is the token-index tensor (input IDs), \texttt{h} is the hidden-state array \(\mathbf{H}\), \texttt{mask} is the attention mask, and \texttt{embed(x)} denotes an embedding lookup into the learned table \(\mathbf{W}\) (written algebraically as a row-selection, e.g., \(\mathbf{W}[w_t]\), in \Cref{chap:nlp}).

\subsection{BERT vs.\ GPT vs.\ encoder--decoder}
\label{sec:transformers_model_families}
Once you have attention blocks, most of the ``model family'' distinctions come from two choices: (i) which directions a position is allowed to attend to (masking), and (ii) what prediction problem you train on. Encoder-only stacks are built to produce strong \emph{representations}; decoder-only stacks are built to \emph{generate}; encoder--decoder stacks are built to condition one sequence on another (translation is the clean example).

\paragraph{BERT: encoder-only, bidirectional self-attention, masked-token prediction.}
BERT \citep{Devlin2019} is a stack of Transformer \emph{encoders}. There is no causal mask: each token can attend to both left and right context, so the representation at position \(i\) can use the whole sentence. In practice you still apply a padding mask when you batch variable-length sequences; ``bidirectional'' means ``not restricted to past-only,'' not ``no masking ever.''
Pre-training uses \emph{masked language modeling} (MLM): choose a subset of token positions \(\mathcal{M}\) and ask the model to reconstruct the original tokens from a corrupted input. If \(w_{1:n}\) are the original token IDs and \(\tilde{w}_{1:n}\) is the corrupted sequence (where some positions were replaced by \texttt{[MASK]} or other substitutions), a convenient loss view is
\begin{equation}
    \mathcal{L}_{\text{MLM}}
    = -\sum_{i\in\mathcal{M}} \log p_\theta\!\left(w_i \mid \tilde{w}_{1:n}\right),
    \label{eq:transformers_mlm_objective}
\end{equation}
where the model produces a vocabulary distribution at each position and we only score the masked positions.
Because masked tokens are predicted using \emph{both} left and right context, MLM does not train a left-to-right next-token factorization. This is why BERT is usually used as an encoder (representations + classifiers) rather than as a free-form decoder without additional machinery.
The original BERT recipe masked about 15\% of tokens and used a pragmatic corruption mix: most masked positions are replaced by \texttt{[MASK]}, some by a random token, and some are left unchanged so the model does not overfit to seeing \texttt{[MASK]} at fine-tuning time \citep{Devlin2019}.

\begin{tcolorbox}[summarybox, title={Author's note: Word2Vec vs.\ BERT (the intuition that still helps)}]
Word2Vec \citep{Mikolov2013} taught us a useful lesson: meaning leaks into geometry through co-occurrence. If you repeatedly see certain words together, a vector model can pick that up and cluster them.

I like to see BERT as a more powerful version of the same distributional idea. The training question is still ``can you predict a missing piece from what surrounds it?'' The difference is that the representation is no longer one vector \emph{per vocabulary item}. BERT produces a \emph{contextual} vector per token occurrence: the vector for ``bank'' in ``river bank'' is not forced to match the vector for ``bank'' in ``bank account,'' because the surrounding tokens are part of the computation.
\end{tcolorbox}

\paragraph{\texttt{[CLS]} as a learned pooling token.}
BERT-style models typically prepend a special \texttt{[CLS]} token and (for sentence pairs) separate segments with \texttt{[SEP]}. In the original setup, a small \emph{token-type} (segment) embedding marks whether each token came from sentence A or sentence B \citep{Devlin2019}. Because \texttt{[CLS]} participates in the same self-attention as every other token, its representation vector is repeatedly updated as the model mixes information across the sequence. In practice, the final-layer \texttt{[CLS]} vector is treated as a learned, task-adaptable summary and is fed to a small classifier head for sentence-level tasks.

\begin{tcolorbox}[summarybox, title={BERT inputs in one line (token + position + segment)}]
At position \(i\), BERT forms an input vector by summing three components \citep{Devlin2019}:
\[
\mathbf{x}_i
= \mathbf{e}_{\text{tok}}(w_i)
+ \mathbf{e}_{\text{pos}}(i)
+ \mathbf{e}_{\text{seg}}(s_i),
\]
where \(w_i\) is the token ID, \(i\) is the position index, and \(s_i\in\{\mathrm{A},\mathrm{B}\}\) indicates which sentence/segment the token belongs to (for single-sentence inputs, \(s_i\) is constant).
\end{tcolorbox}

The original BERT paper also used an auxiliary ``next sentence prediction'' (NSP) objective for sentence pairs; later variants often drop it. For our purposes, the core encoder-only story is: bidirectional self-attention plus an objective that forces representations to be predictive of missing content.

\paragraph{GPT: decoder-only, causal self-attention, next-token prediction.}
GPT-style models are stacks of Transformer \emph{decoders} with the cross-attention removed: they use masked (causal) self-attention plus an FFN in each block. The causal mask enforces the autoregressive factorization:
\begin{equation}
    p_\theta(w_{1:n}) = \prod_{t=1}^{n-1} p_\theta\!\left(w_{t+1}\mid w_{1:t}\right),
    \label{eq:transformers_clm_factorization}
\end{equation}
and the training objective (causal language modeling, CLM) is the corresponding negative log-likelihood,
\begin{equation}
    \mathcal{L}_{\text{CLM}}
    = -\sum_{t=1}^{n-1} \log p_\theta\!\left(w_{t+1}\mid w_{1:t}\right).
    \label{eq:transformers_clm_objective}
\end{equation}
This is where it is easy to mix up training and generation. During training you do \emph{not} feed back the model's sampled token at each step; you feed the true prefix (teacher forcing) and compute the loss for all positions in parallel under the causal mask (as in the code snapshot earlier). During inference you do sample/argmax and append one token at a time, and that is where decoding settings (temperature, top-\(k\), top-\(p\)) matter; see \Cref{sec:transformers_decoding_and_evaluation}.

Historically, GPT-1 used a 12-layer decoder-only stack with about 117M parameters and showed that simple next-token pre-training can transfer to many downstream tasks with surprisingly little architectural customization \citep{Radford2018}. GPT-2 and later work scaled the same basic recipe with larger models and broader data \citep{Radford2019,Brown2020}.

\paragraph{Encoder--decoder (seq2seq): conditional generation via cross-attention.}
Encoder--decoder Transformers keep the clean translation structure: the encoder reads the source; the decoder generates the target with causal self-attention plus cross-attention into the encoder outputs. This remains a natural fit whenever the output is explicitly conditioned on an input sequence (translation, summarization, structured prediction). Text-to-text models such as T5 make this conditioning view explicit by casting many tasks into a single seq2seq format \citep{Raffel2020}.

\subsection{Long Contexts and Efficient Attention}
\label{sec:transformers_long_contexts_and_efficient_attention}
Memory and compute scale quadratically with sequence length. Practical systems therefore mix several tricks:
\begin{itemize}
    \item \textbf{Sparse or local attention} (e.g., Longformer, BigBird) to limit each query to a sliding or block-sparse neighborhood.
    \item \textbf{Low-rank/kernelized approximations} and recurrent chunking (Performer, Transformer-XL) so that computation/storage grows roughly linearly in context length.
    \item \textbf{I/O-aware kernels} such as FlashAttention that stream tiles through SRAM so the \(O(n^2)\) attention computation remains exact while memory stays manageable.
\end{itemize}
Relative/rotary position schemes and KV caching are summarized in \Cref{sec:transformers_advanced_attention_and_efficiency_notes_snapshot} and \Cref{fig:lec13_micro_figures}.

\subsection{Fine-Tuning and Parameter-Efficient Adaptation}
\label{sec:transformers_fine_tuning_and_parameter_efficient_adaptation}
Pre-training gives you a general-purpose language model; fine-tuning adapts that model to a task, domain, or interaction style. \emph{Full fine-tuning} updates all weights, which can yield the best performance when you have enough high-quality data and a stable objective, but it is also the easiest to overfit or to accidentally ``forget'' useful general behavior.

\emph{Parameter-efficient} methods (LoRA, adapters, prefix/prompt tuning, and related variants) inject small trainable modules while freezing most of the base model, enabling rapid adaptation with lower memory and more predictable changes. Practically, PEFT is attractive when you want many task-specific variants of a shared base model, or when you need to keep the base weights fixed for deployment and auditing.

\paragraph{Audit hooks for adaptation.}
Regardless of whether you update all weights or only a small adapter, treat fine-tuning like any other ERM pipeline: keep a held-out evaluation set that matches the deployment slice, monitor calibration and failure modes (not just loss), and log the exact base checkpoint, tokenizer, and data snapshot so results are reproducible. When fine-tuning for instruction-following or conversational behavior, add explicit tests for regressions (refusals, hallucinations on factual probes, and brittleness to prompt variants) rather than relying on a single aggregate score.

\subsection{Decoding and Evaluation}
\label{sec:transformers_decoding_and_evaluation}
Training produces a distribution over the next token; decoding turns that distribution into an actual sequence. For decoder-only models, decoding runs autoregressively: predict \(p_\theta(x_{t+1}\mid x_{1:t})\), choose a token, append it, and repeat. This is also where the KV cache matters (see \Cref{fig:lec13_micro_figures}): you reuse past keys/values so generating one more token does not require recomputing attention over the entire prefix from scratch.

\paragraph{From token representations to a next-token distribution.}
At position \(t\), the final block produces a token representation \(\mathbf{h}_t\in\mathbb{R}^{d_{\text{model}}}\). A language model head is a linear map into the vocabulary \(\mathcal{V}\):
\begin{equation}
    \boldsymbol{\ell}_t = \mathbf{h}_t \mathbf{W}_{\text{out}} + \mathbf{b}_{\text{out}}, \qquad
    \boldsymbol{\ell}_t \in \mathbb{R}^{|\mathcal{V}|},
    \label{eq:transformers_lm_head_logits}
\end{equation}
where \(\boldsymbol{\ell}_t\) are the logits (unnormalized scores). Softmax converts logits to probabilities,
\(p_\theta(x_{t+1}=v\mid x_{1:t})=\operatorname{softmax}(\boldsymbol{\ell}_t)_v\),
and cross-entropy (NLL) trains the model to put mass on the observed next token.

\paragraph{Temperature and sampling (how you choose a token).}
Temperature rescales logits before softmax:
\begin{equation}
    p_T(v\mid x_{1:t}) = \operatorname{softmax}\!\left(\boldsymbol{\ell}_t/T\right)_v, \qquad T>0.
    \label{eq:transformers_temperature_softmax}
\end{equation}
Small \(T\) makes the distribution sharper (more deterministic); large \(T\) makes it flatter (more random). The limit \(T\to 0\) approaches greedy argmax behavior; the limit \(T\to\infty\) approaches a uniform distribution. Temperature \(T=0\) is not defined mathematically; in code it is simply implemented as ``take \(\arg\max\).'' A common sampling implementation is the roulette-wheel (inverse-CDF) rule: draw \(u\sim \mathrm{Uniform}(0,1)\), accumulate the probabilities until the cumulative sum exceeds \(u\), and pick that token. In practice you often combine this with top-\(k\) or nucleus (top-\(p\)) truncation: restrict to a shortlist of likely tokens, renormalize, then sample.

Greedy decoding takes the argmax at each step; it is fast and often strong for short, factual completions, but it can get stuck in repetitive loops. Beam search keeps multiple partial hypotheses; it can improve likelihood but sometimes harms perceived quality in open-ended generation. Sampling (often with top-\(k\) or top-\(p\) truncation) trades certainty for diversity; pick it deliberately and log the settings.

For evaluation, perplexity summarizes next-token performance for language modeling (see \Cref{chap:nlp}), but it does not tell you whether generations are useful, safe, or faithful. For downstream classification, prefer metrics that match the deployment slice (e.g., AUPRC for imbalanced problems) and keep decoding settings logged alongside checkpoints so results are reproducible.

\subsection{Audit and failure modes (short list)}
\label{sec:transformers_audit_and_failure_modes}
\begin{tcolorbox}[summarybox, title={Audit and failure modes (engineering view)}]
\begin{itemize}
    \item \textbf{Masking bugs:} missing/incorrect causal masks can leak future tokens; missing padding masks can let padding dominate attention in batched training.
    \item \textbf{Train/inference mismatch:} teacher forcing during training does not automatically tell you how errors compound during decoding; test the decoding strategy you plan to ship.
    \item \textbf{Long-context degradation:} attention enables long-range access, but quality can still decay with length; measure how performance changes as you increase context.
    \item \textbf{Calibration vs.\ correctness:} high probability is not a guarantee of correctness; audit reliability on slices and stress tests, not just average loss.
    \item \textbf{Reproducibility:} tokenizer choices, data filters, and decoding hyperparameters can swing results; log them as part of the experiment.
\end{itemize}
\end{tcolorbox}

\subsection{Alignment (Brief)}
\label{sec:transformers_alignment_brief}
Post-training \emph{alignment} shapes model behavior to match human preferences, safety constraints, and interaction norms. In broad terms, alignment objectives do not change the Transformer mechanics; they change what you reward during optimization (and therefore what behaviors are reinforced).

RLHF optimizes a policy against a learned reward model (with careful regularization to avoid drifting too far from the base model). Preference-based objectives such as DPO, KTO, or ORPO optimize directly from ranked pairs without a full reinforcement-learning loop.

\paragraph{Alignment is not a proof of correctness.}
Alignment can improve helpfulness and reduce obvious failures, but it can also introduce new ones (reward hacking, over-refusal, brittleness to prompt phrasing, or degraded performance off-distribution). Treat it as an engineering stage with explicit test suites and logging: evaluate on held-out tasks, check calibration and refusal behavior, and track changes relative to the pre-alignment model.

\subsection{Advanced notes (optional, practitioner snapshot)}
\label{sec:transformers_advanced_attention_and_efficiency_notes_snapshot}
\begin{itemize}
    \item \textbf{Relative/rotary positions.} RoPE \citep{Su2021RoPE} and ALiBi \citep{Press2022ALiBi} replace absolute sinusoidal embeddings with rotation/bias terms so extrapolating to longer sequences no longer requires re-fitting positional lookups; the trade-off is that absolute tables keep fixed anchors for classification tokens while rotary/relative schemes favour extrapolation and smoothly sliding windows.
    \item \textbf{KV-cache management.} Decoder-only inference stores per-layer key/value tensors; chunked caching, paged attention, and sliding windows keep memory linear in context length. Speculative decoding and assisted decoding reuse a lightweight draft model to propose tokens that the full model verifies before committing.
    \item \textbf{Efficient kernels.} FlashAttention \citep{Dao2022FlashAttention} computes attention blocks in streaming tiles to keep activations in SRAM. Long-context variants mix windowed attention, recurrent memory, or low-rank adapters; state-space models such as Mamba \citep{Gu2023Mamba} provide linear-time alternatives that back-propagate through implicitly defined kernels.
    \item \textbf{Mixture-of-experts and routing.} Sparse MoE layers \citep{Shazeer2017MoE} add conditional capacity; router z-losses, capacity factors, and load-balancing losses are essential to avoid expert collapse.
    \item \textbf{Test-time scaling.} Curriculum-based sampling (nucleus, temperature annealing), classifier-free guidance, and beam-search variants all tune the accuracy/latency frontier; plan to log decoding hyperparameters alongside checkpoints so experiments are reproducible.
\end{itemize}
Parameter-efficient tuning methods are covered in \Cref{sec:transformers_fine_tuning_and_parameter_efficient_adaptation}.

\subsection{RNNs vs. Transformers: When and Why}
\label{sec:transformers_rnns_vs_transformers_when_and_why}
\begin{center}
\begin{tabular}{@{}>{\raggedright\arraybackslash}p{0.24\linewidth} >{\raggedright\arraybackslash}p{0.34\linewidth} >{\raggedright\arraybackslash}p{0.34\linewidth}@{}}
\toprule
 & \textbf{RNN/LSTM/GRU} & \textbf{Transformer} \\
\midrule
Parallelism & Limited (sequential) & High (tokens in parallel) \\
Long context & Challenging (vanishing) & Natural; quadratic cost \\
Inductive bias & Order, recurrence & Content-based attention \\
Best for & Small data, streaming & Large data, global deps \\
Equivariance & N/A & Permutation-equivariant until positions (cf. conv translation equivariance in \Cref{chap:cnn}) \\
\bottomrule
\end{tabular}
\end{center}

\begin{tcolorbox}[summarybox, title={Key takeaways}]
\noindent\textbf{Terminology.} Masked-LM and next-token LM are \emph{self-supervised} (targets derived from input), not unsupervised.
\par\smallskip
\begin{itemize}
    \item Attention replaces recurrence with content-based mixing, enabling highly parallel training but introducing quadratic cost in sequence length.
    \item Practical stability depends on details (pre-LN vs.\ post-LN, optimizer choices, masking, and careful decoding/evaluation).
    \item Architecture choices (encoder/decoder, positions, caching) are not cosmetic: they determine what the model can reuse at inference time.
\end{itemize}

\medskip
\noindent\textbf{What to be able to do.}
\begin{itemize}
    \item Compute masked attention for a short sequence and explain why the mask enforces causality.
    \item Explain pre-LN vs.\ post-LN and why residual paths influence optimization stability.
    \item Describe KV caching and how it changes inference-time cost compared to training-time cost.
\end{itemize}

\noindent\textbf{Common pitfalls to watch for.}
\begin{itemize}
    \item Incorrect masking (future leakage) or inconsistent tokenization between training and evaluation.
    \item Reporting speed or memory without stating context length, batch size, and caching assumptions.
    \item Treating decoding strategy as an afterthought; greedy, beam, and sampling regimes change observed quality.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item Hand-compute a 2-token attention step with masking; verify against a short script.
    \item Implement a single-block decoder-only transformer (embed + pos + pre-LN MHA + FFN) and train on a tiny character corpus; report perplexity.
    \item Compare naive attention vs.\ FlashAttention on \(n \in \{256, 1024, 4096\}\); log peak memory and tokens/s.
    \item Fine-tune a base model with RoPE vs.\ ALiBi and evaluate extrapolation to 2\(\times\) the training context.
    \item Implement DPO on a small preference dataset; report win-rates versus the SFT baseline.
\end{itemize}

\medskip
\noindent\textbf{If you are skipping ahead.} After this chapter, the book pivots away from neural sequence models, so treat this chapter as the last stop for masking discipline and evaluation hygiene. If you need the embedding objectives and bias/deployment checklist, see \Cref{chap:nlp}.
\end{tcolorbox}

\medskip
\paragraph{Where we head next.} \Cref{chap:softcomp} steps away from neural sequence models and re-enters the broader soft-computing toolkit (fuzzy logic and evolutionary ideas) previewed in \Cref{chap:intro}. Read this chapter as the endpoint of the neural sequence thread: representation objectives from \Cref{chap:nlp} plus masking/calibration discipline from \Crefrange{chap:rnn}{chap:transformers}.

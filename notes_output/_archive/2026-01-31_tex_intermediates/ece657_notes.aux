\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{Preface}{3}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgments}{5}{Doc-Start}\protected@file@percent }
\citation{Hochreiter1997,Gers2000}
\citation{Cho2014}
\@writefile{toc}{\contentsline {section}{Notation and Conventions}{27}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}About This Book}{30}{section.1}\protected@file@percent }
\newlabel{chap:intro}{{1}{30}{About This Book}{section.1}{}}
\newlabel{chap:intro@cref}{{[section][1][]1}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Historical Foundations of Intelligent Systems}{30}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mechanical Automata and Scholastic Logic}{30}{subsection.1.1}\protected@file@percent }
\citation{Risch1969}
\@writefile{toc}{\contentsline {paragraph}{The Mechanical Computer and Early Programming}{31}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mathematical Logic and Formal Reasoning}{31}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Turing Test and the Birth of AI}{31}{equation.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Early Machine Learning and Symbolic AI}{31}{equation.1.1}\protected@file@percent }
\citation{RussellNorvig2021}
\citation{PooleMackworth2017}
\@writefile{toc}{\contentsline {paragraph}{Summary of Key Historical Milestones}{32}{equation.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Defining Artificial Intelligence and Intelligent Systems}{32}{subsection.1.2}\protected@file@percent }
\citation{Brooks1986,Arkin1998}
\@writefile{toc}{\contentsline {paragraph}{Core Definition of AI}{33}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Intelligent Systems}{34}{subsection.1.3}\protected@file@percent }
\newlabel{par:intelligent-systems}{{1.3}{34}{Intelligent Systems}{subsection.1.3}{}}
\newlabel{par:intelligent-systems@cref}{{[subsection][3][1]1.3}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}From value-centric questions to concrete designs}{35}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Components of AI Systems: Thinking, Perception, and Action}{35}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Autonomous Vehicle}{36}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Case Study: AI-Enabled Camera as an Intelligent System}{36}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Levels and Architectures of Intelligent Systems}{38}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Constitutes Intelligence in Systems?}{38}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Levels of Intelligence (as an organizing lens)}{38}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Connectionist vs.\ agent-based/decentralized approaches}{38}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Swarm Intelligence}{39}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples of Input and Output Variables in Dynamic Systems}{39}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Characteristics of Intelligent Systems}{40}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intelligent Systems as Decision Makers}{41}{Item.9}\protected@file@percent }
\newlabel{eq:intelligent_mapping}{{1.2}{41}{Intelligent Systems as Decision Makers}{equation.1.2}{}}
\newlabel{eq:intelligent_mapping@cref}{{[equation][2][1]1.2}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Intelligent Systems and Intelligent Machines}{41}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Terminology Clarification}{41}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Behavior, Not Components}{42}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples}{42}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Consciousness and Intelligence}{42}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Levels, Meta-cognition, and Safety}{43}{subsection.1.7}\protected@file@percent }
\newlabel{subsec:levels}{{1.7}{43}{Levels, Meta-cognition, and Safety}{subsection.1.7}{}}
\newlabel{subsec:levels@cref}{{[subsection][7][1]1.7}{[1][43][]43}}
\@writefile{toc}{\contentsline {paragraph}{Meta-cognition (Operational View)}{43}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implications and Risks}{43}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Designing Safe Intelligent Systems}{43}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Audience, Prerequisites, and Scope}{44}{subsection.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Roadmap and Reading Paths}{45}{subsection.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Using and Navigating This Book}{45}{subsection.1.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic: Roadmap (core supervised path; SOM/fuzzy; optimization/evolutionary).\relax }}{45}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:roadmap}{{1}{45}{Schematic: Roadmap (core supervised path; SOM/fuzzy; optimization/evolutionary).\relax }{figure.caption.1}{}}
\newlabel{fig:roadmap@cref}{{[figure][1][]1}{[1][45][]45}}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{46}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{47}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Symbolic Integration and Problem-Solving Strategies}{47}{section.2}\protected@file@percent }
\newlabel{chap:symbolic}{{2}{47}{Symbolic Integration and Problem-Solving Strategies}{section.2}{}}
\newlabel{chap:symbolic@cref}{{[section][2][]2}{[1][47][]47}}
\citation{Risch1969}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Context and Motivation}{48}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Problem Decomposition and Transformation}{49}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Safe Transformations}{49}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Applying Safe Transformations}{50}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Limitations of Safe Transformations}{50}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Heuristic Transformations}{50}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{50}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Trigonometric Heuristics}{51}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Heuristics as a Form of Intelligence}{51}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Summary of the Approach}{51}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cost heuristic.}{52}{Item.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Heuristic Transformations: Revisiting the Integral with \(1 - x^2\)}{52}{subsection.2.6}\protected@file@percent }
\newlabel{eq:original_integral}{{2.1}{52}{Heuristic Transformations: Revisiting the Integral with \texorpdfstring {\(1 - x^2\)}{1 - x squared}}{equation.2.1}{}}
\newlabel{eq:original_integral@cref}{{[equation][1][2]2.1}{[1][52][]52}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Substitution and Differential}{53}{equation.2.1}\protected@file@percent }
\newlabel{eq:sec4_integral}{{2.2}{53}{Step 1: Substitution and Differential}{equation.2.2}{}}
\newlabel{eq:sec4_integral@cref}{{[equation][2][2]2.2}{[1][53][]53}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Choosing the Next Transformation}{53}{equation.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Functional Composition and Path Selection}{54}{equation.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Two safe options from here}{54}{equation.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Back-substitution and check}{54}{equation.2.2}\protected@file@percent }
\citation{Bronstein2005,Risch1969}
\@writefile{toc}{\contentsline {paragraph}{Pattern rule}{55}{equation.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Example: Solving an Integral via Transformation Trees}{55}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Transformation Trees and Search Strategies}{55}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition:}{55}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{55}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic: Transformation tree for the running example integral; badges \textbf  {[S]}/\textbf  {[H]} mark safe vs.\ heuristic moves; the dashed branch mirrors the sine substitution.\relax }}{56}{figure.caption.2}\protected@file@percent }
\newlabel{fig:lec3_transform_tree}{{2}{56}{Schematic: Transformation tree for the running example integral; badges \textbf {[S]}/\textbf {[H]} mark safe vs.\ heuristic moves; the dashed branch mirrors the sine substitution.\relax }{figure.caption.2}{}}
\newlabel{fig:lec3_transform_tree@cref}{{[figure][2][]2}{[1][55][]56}}
\@writefile{toc}{\contentsline {paragraph}{Safe vs. Heuristic Transformations:}{56}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backtracking:}{56}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Algorithmic Outline for Symbolic Problem Solving}{57}{subsection.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Note:}{57}{Item.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Residual test implementation.}{59}{Item.23}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Schematic: Transformation toolkit (safe vs.\ heuristic). Preconditions keep domains/branches explicit (e.g., restrictions like ``x in (-1,1)'' for square-root expressions); principal branches unless noted.\relax }}{60}{table.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Worked example: Beta template vs.\ numeric fallback}{60}{table.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Failure path with certified numeric residual.}{61}{table.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10}Discussion: What this example illustrates}{61}{subsection.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{63}{subsection.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{63}{subsection.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Supervised Learning Foundations}{63}{section.3}\protected@file@percent }
\newlabel{chap:supervised}{{3}{63}{Supervised Learning Foundations}{section.3}{}}
\newlabel{chap:supervised@cref}{{[section][3][]3}{[1][63][]63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Setup and Notation}{65}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fitting, Overfitting, and Underfitting}{66}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Underfitting.}{66}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overfitting.}{67}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What we aim for.}{67}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Schematic: Underfitting and overfitting as a function of model complexity. Training error typically decreases with complexity, while validation error often has a U-shape. Regularization and model selection aim to operate near the minimum of the validation curve.\relax }}{67}{figure.caption.4}\protected@file@percent }
\newlabel{fig:lec1_fit_regimes}{{3}{67}{Schematic: Underfitting and overfitting as a function of model complexity. Training error typically decreases with complexity, while validation error often has a U-shape. Regularization and model selection aim to operate near the minimum of the validation curve.\relax }{figure.caption.4}{}}
\newlabel{fig:lec1_fit_regimes@cref}{{[figure][3][]3}{[1][67][]67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Empirical Risk Minimization and Regularization}{67}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ridge and lasso.}{68}{equation.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Elastic-net paths and cross-validation}{69}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Schematic: Why L1 promotes sparsity. Minimizing loss subject to an L2 constraint tends to hit a smooth boundary; an L1 constraint has corners aligned with coordinate axes, so tangency often occurs at a point where some coordinates are exactly zero.\relax }}{70}{figure.caption.5}\protected@file@percent }
\newlabel{fig:lec1_l1_l2_geometry}{{4}{70}{Schematic: Why L1 promotes sparsity. Minimizing loss subject to an L2 constraint tends to hit a smooth boundary; an L1 constraint has corners aligned with coordinate axes, so tangency often occurs at a point where some coordinates are exactly zero.\relax }{figure.caption.5}{}}
\newlabel{fig:lec1_l1_l2_geometry@cref}{{[figure][4][]4}{[1][69][]70}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Schematic: A typical lasso path as the regularization strength increases. Coefficients shrink, and some become exactly zero, yielding sparse models.\relax }}{70}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lec1_lasso_path}{{5}{70}{Schematic: A typical lasso path as the regularization strength increases. Coefficients shrink, and some become exactly zero, yielding sparse models.\relax }{figure.caption.6}{}}
\newlabel{fig:lec1_lasso_path@cref}{{[figure][5][]5}{[1][69][]70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Common Loss Functions}{71}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Schematic: Common losses and typical use (reference for \Crefrange  {chap:supervised}{chap:perceptron}).\relax }}{71}{table.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Schematic: Classification losses as functions of the signed margin z.\relax }}{72}{figure.caption.8}\protected@file@percent }
\newlabel{fig:lec1_class_losses}{{6}{72}{Schematic: Classification losses as functions of the signed margin z.\relax }{figure.caption.8}{}}
\newlabel{fig:lec1_class_losses@cref}{{[figure][6][]6}{[1][71][]72}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Schematic: Regression losses versus prediction error. The Huber loss transitions from quadratic to linear to reduce sensitivity to outliers.\relax }}{72}{figure.caption.9}\protected@file@percent }
\newlabel{fig:lec1_reg_losses}{{7}{72}{Schematic: Regression losses versus prediction error. The Huber loss transitions from quadratic to linear to reduce sensitivity to outliers.\relax }{figure.caption.9}{}}
\newlabel{fig:lec1_reg_losses@cref}{{[figure][7][]7}{[1][71][]72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Model Selection, Splits, and Learning Curves}{73}{subsection.3.6}\protected@file@percent }
\newlabel{sec:lec1_model_selection}{{3.6}{73}{Model Selection, Splits, and Learning Curves}{subsection.3.6}{}}
\newlabel{sec:lec1_model_selection@cref}{{[subsection][6][3]3.6}{[1][73][]73}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Schematic: Dataset partitioning into training, validation, and test segments. Any resampling scheme should preserve disjoint evaluation data; when classes are imbalanced, shuffle within strata so each split reflects the overall class mix.\relax }}{74}{figure.caption.10}\protected@file@percent }
\newlabel{fig:lec1_splits}{{8}{74}{Schematic: Dataset partitioning into training, validation, and test segments. Any resampling scheme should preserve disjoint evaluation data; when classes are imbalanced, shuffle within strata so each split reflects the overall class mix.\relax }{figure.caption.10}{}}
\newlabel{fig:lec1_splits@cref}{{[figure][8][]8}{[1][73][]74}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Schematic: Mini ERM pipeline (split once, iterate train/validate, then test only the best model on the held-out set).\relax }}{74}{figure.caption.11}\protected@file@percent }
\newlabel{fig:lec1_pipeline}{{9}{74}{Schematic: Mini ERM pipeline (split once, iterate train/validate, then test only the best model on the held-out set).\relax }{figure.caption.11}{}}
\newlabel{fig:lec1_pipeline@cref}{{[figure][9][]9}{[1][73][]74}}
\citation{Belkin2019}
\citation{Kaplan2020,Hoffmann2022}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Schematic: Learning curves reveal under/overfitting: the validation curve flattens while additional data continue to decrease training error only marginally. A shaded patience window marks when early stopping would halt if no validation improvement occurs.\relax }}{75}{figure.caption.12}\protected@file@percent }
\newlabel{fig:lec1_learning_curves}{{10}{75}{Schematic: Learning curves reveal under/overfitting: the validation curve flattens while additional data continue to decrease training error only marginally. A shaded patience window marks when early stopping would halt if no validation improvement occurs.\relax }{figure.caption.12}{}}
\newlabel{fig:lec1_learning_curves@cref}{{[figure][10][]10}{[1][73][]75}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Calibration and capacity diagnostics (reliability and double descent)}}{76}{figure.caption.13}\protected@file@percent }
\newlabel{fig:lec1-calibration-double-descent}{{11}{76}{Calibration and capacity diagnostics (reliability and double descent)}{figure.caption.13}{}}
\newlabel{fig:lec1-calibration-double-descent@cref}{{[figure][11][]11}{[1][76][]76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Linear regression: a first full case study}{76}{subsection.3.7}\protected@file@percent }
\newlabel{sec:linear_regression_closed}{{3.7}{76}{Linear regression: a first full case study}{subsection.3.7}{}}
\newlabel{sec:linear_regression_closed@cref}{{[subsection][7][3]3.7}{[1][76][]76}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Schematic: Ridge regularization shrinks parameter norms as the penalty strength increases.\relax }}{77}{figure.caption.14}\protected@file@percent }
\newlabel{fig:lec1_ridge}{{12}{77}{Schematic: Ridge regularization shrinks parameter norms as the penalty strength increases.\relax }{figure.caption.14}{}}
\newlabel{fig:lec1_ridge@cref}{{[figure][12][]12}{[1][76][]77}}
\@writefile{toc}{\contentsline {paragraph}{Model.}{77}{subsection.3.7}\protected@file@percent }
\newlabel{eq:linear_model}{{3.8}{77}{Model}{equation.3.8}{}}
\newlabel{eq:linear_model@cref}{{[equation][8][3]3.8}{[1][77][]77}}
\@writefile{toc}{\contentsline {paragraph}{A noise model (why squared error shows up).}{78}{equation.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Objective.}{78}{equation.3.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Closed form and geometry.}{79}{equation.3.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where overfitting enters.}{79}{equation.3.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ridge and lasso in one line.}{79}{equation.3.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{80}{equation.3.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{80}{equation.3.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Classification and Logistic Regression}{81}{section.4}\protected@file@percent }
\newlabel{chap:logistic}{{4}{81}{Classification and Logistic Regression}{section.4}{}}
\newlabel{chap:logistic@cref}{{[section][4][]4}{[1][80][]81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}From regression to classification}{81}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Classification problem statement}{82}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Bayes Optimal Classifier}{82}{subsection.4.3}\protected@file@percent }
\newlabel{eq:bayes_theorem}{{4.1}{82}{Bayes Optimal Classifier}{equation.4.1}{}}
\newlabel{eq:bayes_theorem@cref}{{[equation][1][4]4.1}{[1][82][]82}}
\@writefile{toc}{\contentsline {paragraph}{Challenges in Practice}{83}{equation.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Running example: a two-cluster dataset}{83}{equation.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Naive Bayes Approximation}{83}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Schematic: Synthetic binary dataset.}}{84}{figure.caption.15}\protected@file@percent }
\newlabel{fig:lec1_dataset}{{13}{84}{Schematic: Synthetic binary dataset}{figure.caption.15}{}}
\newlabel{fig:lec1_dataset@cref}{{[figure][13][]13}{[1][83][]84}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Schematic: Bayes-optimal boundary for two Gaussian classes with equal covariances and similar priors (LDA setting), which yields a linear separator. Unequal covariances produce a quadratic boundary. We place the boundary near the equal-posterior line (vertical, pink); left/right regions correspond to predicted classes R0 and R1.\relax }}{84}{figure.caption.16}\protected@file@percent }
\newlabel{fig:lec1_bayes}{{14}{84}{Schematic: Bayes-optimal boundary for two Gaussian classes with equal covariances and similar priors (LDA setting), which yields a linear separator. Unequal covariances produce a quadratic boundary. We place the boundary near the equal-posterior line (vertical, pink); left/right regions correspond to predicted classes R0 and R1.\relax }{figure.caption.16}{}}
\newlabel{fig:lec1_bayes@cref}{{[figure][14][]14}{[1][83][]84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Logistic Regression: A Probabilistic Discriminative Model}{85}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Binary Classification Setup}{85}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Model for the Log-Odds}{85}{subsection.4.4}\protected@file@percent }
\newlabel{eq:logit_linear}{{4.2}{85}{Linear Model for the Log-Odds}{equation.4.2}{}}
\newlabel{eq:logit_linear@cref}{{[equation][2][4]4.2}{[1][85][]85}}
\newlabel{eq:logistic_probability}{{4.3}{85}{Linear Model for the Log-Odds}{equation.4.3}{}}
\newlabel{eq:logistic_probability@cref}{{[equation][3][4]4.3}{[1][85][]85}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Likelihood, loss, and gradient}{86}{subsubsection.4.4.1}\protected@file@percent }
\newlabel{sec:lec2_logistic_likelihood}{{4.4.1}{86}{Likelihood, loss, and gradient}{subsubsection.4.4.1}{}}
\newlabel{sec:lec2_logistic_likelihood@cref}{{[subsubsection][1][4,4]4.4.1}{[1][86][]86}}
\newlabel{eq:lec2_bernoulli_likelihood}{{4.4}{86}{Likelihood, loss, and gradient}{equation.4.4}{}}
\newlabel{eq:lec2_bernoulli_likelihood@cref}{{[equation][4][4]4.4}{[1][86][]86}}
\newlabel{eq:lec2_loglik}{{4.5}{86}{Likelihood, loss, and gradient}{equation.4.5}{}}
\newlabel{eq:lec2_loglik@cref}{{[equation][5][4]4.5}{[1][86][]86}}
\newlabel{eq:lec2_logistic_grad}{{4.6}{86}{Likelihood, loss, and gradient}{equation.4.6}{}}
\newlabel{eq:lec2_logistic_grad@cref}{{[equation][6][4]4.6}{[1][86][]86}}
\@writefile{toc}{\contentsline {paragraph}{Optimization geometry (why iterative solvers)}{86}{figure.caption.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Schematic: The sigmoid maps logits to probabilities (left). The binary cross\hyp  {}entropy (negative log\hyp  {}likelihood) penalizes confident wrong predictions sharply (middle). Regularization typically shrinks parameter norms as the penalty strength increases (right).\relax }}{87}{figure.caption.17}\protected@file@percent }
\newlabel{fig:lec2_sigmoid_bce}{{15}{87}{Schematic: The sigmoid maps logits to probabilities (left). The binary cross\hyp {}entropy (negative log\hyp {}likelihood) penalizes confident wrong predictions sharply (middle). Regularization typically shrinks parameter norms as the penalty strength increases (right).\relax }{figure.caption.17}{}}
\newlabel{fig:lec2_sigmoid_bce@cref}{{[figure][15][]15}{[1][86][]87}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Schematic: Gradient-descent iterates contracting toward the minimizer of a convex quadratic cost. Ellipses are level sets; arrows show the ``steepest descent along contours'' direction.\relax }}{87}{figure.caption.18}\protected@file@percent }
\newlabel{fig:lec1_gd}{{16}{87}{Schematic: Gradient-descent iterates contracting toward the minimizer of a convex quadratic cost. Ellipses are level sets; arrows show the ``steepest descent along contours'' direction.\relax }{figure.caption.18}{}}
\newlabel{fig:lec1_gd@cref}{{[figure][16][]16}{[1][87][]87}}
\@writefile{toc}{\contentsline {paragraph}{Geometry of the logistic surface.}{88}{figure.caption.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Schematic: Illustrative logistic-regression boundary. The dashed line marks the linear decision boundary at probability 0.5; labeled contours show how the posterior varies smoothly with margin, enabling calibrated decisions and adjustable thresholds.\relax }}{88}{figure.caption.19}\protected@file@percent }
\newlabel{fig:lec2-logistic-boundary}{{17}{88}{Schematic: Illustrative logistic-regression boundary. The dashed line marks the linear decision boundary at probability 0.5; labeled contours show how the posterior varies smoothly with margin, enabling calibrated decisions and adjustable thresholds.\relax }{figure.caption.19}{}}
\newlabel{fig:lec2-logistic-boundary@cref}{{[figure][17][]17}{[1][87][]88}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Probabilistic Interpretation: MLE and MAP}{88}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Schematic: MAP estimates interpolate between the prior mean and the data-driven MLE. As the sample size grows, the MAP curve approaches the true mean.\relax }}{89}{figure.caption.20}\protected@file@percent }
\newlabel{fig:lec1_mle_map}{{18}{89}{Schematic: MAP estimates interpolate between the prior mean and the data-driven MLE. As the sample size grows, the MAP curve approaches the true mean.\relax }{figure.caption.20}{}}
\newlabel{fig:lec1_mle_map@cref}{{[figure][18][]18}{[1][89][]89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Confusion Matrices and Derived Metrics}{89}{subsection.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Schematic: ROC and PR curves with an explicit operating point. Left: ROC curve with iso-cost lines; right: PR curve with a class-prevalence baseline and iso-F1 contours. Together they visualize threshold trade-offs and calibration quality.\relax }}{90}{figure.caption.21}\protected@file@percent }
\newlabel{fig:lec1-roc-pr}{{19}{90}{Schematic: ROC and PR curves with an explicit operating point. Left: ROC curve with iso-cost lines; right: PR curve with a class-prevalence baseline and iso-F1 contours. Together they visualize threshold trade-offs and calibration quality.\relax }{figure.caption.21}{}}
\newlabel{fig:lec1-roc-pr@cref}{{[figure][19][]19}{[1][89][]90}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Schematic: Confusion matrix for a three-class classifier; diagonals dominate, indicating strong accuracy with modest confusion between classes B and C.\relax }}{90}{figure.caption.22}\protected@file@percent }
\newlabel{fig:lec1_confusion}{{20}{90}{Schematic: Confusion matrix for a three-class classifier; diagonals dominate, indicating strong accuracy with modest confusion between classes B and C.\relax }{figure.caption.22}{}}
\newlabel{fig:lec1_confusion@cref}{{[figure][20][]20}{[1][90][]90}}
\citation{Platt1999,Guo2017}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Schematic: Handling class imbalance for logistic models (\Cref  {chap:logistic} reference table).\relax }}{91}{table.caption.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{92}{table.caption.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{92}{table.caption.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Introduction to Neural Networks}{92}{section.5}\protected@file@percent }
\newlabel{chap:perceptron}{{5}{92}{Introduction to Neural Networks}{section.5}{}}
\newlabel{chap:perceptron@cref}{{[section][5][]5}{[1][92][]92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Biological Inspiration}{93}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neurons and Neural Activity}{93}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Complexities and Unknowns}{93}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}From Biological to Artificial Neural Networks}{94}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Features of Artificial Neural Networks}{94}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Historical Context}{94}{Item.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Outline of Neural Network Study}{95}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Neural Network Architectures}{95}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feedforward Neural Networks}{95}{subsection.5.4}\protected@file@percent }
\newlabel{eq:feedforward}{{5.2}{95}{Feedforward Neural Networks}{equation.5.2}{}}
\newlabel{eq:feedforward@cref}{{[equation][2][5]5.2}{[1][95][]95}}
\@writefile{toc}{\contentsline {paragraph}{Shapes and convention.}{96}{equation.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recurrent Neural Networks}{96}{equation.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Activation Functions}{96}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Biological Motivation}{96}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common Activation Functions}{96}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Learning Paradigms in Neural Networks}{98}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised Learning}{98}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unsupervised Learning}{98}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reinforcement Learning}{99}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Fundamentals of Artificial Neural Networks}{99}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{McCulloch-Pitts Neuron Model}{99}{subsection.5.7}\protected@file@percent }
\newlabel{eq:weighted_sum}{{5.3}{99}{McCulloch-Pitts Neuron Model}{equation.5.3}{}}
\newlabel{eq:weighted_sum@cref}{{[equation][3][5]5.3}{[1][99][]99}}
\newlabel{eq:threshold_output}{{5.4}{99}{McCulloch-Pitts Neuron Model}{equation.5.4}{}}
\newlabel{eq:threshold_output@cref}{{[equation][4][5]5.4}{[1][99][]99}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{100}{equation.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Excitation and Inhibition}{100}{equation.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning Objective}{100}{equation.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Mathematical Formulation of the Neuron Output}{100}{subsection.5.8}\protected@file@percent }
\newlabel{eq:neuron_output}{{5.6}{100}{Mathematical Formulation of the Neuron Output}{equation.5.6}{}}
\newlabel{eq:neuron_output@cref}{{[equation][6][5]5.6}{[1][100][]100}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}McCulloch-Pitts neuron: examples and limits}{101}{subsection.5.9}\protected@file@percent }
\newlabel{eq:mp-neuron}{{5.8}{101}{McCulloch-Pitts neuron: examples and limits}{equation.5.8}{}}
\newlabel{eq:mp-neuron@cref}{{[equation][8][5]5.8}{[1][101][]101}}
\@writefile{toc}{\contentsline {paragraph}{Example: AND and OR gates}{101}{equation.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of the MP model}{102}{equation.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10}From MP Neuron to Perceptron and Beyond}{102}{subsection.5.10}\protected@file@percent }
\newlabel{sec:perceptron}{{5.10}{102}{From MP Neuron to Perceptron and Beyond}{subsection.5.10}{}}
\newlabel{sec:perceptron@cref}{{[subsection][10][5]5.10}{[1][102][]102}}
\@writefile{toc}{\contentsline {paragraph}{Perceptron model}{102}{subsection.5.10}\protected@file@percent }
\newlabel{eq:perceptron}{{5.9}{102}{Perceptron model}{equation.5.9}{}}
\newlabel{eq:perceptron@cref}{{[equation][9][5]5.9}{[1][102][]102}}
\@writefile{toc}{\contentsline {paragraph}{Perceptron update from the signed margin.}{103}{equation.5.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perceptron convergence theorem.}{103}{equation.5.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Schematic: Perceptron geometry. Points on either side of the separating hyperplane receive different labels, and the signed distance to the boundary controls both the class prediction and the magnitude of the update during learning. Compare to \Cref  {fig:lec2-logistic-boundary} in \Cref  {chap:logistic}: both share a linear separator, but logistic smooths the boundary into calibrated probabilities.\relax }}{105}{figure.caption.24}\protected@file@percent }
\newlabel{fig:lec3-perceptron-geometry}{{21}{105}{Schematic: Perceptron geometry. Points on either side of the separating hyperplane receive different labels, and the signed distance to the boundary controls both the class prediction and the magnitude of the update during learning. Compare to \Cref {fig:lec2-logistic-boundary} in \Cref {chap:logistic}: both share a linear separator, but logistic smooths the boundary into calibrated probabilities.\relax }{figure.caption.24}{}}
\newlabel{fig:lec3-perceptron-geometry@cref}{{[figure][21][]21}{[1][105][]105}}
\@writefile{toc}{\contentsline {paragraph}{Adaline model}{105}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adaline weight update (derivation)}{105}{figure.caption.24}\protected@file@percent }
\citation{McCullochPitts1943}
\citation{Rosenblatt1958}
\citation{WidrowHoff1960}
\citation{Rumelhart1986}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{107}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{107}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Multi-Layer Perceptrons: Challenges and Foundations}{108}{section.6}\protected@file@percent }
\newlabel{chap:mlp}{{6}{108}{Multi-Layer Perceptrons: Challenges and Foundations}{section.6}{}}
\newlabel{chap:mlp@cref}{{[section][6][]6}{[1][107][]108}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}From a single unit to the smallest network}{109}{subsection.6.1}\protected@file@percent }
\newlabel{sec:mlp-limitations}{{6.1}{109}{From a single unit to the smallest network}{subsection.6.1}{}}
\newlabel{sec:mlp-limitations@cref}{{[subsection][1][6]6.1}{[1][109][]109}}
\@writefile{toc}{\contentsline {paragraph}{Function estimation as the unifying view.}{109}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From one unit to a chain of units.}{109}{subsection.6.1}\protected@file@percent }
\newlabel{eq:perceptron_forward}{{6.1}{109}{From one unit to a chain of units}{equation.6.1}{}}
\newlabel{eq:perceptron_forward@cref}{{[equation][1][6]6.1}{[1][109][]109}}
\newlabel{eq:two_neuron_forward}{{6.3}{109}{From one unit to a chain of units}{equation.6.3}{}}
\newlabel{eq:two_neuron_forward@cref}{{[equation][3][6]6.3}{[1][109][]109}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Schematic: The minimal neural network used in this chapter is a two-neuron chain. The first unit produces an intermediate signal, and the second unit maps that signal to the final output.\relax }}{110}{figure.caption.25}\protected@file@percent }
\newlabel{fig:mlp_minimal_chain}{{22}{110}{Schematic: The minimal neural network used in this chapter is a two-neuron chain. The first unit produces an intermediate signal, and the second unit maps that signal to the final output.\relax }{figure.caption.25}{}}
\newlabel{fig:mlp_minimal_chain@cref}{{[figure][22][]22}{[1][110][]110}}
\@writefile{toc}{\contentsline {paragraph}{Bias as a learned threshold.}{111}{figure.caption.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Performance: what are we trying to improve?}{111}{subsection.6.2}\protected@file@percent }
\newlabel{eq:performance}{{6.4}{111}{Performance: what are we trying to improve?}{equation.6.4}{}}
\newlabel{eq:performance@cref}{{[equation][4][6]6.4}{[1][111][]111}}
\@writefile{toc}{\contentsline {paragraph}{Why a square?}{111}{equation.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A geometric intuition.}{111}{equation.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Gradient descent: how do weights move?}{112}{subsection.6.3}\protected@file@percent }
\newlabel{eq:gd_update}{{6.5}{112}{Gradient descent: how do weights move?}{equation.6.5}{}}
\newlabel{eq:gd_update@cref}{{[equation][5][6]6.5}{[1][111][]112}}
\newlabel{eq:vectorized_update}{{6.6}{112}{Gradient descent: how do weights move?}{equation.6.6}{}}
\newlabel{eq:vectorized_update@cref}{{[equation][6][6]6.6}{[1][112][]112}}
\@writefile{toc}{\contentsline {paragraph}{Step size is a design choice.}{112}{equation.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Why hard thresholds block learning}{112}{subsection.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Schematic: Think of performance as a surface over the weights. Gradient descent moves in one vector step (blue), whereas coordinate-wise updates can zig-zag (orange).\relax }}{113}{figure.caption.26}\protected@file@percent }
\newlabel{fig:mlp_gd_surface}{{23}{113}{Schematic: Think of performance as a surface over the weights. Gradient descent moves in one vector step (blue), whereas coordinate-wise updates can zig-zag (orange).\relax }{figure.caption.26}{}}
\newlabel{fig:mlp_gd_surface@cref}{{[figure][23][]23}{[1][112][]113}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Schematic: Hard thresholds block gradient-based learning because the derivative is zero almost everywhere. A smooth activation like the sigmoid provides informative derivatives across a wide range of inputs.\relax }}{113}{figure.caption.27}\protected@file@percent }
\newlabel{fig:mlp_step_vs_sigmoid}{{24}{113}{Schematic: Hard thresholds block gradient-based learning because the derivative is zero almost everywhere. A smooth activation like the sigmoid provides informative derivatives across a wide range of inputs.\relax }{figure.caption.27}{}}
\newlabel{fig:mlp_step_vs_sigmoid@cref}{{[figure][24][]24}{[1][113][]113}}
\@writefile{toc}{\contentsline {paragraph}{Absorbing the threshold.}{113}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Differentiable activations and the sigmoid trick}{113}{subsection.6.5}\protected@file@percent }
\newlabel{eq:sigmoid}{{6.7}{113}{Differentiable activations and the sigmoid trick}{equation.6.7}{}}
\newlabel{eq:sigmoid@cref}{{[equation][7][6]6.7}{[1][113][]113}}
\newlabel{eq:sigmoid_derivative}{{6.8}{114}{Differentiable activations and the sigmoid trick}{equation.6.8}{}}
\newlabel{eq:sigmoid_derivative@cref}{{[equation][8][6]6.8}{[1][113][]114}}
\@writefile{toc}{\contentsline {paragraph}{Derivation sketch.}{114}{equation.6.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Deriving weight updates for the two\hyp  {}neuron network}{114}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Second layer.}{115}{subsection.6.6}\protected@file@percent }
\newlabel{eq:grad_w2}{{6.9}{115}{Second layer}{equation.6.9}{}}
\newlabel{eq:grad_w2@cref}{{[equation][9][6]6.9}{[1][114][]115}}
\newlabel{eq:grad_b2}{{6.10}{115}{Second layer}{equation.6.10}{}}
\newlabel{eq:grad_b2@cref}{{[equation][10][6]6.10}{[1][115][]115}}
\@writefile{toc}{\contentsline {paragraph}{First layer.}{115}{equation.6.10}\protected@file@percent }
\newlabel{eq:grad_w1}{{6.12}{115}{First layer}{equation.6.12}{}}
\newlabel{eq:grad_w1@cref}{{[equation][12][6]6.12}{[1][115][]115}}
\newlabel{eq:grad_b1}{{6.13}{115}{First layer}{equation.6.13}{}}
\newlabel{eq:grad_b1@cref}{{[equation][13][6]6.13}{[1][115][]115}}
\@writefile{toc}{\contentsline {paragraph}{Error terms (backprop view).}{115}{equation.6.13}\protected@file@percent }
\newlabel{eq:delta2}{{6.14}{115}{Error terms (backprop view)}{equation.6.14}{}}
\newlabel{eq:delta2@cref}{{[equation][14][6]6.14}{[1][115][]115}}
\newlabel{eq:delta1}{{6.15}{115}{Error terms (backprop view)}{equation.6.15}{}}
\newlabel{eq:delta1@cref}{{[equation][15][6]6.15}{[1][115][]115}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}From two neurons to multi\hyp  {}layer networks}{116}{subsection.6.7}\protected@file@percent }
\newlabel{eq:delta_recursion}{{6.16}{116}{From two neurons to multi\hyp {}layer networks}{equation.6.16}{}}
\newlabel{eq:delta_recursion@cref}{{[equation][16][6]6.16}{[1][116][]116}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Summary}{117}{subsection.6.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{118}{subsection.6.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{118}{subsection.6.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Backpropagation Learning in Multi-Layer Perceptrons}{118}{section.7}\protected@file@percent }
\newlabel{chap:backprop}{{7}{118}{Backpropagation Learning in Multi-Layer Perceptrons}{section.7}{}}
\newlabel{chap:backprop@cref}{{[section][7][]7}{[1][118][]118}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Context and Motivation}{119}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Problem Setup}{119}{subsection.7.2}\protected@file@percent }
\newlabel{eq:forward_z}{{7.1}{119}{Problem Setup}{equation.7.1}{}}
\newlabel{eq:forward_z@cref}{{[equation][1][7]7.1}{[1][119][]119}}
\newlabel{eq:forward_a}{{7.2}{119}{Problem Setup}{equation.7.2}{}}
\newlabel{eq:forward_a@cref}{{[equation][2][7]7.2}{[1][119][]119}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Loss and Objective}{119}{subsection.7.3}\protected@file@percent }
\newlabel{eq:error_function}{{7.3}{120}{Loss and Objective}{equation.7.3}{}}
\newlabel{eq:error_function@cref}{{[equation][3][7]7.3}{[1][119][]120}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Challenges in Weight Updates}{120}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Notation for Layers and Neurons}{120}{subsection.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Forward Pass Recap}{121}{subsection.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Backpropagation: Recursive Computation of Error Terms}{123}{subsection.7.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chain rule decomposition}{123}{equation.7.6}\protected@file@percent }
\newlabel{eq:chain_rule_weight}{{7.7}{123}{Chain rule decomposition}{equation.7.7}{}}
\newlabel{eq:chain_rule_weight@cref}{{[equation][7][7]7.7}{[1][123][]123}}
\newlabel{eq:weight_gradient}{{7.8}{124}{Chain rule decomposition}{equation.7.8}{}}
\newlabel{eq:weight_gradient@cref}{{[equation][8][7]7.8}{[1][124][]124}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation of \(\delta _j^{(l+1)}\)}{124}{equation.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.7.1}Output layer error terms}{124}{subsubsection.7.7.1}\protected@file@percent }
\newlabel{eq:delta_output}{{7.11}{125}{Output layer error terms}{equation.7.11}{}}
\newlabel{eq:delta_output@cref}{{[equation][11][7]7.11}{[1][124][]125}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.7.2}Hidden layer error terms}{125}{subsubsection.7.7.2}\protected@file@percent }
\newlabel{eq:delta_hidden_chain}{{7.14}{125}{Hidden layer error terms}{equation.7.14}{}}
\newlabel{eq:delta_hidden_chain@cref}{{[equation][14][7]7.14}{[1][125][]125}}
\newlabel{eq:delta_hidden}{{7.15}{125}{Hidden layer error terms}{equation.7.15}{}}
\newlabel{eq:delta_hidden@cref}{{[equation][15][7]7.15}{[1][125][]125}}
\@writefile{toc}{\contentsline {paragraph}{Summary: Backpropagation recursion}{126}{equation.7.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8}Backpropagation Algorithm: Detailed Derivation}{126}{subsection.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Error function and its derivatives}{126}{subsection.7.8}\protected@file@percent }
\newlabel{eq:chain_rule}{{7.17}{126}{Error function and its derivatives}{equation.7.17}{}}
\newlabel{eq:chain_rule@cref}{{[equation][17][7]7.17}{[1][126][]126}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Derivative of error with respect to output}{126}{equation.7.17}\protected@file@percent }
\newlabel{eq:dE_do}{{7.18}{126}{Step 1: Derivative of error with respect to output}{equation.7.18}{}}
\newlabel{eq:dE_do@cref}{{[equation][18][7]7.18}{[1][126][]126}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Derivative of output with respect to activation}{126}{equation.7.18}\protected@file@percent }
\newlabel{eq:do_da}{{7.19}{127}{Step 2: Derivative of output with respect to activation}{equation.7.19}{}}
\newlabel{eq:do_da@cref}{{[equation][19][7]7.19}{[1][126][]127}}
\@writefile{toc}{\contentsline {paragraph}{Step 3: Derivative of activation with respect to weight}{127}{equation.7.19}\protected@file@percent }
\newlabel{eq:da_dw}{{7.20}{127}{Step 3: Derivative of activation with respect to weight}{equation.7.20}{}}
\newlabel{eq:da_dw@cref}{{[equation][20][7]7.20}{[1][127][]127}}
\@writefile{toc}{\contentsline {paragraph}{Putting it all together}{127}{equation.7.20}\protected@file@percent }
\newlabel{eq:dE_dw}{{7.21}{127}{Putting it all together}{equation.7.21}{}}
\newlabel{eq:dE_dw@cref}{{[equation][21][7]7.21}{[1][127][]127}}
\newlabel{eq:error_signal_output}{{7.22}{127}{Putting it all together}{equation.7.22}{}}
\newlabel{eq:error_signal_output@cref}{{[equation][22][7]7.22}{[1][127][]127}}
\newlabel{eq:dE_dw_delta}{{7.23}{127}{Putting it all together}{equation.7.23}{}}
\newlabel{eq:dE_dw_delta@cref}{{[equation][23][7]7.23}{[1][127][]127}}
\newlabel{eq:weight_update_output}{{7.24}{127}{Putting it all together}{equation.7.24}{}}
\newlabel{eq:weight_update_output@cref}{{[equation][24][7]7.24}{[1][127][]127}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9}Backpropagation for Hidden Layers}{127}{subsection.7.9}\protected@file@percent }
\newlabel{eq:error_signal_hidden}{{7.25}{127}{Backpropagation for Hidden Layers}{equation.7.25}{}}
\newlabel{eq:error_signal_hidden@cref}{{[equation][25][7]7.25}{[1][127][]127}}
\newlabel{eq:weight_update_hidden}{{7.26}{128}{Backpropagation for Hidden Layers}{equation.7.26}{}}
\newlabel{eq:weight_update_hidden@cref}{{[equation][26][7]7.26}{[1][128][]128}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10}Batch and Stochastic Gradient Descent}{128}{subsection.7.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Computational graph for backpropagation (reverse-mode AD)}}{129}{figure.caption.28}\protected@file@percent }
\newlabel{fig:backprop-computational-graph}{{25}{129}{Computational graph for backpropagation (reverse-mode AD)}{figure.caption.28}{}}
\newlabel{fig:backprop-computational-graph@cref}{{[figure][25][]25}{[1][128][]129}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11}Backpropagation Algorithm: Brief Numerical Check}{130}{subsection.7.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Aside: squared-error loss (alternative)}{130}{subsection.7.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backward Propagation of Error}{131}{equation.7.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weight Update Rule}{131}{equation.7.30}\protected@file@percent }
\newlabel{eq:weight_update}{{7.31}{131}{Weight Update Rule}{equation.7.31}{}}
\newlabel{eq:weight_update@cref}{{[equation][31][7]7.31}{[1][131][]131}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation of Learning Rate and Momentum}{132}{equation.7.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step-by-Step Example}{132}{equation.7.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Remarks}{133}{Item.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12}Training Procedure and Epochs in Multi-Layer Perceptrons}{133}{subsection.7.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Remarks:}{134}{Item.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13}Role and Design of Hidden Layers}{134}{subsection.7.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Questions Regarding Hidden Layers:}{134}{subsection.7.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design Considerations:}{135}{subsection.7.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trade-offs:}{135}{subsection.7.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14}Case Study: Learning the Function \( y = x \sin x \)}{135}{subsection.7.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Setup:}{135}{subsection.7.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Questions to Explore:}{135}{subsection.7.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Remarks:}{136}{subsection.7.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.15}Applications of Multi-Layer Perceptrons}{136}{subsection.7.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary:}{136}{subsection.7.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.16}Limitations of Multi-Layer Perceptrons}{137}{subsection.7.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.17}Conclusion of Multi-Layer Perceptron Derivations}{137}{subsection.7.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation Algorithm Recap}{137}{subsection.7.17}\protected@file@percent }
\newlabel{eq:grad_W}{{7.34}{138}{Backpropagation Algorithm Recap}{equation.7.34}{}}
\newlabel{eq:grad_W@cref}{{[equation][34][7]7.34}{[1][138][]138}}
\newlabel{eq:grad_b}{{7.35}{138}{Backpropagation Algorithm Recap}{equation.7.35}{}}
\newlabel{eq:grad_b@cref}{{[equation][35][7]7.35}{[1][138][]138}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Schematic: Forward (blue) and backward (orange) flows for a two-layer MLP. The cached activations and the layerwise error terms (deltas) are exactly the quantities carried along these arrows.\relax }}{138}{figure.caption.29}\protected@file@percent }
\newlabel{fig:lec4_backprop_flow}{{26}{138}{Schematic: Forward (blue) and backward (orange) flows for a two-layer MLP. The cached activations and the layerwise error terms (deltas) are exactly the quantities carried along these arrows.\relax }{figure.caption.29}{}}
\newlabel{fig:lec4_backprop_flow@cref}{{[figure][26][]26}{[1][138][]138}}
\@writefile{toc}{\contentsline {paragraph}{Example Execution}{138}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Remarks on Convergence and Practical Considerations}{139}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparing canonical nonlinearities}{139}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trade-offs}{139}{figure.caption.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Schematic: Canonical activation functions on a common axis. Solid curves show the activation; dashed curves show its derivative.\relax }}{140}{figure.caption.30}\protected@file@percent }
\newlabel{fig:lec4-activations}{{27}{140}{Schematic: Canonical activation functions on a common axis. Solid curves show the activation; dashed curves show its derivative.\relax }{figure.caption.30}{}}
\newlabel{fig:lec4-activations@cref}{{[figure][27][]27}{[1][139][]140}}
\citation{Rumelhart1986}
\citation{Haykin2009}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{141}{figure.caption.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{141}{figure.caption.30}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Radial Basis Function Networks (RBFNs)}{142}{section.8}\protected@file@percent }
\newlabel{chap:rbf}{{8}{142}{Radial Basis Function Networks (RBFNs)}{section.8}{}}
\newlabel{chap:rbf@cref}{{[section][8][]8}{[1][141][]142}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Overview and Motivation}{142}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Architecture of RBFNs}{143}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Schematic: How the width parameter (sigma) influences decision boundaries on a 2D toy dataset. Too-large sigma underfits, intermediate sigma captures the boundary, too-small sigma overfits with fragmented regions. Use \Cref  {chap:supervised}'s validation curves to pick model size and regularization.\relax }}{144}{figure.caption.31}\protected@file@percent }
\newlabel{fig:rbf_sigma_sweep}{{28}{144}{Schematic: How the width parameter (sigma) influences decision boundaries on a 2D toy dataset. Too-large sigma underfits, intermediate sigma captures the boundary, too-small sigma overfits with fragmented regions. Use \Cref {chap:supervised}'s validation curves to pick model size and regularization.\relax }{figure.caption.31}{}}
\newlabel{fig:rbf_sigma_sweep@cref}{{[figure][28][]28}{[1][143][]144}}
\@writefile{toc}{\contentsline {paragraph}{A picture to keep in mind}{144}{figure.caption.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Schematic: RBFN architecture. Inputs feed fixed radial units parameterized by centers and widths; a linear readout with weights (and bias) is trained by a regression or classification loss. Only the output weights are typically learned; centers/widths come from k-means or spacing heuristics.\relax }}{145}{figure.caption.32}\protected@file@percent }
\newlabel{fig:rbf_architecture_weights}{{29}{145}{Schematic: RBFN architecture. Inputs feed fixed radial units parameterized by centers and widths; a linear readout with weights (and bias) is trained by a regression or classification loss. Only the output weights are typically learned; centers/widths come from k-means or spacing heuristics.\relax }{figure.caption.32}{}}
\newlabel{fig:rbf_architecture_weights@cref}{{[figure][29][]29}{[1][144][]145}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Mathematical Formulation}{145}{subsubsection.8.2.1}\protected@file@percent }
\newlabel{eq:rbfn_output}{{8.1}{145}{Mathematical Formulation}{equation.8.1}{}}
\newlabel{eq:rbfn_output@cref}{{[equation][1][8]8.1}{[1][145][]145}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Schematic: Localized Gaussian basis functions (dashed) and their weighted sum (solid). Overlapping bumps allow RBF networks to interpolate complex signals smoothly.\relax }}{146}{figure.caption.33}\protected@file@percent }
\newlabel{fig:rbf_gaussian_bumps}{{30}{146}{Schematic: Localized Gaussian basis functions (dashed) and their weighted sum (solid). Overlapping bumps allow RBF networks to interpolate complex signals smoothly.\relax }{figure.caption.33}{}}
\newlabel{fig:rbf_gaussian_bumps@cref}{{[figure][30][]30}{[1][144][]146}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{146}{equation.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Radial Basis Functions}{146}{subsection.8.3}\protected@file@percent }
\newlabel{eq:gaussian_rbf}{{8.2}{146}{Radial Basis Functions}{equation.8.2}{}}
\newlabel{eq:gaussian_rbf@cref}{{[equation][2][8]8.2}{[1][146][]146}}
\@writefile{toc}{\contentsline {paragraph}{Normalized RBFs.}{146}{equation.8.2}\protected@file@percent }
\citation{ParkSandberg1991,Micchelli1986}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Schematic: Center placement and overlap. Top: K-means prototypes roughly tile the data manifold, giving even overlap; bottom: random centers can leave gaps or excessive overlap, influencing the width (sigma) choice and conditioning.\relax }}{147}{figure.caption.34}\protected@file@percent }
\newlabel{fig:rbf_centres}{{31}{147}{Schematic: Center placement and overlap. Top: K-means prototypes roughly tile the data manifold, giving even overlap; bottom: random centers can leave gaps or excessive overlap, influencing the width (sigma) choice and conditioning.\relax }{figure.caption.34}{}}
\newlabel{fig:rbf_centres@cref}{{[figure][31][]31}{[1][144][]147}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Key Properties and Advantages}{147}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Curse of dimensionality.}{148}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Transforming Nonlinearly Separable Data into Linearly Separable Space}{148}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example Setup:}{148}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Assumptions:}{148}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transformation Results:}{148}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Finding the Optimal Weight Vector \(\mathbf  {w}\)}{149}{subsection.8.6}\protected@file@percent }
\newlabel{eq:cost_function}{{8.3}{149}{Finding the Optimal Weight Vector \texorpdfstring {\(\mathbf {w}\)}{w}}{equation.8.3}{}}
\newlabel{eq:cost_function@cref}{{[equation][3][8]8.3}{[1][149][]149}}
\@writefile{toc}{\contentsline {paragraph}{Normal Equations for the Weights:}{149}{equation.8.3}\protected@file@percent }
\newlabel{eq:normal_eq_weights}{{8.4}{149}{Normal Equations for the Weights:}{equation.8.4}{}}
\newlabel{eq:normal_eq_weights@cref}{{[equation][4][8]8.4}{[1][149][]149}}
\@writefile{toc}{\contentsline {paragraph}{Conditioning and capacity.}{149}{equation.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}The Role of the Transformation Function \(g(\cdot )\)}{149}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Choosing \(g(\cdot )\):}{150}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Examples of Kernel Functions}{150}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Inverse Distance Function:}{150}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Gaussian Radial Basis Function:}{150}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9}Interpretation of the Width Parameter \(\sigma \)}{150}{subsection.8.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10}Effect of \(\sigma \) on Classification Boundaries}{151}{subsection.8.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notation note.}{151}{subsection.8.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.11}Radial Basis Function Networks: Parameter Estimation and Training}{151}{subsection.8.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Finding the Centers \(\mathbf  {v}_i\):}{151}{subsection.8.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Determining the Spread Parameters \(\sigma _i\):}{152}{subsection.8.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training the Output Weights \( w_i \):}{152}{subsection.8.11}\protected@file@percent }
\newlabel{eq:rbf_weight_training}{{8.5}{152}{Training the Output Weights \( w_i \):}{equation.8.5}{}}
\newlabel{eq:rbf_weight_training@cref}{{[equation][5][8]8.5}{[1][152][]152}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Schematic: Primal (finite basis) vs.\ dual (kernel ridge) viewpoints. Using as many centers as data points recovers the dual form; using fewer centers corresponds to a Nystr\"om approximation. The same trade-off appears in kernel methods through the choice of kernel and effective rank.\relax }}{154}{figure.caption.35}\protected@file@percent }
\newlabel{fig:rbf_primal_dual}{{32}{154}{Schematic: Primal (finite basis) vs.\ dual (kernel ridge) viewpoints. Using as many centers as data points recovers the dual form; using fewer centers corresponds to a Nystr\"om approximation. The same trade-off appears in kernel methods through the choice of kernel and effective rank.\relax }{figure.caption.35}{}}
\newlabel{fig:rbf_primal_dual@cref}{{[figure][32][]32}{[1][153][]154}}
\@writefile{toc}{\contentsline {paragraph}{Iterative Optimization of \(\sigma _i\) and \( w_i \):}{154}{figure.caption.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of the Training Algorithm:}{154}{Item.51}\protected@file@percent }
\citation{Micchelli1986,ParkSandberg1991}
\@writefile{toc}{\contentsline {paragraph}{Worked toy (classification, XOR-like).}{155}{Item.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.12}Remarks on Radial Basis Function Networks}{155}{subsection.8.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{155}{subsection.8.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Schematic: RBFN decision boundary on the XOR toy for a model with 4 centers, width sigma = 0.8, and ridge lambda = 1e-3. Shading indicates the predicted class under a 0.5 threshold; the black contour marks the 0.5 boundary. Training points are overlaid (class 0: open circles; class 1: filled squares). See \Cref  {fig:rbf_sigma_sweep} for how sigma changes this boundary.\relax }}{156}{figure.caption.36}\protected@file@percent }
\newlabel{fig:rbf_boundary}{{33}{156}{Schematic: RBFN decision boundary on the XOR toy for a model with 4 centers, width sigma = 0.8, and ridge lambda = 1e-3. Shading indicates the predicted class under a 0.5 threshold; the black contour marks the 0.5 boundary. Training points are overlaid (class 0: open circles; class 1: filled squares). See \Cref {fig:rbf_sigma_sweep} for how sigma changes this boundary.\relax }{figure.caption.36}{}}
\newlabel{fig:rbf_boundary@cref}{{[figure][33][]33}{[1][155][]156}}
\@writefile{toc}{\contentsline {paragraph}{Disadvantages:}{157}{subsection.8.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sidebar: why include Wiener filtering here?}{157}{subsection.8.12}\protected@file@percent }
\newlabel{eq:mse_cost}{{8.6}{157}{Sidebar: why include Wiener filtering here?}{equation.8.6}{}}
\newlabel{eq:mse_cost@cref}{{[equation][6][8]8.6}{[1][157][]157}}
\newlabel{eq:wiener_hopf}{{8.7}{157}{Sidebar: why include Wiener filtering here?}{equation.8.7}{}}
\newlabel{eq:wiener_hopf@cref}{{[equation][7][8]8.7}{[1][157][]157}}
\newlabel{eq:wiener_solution}{{8.8}{158}{Sidebar: why include Wiener filtering here?}{equation.8.8}{}}
\newlabel{eq:wiener_solution@cref}{{[equation][8][8]8.8}{[1][157][]158}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.13}Interpretation and Properties of the Wiener Filter}{158}{subsection.8.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{158}{subsection.8.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Properties:}{158}{subsection.8.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.14}Extension: Frequency-Domain Wiener Filter}{158}{subsection.8.14}\protected@file@percent }
\newlabel{eq:wiener_freq}{{8.9}{158}{Extension: Frequency-Domain Wiener Filter}{equation.8.9}{}}
\newlabel{eq:wiener_freq@cref}{{[equation][9][8]8.9}{[1][158][]158}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.15}Closing Remarks on Adaptive Filtering}{158}{subsection.8.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.16}Preview: Unsupervised and Localized Learning}{159}{subsection.8.16}\protected@file@percent }
\citation{Haykin2013AdaptiveFilterTheory}
\citation{WidrowStearns1985}
\citation{SchreierScharf2010}
\citation{Micchelli1986}
\citation{ParkSandberg1991}
\citation{PoggioGirosi1990}
\citation{Bishop1995}
\citation{HastieTibshiraniFriedman2009}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{160}{subsection.8.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{160}{subsection.8.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Introduction to Self-Organizing Networks\\and Unsupervised Learning}{160}{section.9}\protected@file@percent }
\newlabel{chap:som}{{9}{160}{\texorpdfstring {Introduction to Self-Organizing Networks\\and Unsupervised Learning}{Introduction to Self-Organizing Networks and Unsupervised Learning}}{section.9}{}}
\newlabel{chap:som@cref}{{[section][9][]9}{[1][160][]160}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Overview of Self-Organizing Networks}{161}{subsection.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Schematic: Learning-rate scheduling intuition. On a smooth objective (left), large initial steps quickly cover ground and roughly align prototypes, while a decaying step-size refines the solution near convergence. Right: common exponential and multiplicative decays used in SOM training.\relax }}{162}{figure.caption.37}\protected@file@percent }
\newlabel{fig:lec5-learning-rate}{{34}{162}{Schematic: Learning-rate scheduling intuition. On a smooth objective (left), large initial steps quickly cover ground and roughly align prototypes, while a decaying step-size refines the solution near convergence. Right: common exponential and multiplicative decays used in SOM training.\relax }{figure.caption.37}{}}
\newlabel{fig:lec5-learning-rate@cref}{{[figure][34][]34}{[1][162][]162}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Clustering: Identifying Similarities and Dissimilarities}{163}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{163}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{K-means Clustering:}{163}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Dimensionality Reduction: Simplifying High-Dimensional Data}{164}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{164}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenges:}{164}{figure.caption.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common Techniques:}{164}{figure.caption.38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Schematic: Classical MDS intuition. Projecting a cube onto a plane via an orthogonal map yields a square (left), whereas an oblique projection along a body diagonal produces a hexagon (right). The local adjacency of vertices is preserved even though metric structure is distorted.\relax }}{165}{figure.caption.38}\protected@file@percent }
\newlabel{fig:lec5-mds-projection}{{35}{165}{Schematic: Classical MDS intuition. Projecting a cube onto a plane via an orthogonal map yields a square (left), whereas an oblique projection along a body diagonal produces a hexagon (right). The local adjacency of vertices is preserved even though metric structure is distorted.\relax }{figure.caption.38}{}}
\newlabel{fig:lec5-mds-projection@cref}{{[figure][35][]35}{[1][164][]165}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Dimensionality Reduction and Feature Mapping}{165}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Self-Organizing Maps (SOMs): Introduction}{166}{subsection.9.5}\protected@file@percent }
\citation{WillshawVonDerMalsburg1976}
\citation{Kohonen1982}
\citation{Kohonen2001}
\@writefile{toc}{\contentsline {paragraph}{Historical Context}{167}{subsection.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Basic Architecture}{167}{subsection.9.5}\protected@file@percent }
\citation{Kohonen2001}
\@writefile{toc}{\contentsline {paragraph}{Key Concept: Topographic Mapping}{168}{subsection.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Conceptual Description of SOM Operation}{168}{subsection.9.6}\protected@file@percent }
\newlabel{eq:bmu}{{9.3}{168}{Conceptual Description of SOM Operation}{equation.9.3}{}}
\newlabel{eq:bmu@cref}{{[equation][3][9]9.3}{[1][168][]168}}
\newlabel{eq:som_update}{{9.4}{169}{Conceptual Description of SOM Operation}{equation.9.4}{}}
\newlabel{eq:som_update@cref}{{[equation][4][9]9.4}{[1][169][]169}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Mathematical Formulation of SOM}{169}{subsection.9.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Best Matching Unit (BMU)}{169}{subsection.9.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neighborhood Function}{169}{subsection.9.7}\protected@file@percent }
\newlabel{eq:gaussian_neighborhood_short}{{9.5}{169}{Neighborhood Function}{equation.9.5}{}}
\newlabel{eq:gaussian_neighborhood_short@cref}{{[equation][5][9]9.5}{[1][169][]169}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Schematic: Gaussian neighborhood weights in SOM training. Early iterations use a broad kernel so many neighbors adapt; later iterations shrink the neighborhood width sigma(t) so only units near the BMU update.\relax }}{170}{figure.caption.39}\protected@file@percent }
\newlabel{fig:lec5-gaussian-neighborhood}{{36}{170}{Schematic: Gaussian neighborhood weights in SOM training. Early iterations use a broad kernel so many neighbors adapt; later iterations shrink the neighborhood width sigma(t) so only units near the BMU update.\relax }{figure.caption.39}{}}
\newlabel{fig:lec5-gaussian-neighborhood@cref}{{[figure][36][]36}{[1][169][]170}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.8}Kohonen Self-Organizing Maps (SOMs): Network Architecture and Operation}{170}{subsection.9.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Network Structure}{170}{subsection.9.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mapping and Competition}{171}{subsection.9.8}\protected@file@percent }
\newlabel{eq:winner_neuron}{{9.6}{171}{Mapping and Competition}{equation.9.6}{}}
\newlabel{eq:winner_neuron@cref}{{[equation][6][9]9.6}{[1][171][]171}}
\@writefile{toc}{\contentsline {paragraph}{Weight Update Rule}{171}{equation.9.6}\protected@file@percent }
\newlabel{eq:som_weight_update}{{9.7}{171}{Weight Update Rule}{equation.9.7}{}}
\newlabel{eq:som_weight_update@cref}{{[equation][7][9]9.7}{[1][171][]171}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.9}Example: SOM with a \(3 \times 3\) Output Map and 4-Dimensional Input}{171}{subsection.9.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feedforward Computation}{172}{subsection.9.9}\protected@file@percent }
\newlabel{eq:neuron_activation}{{9.8}{172}{Feedforward Computation}{equation.9.8}{}}
\newlabel{eq:neuron_activation@cref}{{[equation][8][9]9.8}{[1][172][]172}}
\newlabel{eq:neuron_distance}{{9.9}{172}{Feedforward Computation}{equation.9.9}{}}
\newlabel{eq:neuron_distance@cref}{{[equation][9][9]9.9}{[1][172][]172}}
\@writefile{toc}{\contentsline {paragraph}{Weight Initialization and Update}{172}{equation.9.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustration}{172}{equation.9.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.10}Key Properties of Kohonen SOMs}{173}{subsection.9.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.11}Winner-Takes-All Learning and Weight Update Rules}{173}{subsection.9.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminant Function and Similarity Measures}{173}{subsection.9.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weight Update Rule}{174}{subsection.9.11}\protected@file@percent }
\newlabel{eq:weight_update_general}{{9.10}{174}{Weight Update Rule}{equation.9.10}{}}
\newlabel{eq:weight_update_general@cref}{{[equation][10][9]9.10}{[1][174][]174}}
\newlabel{eq:weight_update_delta}{{9.11}{174}{Weight Update Rule}{equation.9.11}{}}
\newlabel{eq:weight_update_delta@cref}{{[equation][11][9]9.11}{[1][174][]174}}
\@writefile{toc}{\contentsline {paragraph}{Learning Rate Schedule}{174}{equation.9.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of the Competitive Learning Algorithm}{175}{equation.9.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.12}Numerical Example of Competitive Learning}{175}{subsection.9.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initial Weights}{175}{subsection.9.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.13}Winner-Takes-All Learning Recap}{176}{subsection.9.13}\protected@file@percent }
\newlabel{eq:wta_weight_update}{{9.13}{176}{Winner-Takes-All Learning Recap}{equation.9.13}{}}
\newlabel{eq:wta_weight_update@cref}{{[equation][13][9]9.13}{[1][176][]176}}
\@writefile{toc}{\contentsline {paragraph}{Practical considerations}{176}{equation.9.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.14}Regularization and Monitoring During SOM Training}{176}{subsection.9.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Schematic: Bias--variance trade-off when sweeping SOM capacity (number of units or kernel width). The optimum appears near the knee where bias and variance intersect.\relax }}{177}{figure.caption.40}\protected@file@percent }
\newlabel{fig:lec5-bias-variance}{{37}{177}{Schematic: Bias--variance trade-off when sweeping SOM capacity (number of units or kernel width). The optimum appears near the knee where bias and variance intersect.\relax }{figure.caption.40}{}}
\newlabel{fig:lec5-bias-variance@cref}{{[figure][37][]37}{[1][177][]177}}
\@writefile{toc}{\contentsline {paragraph}{Bias--variance view.}{177}{subsection.9.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss-landscape smoothing.}{177}{figure.caption.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantization vs. information preservation.}{177}{figure.caption.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Schematic: Regularization smooths the loss surface. Coupling neighboring prototypes (right) yields wider, flatter basins than the jagged unregularized landscape (left).\relax }}{178}{figure.caption.41}\protected@file@percent }
\newlabel{fig:lec5-regularization}{{38}{178}{Schematic: Regularization smooths the loss surface. Coupling neighboring prototypes (right) yields wider, flatter basins than the jagged unregularized landscape (left).\relax }{figure.caption.41}{}}
\newlabel{fig:lec5-regularization@cref}{{[figure][38][]38}{[1][177][]178}}
\@writefile{toc}{\contentsline {paragraph}{Quantization vs. topographic error.}{178}{figure.caption.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Schematic: Quantization error combined with an entropy-style regularizer (modern SOM variant; for example, a negative sum of p log p over unit usage). Valleys arise when prototypes cover the space evenly; ridges highlight collapse or poor topological preservation.\relax }}{179}{figure.caption.42}\protected@file@percent }
\newlabel{fig:lec5-crossentropy}{{39}{179}{Schematic: Quantization error combined with an entropy-style regularizer (modern SOM variant; for example, a negative sum of p log p over unit usage). Valleys arise when prototypes cover the space evenly; ridges highlight collapse or poor topological preservation.\relax }{figure.caption.42}{}}
\newlabel{fig:lec5-crossentropy@cref}{{[figure][39][]39}{[1][177][]179}}
\@writefile{toc}{\contentsline {paragraph}{Stopping criteria.}{179}{figure.caption.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Schematic: Validation curves used to identify an early\hyp  {}stopping knee. When both quantization and topographic errors flatten (shaded band), further training risks map drift.\relax }}{180}{figure.caption.43}\protected@file@percent }
\newlabel{fig:lec5-early-stopping}{{40}{180}{Schematic: Validation curves used to identify an early\hyp {}stopping knee. When both quantization and topographic errors flatten (shaded band), further training risks map drift.\relax }{figure.caption.43}{}}
\newlabel{fig:lec5-early-stopping@cref}{{[figure][40][]40}{[1][179][]180}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.15}Limitations of Winner-Takes-All and Motivation for Cooperation}{180}{subsection.9.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Schematic: Voronoi-like regions induced by SOM prototypes (left) and the corresponding softmax confidence after shrinking the neighborhood kernel (right). Softer updates blur the decision frontiers and reduce jagged mappings between adjacent neurons.\relax }}{181}{figure.caption.44}\protected@file@percent }
\newlabel{fig:lec5-softmax-regions}{{41}{181}{Schematic: Voronoi-like regions induced by SOM prototypes (left) and the corresponding softmax confidence after shrinking the neighborhood kernel (right). Softer updates blur the decision frontiers and reduce jagged mappings between adjacent neurons.\relax }{figure.caption.44}{}}
\newlabel{fig:lec5-softmax-regions@cref}{{[figure][41][]41}{[1][181][]181}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.16}Cooperation in Competitive Learning}{181}{subsection.9.16}\protected@file@percent }
\newlabel{sec:som_neighborhood}{{9.16}{181}{Cooperation in Competitive Learning}{subsection.9.16}{}}
\newlabel{sec:som_neighborhood@cref}{{[subsection][16][9]9.16}{[1][181][]181}}
\@writefile{toc}{\contentsline {paragraph}{Neighborhood Concept}{181}{subsection.9.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weight Update with Neighborhood Cooperation}{181}{subsection.9.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces Schematic: Left: a 5-by-5 SOM lattice with best matching unit (blue) and neighbors within the Gaussian-kernel radius (green). Right: a toy U-Matrix (colormap chosen to remain interpretable in grayscale) showing average distances between neighboring codebook vectors; higher distances indicate likely cluster boundaries.\relax }}{182}{figure.caption.45}\protected@file@percent }
\newlabel{fig:lec5-som-lattice-umatrix}{{42}{182}{Schematic: Left: a 5-by-5 SOM lattice with best matching unit (blue) and neighbors within the Gaussian-kernel radius (green). Right: a toy U-Matrix (colormap chosen to remain interpretable in grayscale) showing average distances between neighboring codebook vectors; higher distances indicate likely cluster boundaries.\relax }{figure.caption.45}{}}
\newlabel{fig:lec5-som-lattice-umatrix@cref}{{[figure][42][]42}{[1][182][]182}}
\newlabel{eq:coop_weight_update}{{9.14}{182}{Weight Update with Neighborhood Cooperation}{equation.9.14}{}}
\newlabel{eq:coop_weight_update@cref}{{[equation][14][9]9.14}{[1][182][]182}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian Neighborhood Function}{182}{equation.9.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Schematic: Component planes for three features on a trained SOM (toy data). Each plane maps one feature's value across the map; aligned bright/dark regions across planes reveal correlated features, complementing the U-Matrix in \Cref  {fig:lec5-som-lattice-umatrix}. Interpret brightness comparatively within a plane rather than as an absolute calibrated scale.\relax }}{183}{figure.caption.46}\protected@file@percent }
\newlabel{fig:lec5-som-component-planes}{{43}{183}{Schematic: Component planes for three features on a trained SOM (toy data). Each plane maps one feature's value across the map; aligned bright/dark regions across planes reveal correlated features, complementing the U-Matrix in \Cref {fig:lec5-som-lattice-umatrix}. Interpret brightness comparatively within a plane rather than as an absolute calibrated scale.\relax }{figure.caption.46}{}}
\newlabel{fig:lec5-som-component-planes@cref}{{[figure][43][]43}{[1][182][]183}}
\newlabel{eq:gaussian_neighborhood}{{9.15}{183}{Gaussian Neighborhood Function}{equation.9.15}{}}
\newlabel{eq:gaussian_neighborhood@cref}{{[equation][15][9]9.15}{[1][182][]183}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{183}{equation.9.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.17}Example: Neighborhood Update Illustration}{183}{subsection.9.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Schematic: SOM lattice with the best-matching unit (BMU) highlighted in blue and a dashed neighborhood radius indicating which prototype vectors receive cooperative updates.\relax }}{184}{figure.caption.47}\protected@file@percent }
\newlabel{fig:som_neighborhood}{{44}{184}{Schematic: SOM lattice with the best-matching unit (BMU) highlighted in blue and a dashed neighborhood radius indicating which prototype vectors receive cooperative updates.\relax }{figure.caption.47}{}}
\newlabel{fig:som_neighborhood@cref}{{[figure][44][]44}{[1][183][]184}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.18}Summary of Cooperative Competitive Learning Algorithm}{184}{subsection.9.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.19}Wrapping Up the Kohonen Self-Organizing Map (SOM) Derivations}{184}{subsection.9.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neighborhood Function and Its Role}{185}{equation.9.16}\protected@file@percent }
\newlabel{eq:neighborhood_function}{{9.17}{185}{Neighborhood Function and Its Role}{equation.9.17}{}}
\newlabel{eq:neighborhood_function@cref}{{[equation][17][9]9.17}{[1][185][]185}}
\@writefile{toc}{\contentsline {paragraph}{Time-Dependent Parameters}{185}{equation.9.17}\protected@file@percent }
\newlabel{eq:decay_parameters}{{9.19}{185}{Time-Dependent Parameters}{equation.9.19}{}}
\newlabel{eq:decay_parameters@cref}{{[equation][19][9]9.19}{[1][185][]185}}
\@writefile{toc}{\contentsline {paragraph}{Summary of the Six Learning Steps}{185}{equation.9.19}\protected@file@percent }
\newlabel{sec:som_training_steps}{{9.19}{185}{Summary of the Six Learning Steps}{equation.9.19}{}}
\newlabel{sec:som_training_steps@cref}{{[subsection][19][9]9.19}{[1][185][]185}}
\citation{MacQueen1967}
\citation{ErwinObermayerSchulten1992,CottrellFort1986}
\@writefile{toc}{\contentsline {paragraph}{Stages vs. Steps}{187}{Item.86}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.20}Applications of Kohonen Self-Organizing Maps}{187}{subsection.9.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to k-means and modern variants}{187}{subsection.9.20}\protected@file@percent }
\citation{MartinetzBerkovichSchulten1993,Fritzke1994GrowingNeuralGas}
\@writefile{toc}{\contentsline {paragraph}{Complexity and out-of-sample mapping.}{188}{subsection.9.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Theory link to other chapters.}{188}{subsection.9.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quality measures and magnification.}{188}{subsection.9.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{189}{subsection.9.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{189}{subsection.9.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Hopfield Networks: Introduction and Context}{190}{section.10}\protected@file@percent }
\newlabel{chap:hopfield}{{10}{190}{Hopfield Networks: Introduction and Context}{section.10}{}}
\newlabel{chap:hopfield@cref}{{[section][10][]10}{[1][189][]190}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}From Feedforward to Recurrent Neural Networks}{190}{subsection.10.1}\protected@file@percent }
\citation{Hopfield1982}
\@writefile{toc}{\contentsline {paragraph}{Challenges with General Recurrent Networks}{191}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Hopfield's Breakthrough (1982)}{191}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Network Architecture and Dynamics}{192}{subsection.10.3}\protected@file@percent }
\newlabel{eq:hopfield_update_rule}{{10.4}{192}{Network Architecture and Dynamics}{equation.10.4}{}}
\newlabel{eq:hopfield_update_rule@cref}{{[equation][4][10]10.4}{[1][192][]192}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{192}{equation.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Encoding conventions}{193}{subsection.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Energy Function and Stability}{193}{subsection.10.5}\protected@file@percent }
\newlabel{eq:hopfield_energy_general}{{10.5}{193}{Energy Function and Stability}{equation.10.5}{}}
\newlabel{eq:hopfield_energy_general@cref}{{[equation][5][10]10.5}{[1][193][]193}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Hopfield Network States and Energy Function}{194}{subsection.10.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Energy function for \(\pm 1\) states:}{194}{subsection.10.6}\protected@file@percent }
\newlabel{eq:energy_bipolar}{{10.6}{194}{Energy function for \(\pm 1\) states:}{equation.10.6}{}}
\newlabel{eq:energy_bipolar@cref}{{[equation][6][10]10.6}{[1][194][]194}}
\newlabel{eq:energy_bipolar_threshold}{{10.7}{194}{Energy function for \(\pm 1\) states:}{equation.10.7}{}}
\newlabel{eq:energy_bipolar_threshold@cref}{{[equation][7][10]10.7}{[1][194][]194}}
\@writefile{toc}{\contentsline {paragraph}{Energy function for \(\{0,1\}\) states:}{194}{equation.10.7}\protected@file@percent }
\newlabel{eq:energy_binary}{{10.8}{194}{Energy function for \(\{0,1\}\) states:}{equation.10.8}{}}
\newlabel{eq:energy_binary@cref}{{[equation][8][10]10.8}{[1][194][]194}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7}Energy Minimization and Stable States}{195}{subsection.10.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{State update dynamics:}{195}{subsection.10.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.8}Example: Energy Calculation and State Updates}{196}{subsection.10.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{State update attempts:}{196}{equation.10.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Schematic: Single-neuron flips from (1, 1, -1); all raise the energy, so the state is a local minimum.\relax }}{196}{table.caption.48}\protected@file@percent }
\newlabel{tab:hopfield-deltaE}{{4}{196}{Schematic: Single-neuron flips from (1, 1, -1); all raise the energy, so the state is a local minimum.\relax }{table.caption.48}{}}
\newlabel{tab:hopfield-deltaE@cref}{{[table][4][]4}{[1][196][]196}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces Schematic: Hopfield energy decreases monotonically under asynchronous updates. Starting from a noisy probe state s(0), successive single-neuron flips move downhill until the stored memory s(2) is recovered.\relax }}{197}{figure.caption.49}\protected@file@percent }
\newlabel{fig:hopfield-energy-descent}{{45}{197}{Schematic: Hopfield energy decreases monotonically under asynchronous updates. Starting from a noisy probe state s(0), successive single-neuron flips move downhill until the stored memory s(2) is recovered.\relax }{figure.caption.49}{}}
\newlabel{fig:hopfield-energy-descent@cref}{{[figure][45][]45}{[1][197][]197}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.9}Energy Function and Convergence of Hopfield Networks}{197}{subsection.10.9}\protected@file@percent }
\newlabel{eq:energy_function}{{10.11}{197}{Energy Function and Convergence of Hopfield Networks}{equation.10.11}{}}
\newlabel{eq:energy_function@cref}{{[equation][11][10]10.11}{[1][197][]197}}
\@writefile{toc}{\contentsline {paragraph}{Goal:}{197}{equation.10.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.9.1}Energy Change Upon Updating a Single Neuron}{197}{subsubsection.10.9.1}\protected@file@percent }
\citation{Krotov2016,Krotov2020,Ramsauer2021}
\newlabel{eq:deltaE}{{10.15}{198}{Energy Change Upon Updating a Single Neuron}{equation.10.15}{}}
\newlabel{eq:deltaE@cref}{{[equation][15][10]10.15}{[1][198][]198}}
\newlabel{eq:local_field}{{10.16}{198}{Energy Change Upon Updating a Single Neuron}{equation.10.16}{}}
\newlabel{eq:local_field@cref}{{[equation][16][10]10.16}{[1][198][]198}}
\@writefile{toc}{\contentsline {paragraph}{Numeric check (single flip).}{198}{equation.10.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Continuous Hopfield / dense associative memory.}{199}{equation.10.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.9.2}Update Rule and Energy Decrease}{199}{subsubsection.10.9.2}\protected@file@percent }
\newlabel{eq:update_rule}{{10.17}{199}{Update Rule and Energy Decrease}{equation.10.17}{}}
\newlabel{eq:update_rule@cref}{{[equation][17][10]10.17}{[1][199][]199}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.10}Asynchronous vs. Synchronous Updates in Hopfield Networks}{200}{subsection.10.10}\protected@file@percent }
\newlabel{sec:hopfield_async_vs_sync}{{10.10}{200}{Asynchronous vs. Synchronous Updates in Hopfield Networks}{subsection.10.10}{}}
\newlabel{sec:hopfield_async_vs_sync@cref}{{[subsection][10][10]10.10}{[1][200][]200}}
\@writefile{toc}{\contentsline {paragraph}{Why asynchronous updates?}{200}{subsection.10.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion:}{200}{subsection.10.10}\protected@file@percent }
\citation{McEliece1987}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.11}Storage Capacity of Hopfield Networks}{201}{subsection.10.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classical result:}{201}{subsection.10.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inefficiency:}{201}{subsection.10.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stochastic updates (bridge to Boltzmann machines).}{201}{subsection.10.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.12}Improving Storage Capacity via Weight Updates}{201}{subsection.10.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Idea:}{201}{subsection.10.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hebbian learning rule:}{202}{subsection.10.12}\protected@file@percent }
\newlabel{eq:hebbian_weights}{{10.18}{202}{Hebbian learning rule:}{equation.10.18}{}}
\newlabel{eq:hebbian_weights@cref}{{[equation][18][10]10.18}{[1][201][]202}}
\@writefile{toc}{\contentsline {paragraph}{Properties:}{202}{equation.10.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect on capacity:}{202}{equation.10.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.13}Example: Weight Calculation for a Single Pattern}{202}{subsection.10.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 1: Compute outer product}{202}{subsection.10.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Remove diagonal terms}{202}{subsection.10.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.14}Finalizing the Hopfield Network Derivation and Discussion}{203}{subsection.10.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Energy Function and Convergence}{203}{equation.10.19}\protected@file@percent }
\newlabel{eq:hopfield_update}{{10.20}{203}{Energy Function and Convergence}{equation.10.20}{}}
\newlabel{eq:hopfield_update@cref}{{[equation][20][10]10.20}{[1][203][]203}}
\newlabel{eq:hopfield_energy}{{10.21}{203}{Energy Function and Convergence}{equation.10.21}{}}
\newlabel{eq:hopfield_energy@cref}{{[equation][21][10]10.21}{[1][203][]203}}
\@writefile{toc}{\contentsline {paragraph}{Memory Retrieval and Basins of Attraction}{203}{equation.10.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations: Capacity and Classification}{204}{equation.10.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Memory Recovery}{204}{equation.10.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spurious attractors}{205}{equation.10.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Historical and Practical Significance}{205}{equation.10.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Connections to other chapters.}{206}{equation.10.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{206}{equation.10.21}\protected@file@percent }
\citation{Hopfield1982}
\citation{AmitGutfreundSompolinsky1985}
\@writefile{toc}{\contentsline {paragraph}{References.}{207}{equation.10.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Introduction to Deep Learning and Neural Networks}{207}{section.11}\protected@file@percent }
\newlabel{chap:cnn}{{11}{207}{Introduction to Deep Learning and Neural Networks}{section.11}{}}
\newlabel{chap:cnn@cref}{{[section][11][]11}{[1][207][]207}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Historical Context and Motivation}{208}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Overview of Neural Network Architectures}{209}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.1}Feedforward Neural Networks (Multi-Layer Perceptrons)}{209}{subsubsection.11.2.1}\protected@file@percent }
\newlabel{eq:preactivation}{{11.1}{209}{Feedforward Neural Networks (Multi-Layer Perceptrons)}{equation.11.1}{}}
\newlabel{eq:preactivation@cref}{{[equation][1][11]11.1}{[1][209][]209}}
\@writefile{toc}{\contentsline {paragraph}{Fully Connected Layers and Feature Transformation}{209}{equation.11.1}\protected@file@percent }
\citation{Cybenko1989}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Why Shallow Networks Are Insufficient}{210}{subsection.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}Training Neural Networks: Gradient-Based Optimization}{210}{subsection.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation and Gradient Computation}{210}{equation.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenges in Deep Networks}{211}{equation.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5}Deep Network Optimization Challenges}{211}{subsection.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6}Vanishing and Exploding Gradients in Deep Networks}{211}{subsection.11.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mathematical intuition}{211}{subsection.11.6}\protected@file@percent }
\newlabel{eq:gradient_chain}{{11.3}{211}{Mathematical intuition}{equation.11.3}{}}
\newlabel{eq:gradient_chain@cref}{{[equation][3][11]11.3}{[1][211][]211}}
\@writefile{toc}{\contentsline {paragraph}{Consequences}{211}{equation.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Activation function derivatives}{212}{equation.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notation note.}{212}{equation.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7}Strategies to Mitigate Vanishing and Exploding Gradients}{212}{subsection.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weight initialization}{212}{subsection.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Choice of activation function}{212}{subsection.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Batch normalization}{213}{subsection.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient clipping}{213}{subsection.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8}Limitations of Traditional Feedforward Neural Networks}{213}{subsection.11.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Requirement for large datasets}{213}{subsection.11.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High-dimensional inputs and flattening}{213}{subsection.11.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implications}{214}{subsection.11.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9}Challenges in Training Large Fully Connected Networks}{214}{subsection.11.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter Explosion}{214}{subsection.11.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Requirements}{214}{equation.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational and Storage Constraints}{215}{equation.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overfitting Risk}{215}{equation.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10}Sparse connectivity and parameter sharing}{215}{subsection.11.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11}Convolution and pooling mechanics}{216}{subsection.11.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.12}From feature maps to classifiers}{216}{subsection.11.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.13}Historical Context and the 2012 Breakthrough}{216}{subsection.11.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVM geometry refresher.}{216}{subsection.11.13}\protected@file@percent }
\citation{Kingma2015}
\citation{Loshchilov2019}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Dropout effect on training/validation curves (validation flattening)}}{217}{figure.caption.50}\protected@file@percent }
\newlabel{fig:lec6-dropout}{{46}{217}{Dropout effect on training/validation curves (validation flattening)}{figure.caption.50}{}}
\newlabel{fig:lec6-dropout@cref}{{[figure][46][]46}{[1][217][]217}}
\@writefile{toc}{\contentsline {paragraph}{Stack depth versus receptive field.}{217}{subsection.11.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Batch normalization.}{217}{figure.caption.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adaptive optimizers.}{217}{figure.caption.51}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces Schematic: Batch normalization transforms per-channel activations toward zero mean and unit variance prior to the learned affine re-scaling, stabilizing training.\relax }}{218}{figure.caption.51}\protected@file@percent }
\newlabel{fig:lec6-bn}{{47}{218}{Schematic: Batch normalization transforms per-channel activations toward zero mean and unit variance prior to the learned affine re-scaling, stabilizing training.\relax }{figure.caption.51}{}}
\newlabel{fig:lec6-bn@cref}{{[figure][47][]47}{[1][217][]218}}
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces Schematic: Representative training curves for SGD with momentum versus Adam on the same CNN.\relax }}{219}{figure.caption.52}\protected@file@percent }
\newlabel{fig:lec6-optimizers}{{48}{219}{Schematic: Representative training curves for SGD with momentum versus Adam on the same CNN.\relax }{figure.caption.52}{}}
\newlabel{fig:lec6-optimizers@cref}{{[figure][48][]48}{[1][217][]219}}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{220}{figure.caption.52}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{220}{figure.caption.52}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Introduction to Recurrent Neural Networks}{221}{section.12}\protected@file@percent }
\newlabel{chap:rnn}{{12}{221}{Introduction to Recurrent Neural Networks}{section.12}{}}
\newlabel{chap:rnn@cref}{{[section][12][]12}{[1][220][]221}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Quick recap: padding in CNNs}{221}{subsection.12.1}\protected@file@percent }
\citation{Elman1990,Bengio1994}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Autoencoders and latent representations}{222}{subsection.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Code--math dictionary.}{223}{subsection.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Motivation for Recurrent Neural Networks}{224}{subsection.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}Key Idea: State and Memory in RNNs}{224}{subsection.12.4}\protected@file@percent }
\newlabel{eq:rnn_state_update}{{12.1}{224}{Key Idea: State and Memory in RNNs}{equation.12.1}{}}
\newlabel{eq:rnn_state_update@cref}{{[equation][1][12]12.1}{[1][224][]224}}
\newlabel{eq:rnn_output}{{12.2}{225}{Key Idea: State and Memory in RNNs}{equation.12.2}{}}
\newlabel{eq:rnn_output@cref}{{[equation][2][12]12.2}{[1][224][]225}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{225}{equation.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5}Comparison with Feedforward Networks}{225}{subsection.12.5}\protected@file@percent }
\newlabel{eq:ffn_output}{{12.3}{225}{Comparison with Feedforward Networks}{equation.12.3}{}}
\newlabel{eq:ffn_output@cref}{{[equation][3][12]12.3}{[1][225][]225}}
\@writefile{toc}{\contentsline {paragraph}{Summary:}{225}{equation.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6}Outline of this chapter}{225}{subsection.12.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {49}{\ignorespaces Schematic: Decision boundaries for logistic regression (left) versus a shallow MLP (right). Linear models carve a single hyperplane, whereas hidden units can warp the boundary to follow non-convex manifolds such as the moons dataset.\relax }}{226}{figure.caption.53}\protected@file@percent }
\newlabel{fig:lec7-boundaries}{{49}{226}{Schematic: Decision boundaries for logistic regression (left) versus a shallow MLP (right). Linear models carve a single hyperplane, whereas hidden units can warp the boundary to follow non-convex manifolds such as the moons dataset.\relax }{figure.caption.53}{}}
\newlabel{fig:lec7-boundaries@cref}{{[figure][49][]49}{[1][226][]226}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.7}Recap: Feedforward Building Blocks}{226}{subsection.12.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {50}{\ignorespaces Schematic: Binary cross-entropy geometry (left), effect of learning-rate schedules on loss (middle), and the typical training/validation divergence that motivates early stopping (right).\relax }}{227}{figure.caption.54}\protected@file@percent }
\newlabel{fig:lec7-loss-hyperparams}{{50}{227}{Schematic: Binary cross-entropy geometry (left), effect of learning-rate schedules on loss (middle), and the typical training/validation divergence that motivates early stopping (right).\relax }{figure.caption.54}{}}
\newlabel{fig:lec7-loss-hyperparams@cref}{{[figure][50][]50}{[1][227][]227}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.8}Limitations of Feedforward Neural Networks for Sequential Data}{227}{subsection.12.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Language Modeling}{228}{subsection.12.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Predictive Text and Autocomplete}{228}{subsection.12.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Stock Price Prediction}{228}{subsection.12.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenges with Variable-Length Inputs}{228}{subsection.12.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.9}Recurrent Neural Networks (RNNs)}{228}{subsection.12.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Idea}{229}{subsection.12.9}\protected@file@percent }
\newlabel{eq:rnn_hidden}{{12.4}{229}{Key Idea}{equation.12.4}{}}
\newlabel{eq:rnn_hidden@cref}{{[equation][4][12]12.4}{[1][229][]229}}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Feedforward Networks}{229}{equation.12.5}\protected@file@percent }
\citation{Ba2016}
\citation{Hopfield1982}
\@writefile{toc}{\contentsline {paragraph}{Historical Note: Hopfield Networks}{230}{equation.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.10}Input--output configurations and mathematical formulation}{230}{subsection.12.10}\protected@file@percent }
\citation{Rumelhart1986}
\newlabel{eq:rnn_hidden_state}{{12.7}{231}{Input--output configurations and mathematical formulation}{equation.12.7}{}}
\newlabel{eq:rnn_hidden_state@cref}{{[equation][7][12]12.7}{[1][231][]231}}
\newlabel{eq:rnn_output_state}{{12.8}{231}{Input--output configurations and mathematical formulation}{equation.12.8}{}}
\newlabel{eq:rnn_output_state@cref}{{[equation][8][12]12.8}{[1][231][]231}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.11}Recurrent Neural Networks: Historical Context and Motivation}{231}{subsection.12.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.12}The 1986 Breakthrough: Backpropagation and Trainable Multi-Layer Networks}{232}{subsection.12.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.13}State Dynamics in Recurrent Neural Networks}{232}{subsection.12.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{232}{equation.12.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {51}{\ignorespaces Schematic: Unrolling an RNN reveals repeated application of the same parameters across time steps. This view motivates backpropagation through time (BPTT), which accumulates gradients through every copy before updating the shared weights.\relax }}{233}{figure.caption.55}\protected@file@percent }
\newlabel{fig:lec7-rnn-unrolled}{{51}{233}{Schematic: Unrolling an RNN reveals repeated application of the same parameters across time steps. This view motivates backpropagation through time (BPTT), which accumulates gradients through every copy before updating the shared weights.\relax }{figure.caption.55}{}}
\newlabel{fig:lec7-rnn-unrolled@cref}{{[figure][51][]51}{[1][233][]233}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.14}Unfolding the Recurrent Neural Network}{233}{subsection.12.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Process}{233}{subsection.12.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Significance}{233}{subsection.12.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.15}Mathematical Formulation of a Simple RNN Cell}{233}{subsection.12.15}\protected@file@percent }
\newlabel{eq:simple_rnn_hidden}{{12.11}{233}{Mathematical Formulation of a Simple RNN Cell}{equation.12.11}{}}
\newlabel{eq:simple_rnn_hidden@cref}{{[equation][11][12]12.11}{[1][233][]233}}
\newlabel{eq:simple_rnn_output}{{12.12}{233}{Mathematical Formulation of a Simple RNN Cell}{equation.12.12}{}}
\newlabel{eq:simple_rnn_output@cref}{{[equation][12][12]12.12}{[1][233][]233}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.16}Recurrent Neural Network (RNN) Unfolding and Parameter Sharing}{234}{subsection.12.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unfolding the RNN}{234}{subsection.12.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter Sharing}{234}{subsection.12.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.17}Mathematical Formulation of the RNN}{234}{subsection.12.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{235}{equation.12.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reusability of the Hidden State}{235}{equation.12.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.18}Generalized Notation}{235}{subsection.12.18}\protected@file@percent }
\newlabel{eq:rnn_compact}{{12.16}{236}{Generalized Notation}{equation.12.16}{}}
\newlabel{eq:rnn_compact@cref}{{[equation][16][12]12.16}{[1][236][]236}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.19}Recurrent Neural Network (RNN) Architectures and Loss Computation}{236}{subsection.12.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward and Backward Passes in RNNs}{236}{equation.12.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {52}{\ignorespaces Schematic: Backpropagation through time (BPTT): unrolled forward pass (black) and backward gradients (pink) through time.\relax }}{237}{figure.caption.56}\protected@file@percent }
\newlabel{fig:lec7-bptt}{{52}{237}{Schematic: Backpropagation through time (BPTT): unrolled forward pass (black) and backward gradients (pink) through time.\relax }{figure.caption.56}{}}
\newlabel{fig:lec7-bptt@cref}{{[figure][52][]52}{[1][237][]237}}
\@writefile{toc}{\contentsline {paragraph}{Vanishing and Exploding Gradients}{237}{figure.caption.56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {53}{\ignorespaces Schematic: Vanishing (blue) versus exploding (orange) gradients on a log scale. The gray strip highlights the stability band; the inset reminds readers that repeated Jacobian products either shrink gradients (thin blue arrows) or amplify them (thick orange arrows).\relax }}{238}{figure.caption.57}\protected@file@percent }
\newlabel{fig:lec7-vanishing}{{53}{238}{Schematic: Vanishing (blue) versus exploding (orange) gradients on a log scale. The gray strip highlights the stability band; the inset reminds readers that repeated Jacobian products either shrink gradients (thin blue arrows) or amplify them (thick orange arrows).\relax }{figure.caption.57}{}}
\newlabel{fig:lec7-vanishing@cref}{{[figure][53][]53}{[1][237][]238}}
\@writefile{toc}{\contentsline {paragraph}{Parameter Updates}{238}{figure.caption.57}\protected@file@percent }
\citation{Gal2016,Semeniuta2016}
\citation{Krueger2016}
\citation{Bengio2015}
\@writefile{lof}{\contentsline {figure}{\numberline {54}{\ignorespaces Schematic: Gradient norms (left) explode without clipping (orange) but remain bounded when the global norm is clipped at tau (green). Training loss (right) stabilizes as a result.\relax }}{239}{figure.caption.58}\protected@file@percent }
\newlabel{fig:lec7-clipping}{{54}{239}{Schematic: Gradient norms (left) explode without clipping (orange) but remain bounded when the global norm is clipped at tau (green). Training loss (right) stabilizes as a result.\relax }{figure.caption.58}{}}
\newlabel{fig:lec7-clipping@cref}{{[figure][54][]54}{[1][239][]239}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.20}Stabilizing Recurrent Training}{239}{subsection.12.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient clipping.}{239}{subsection.12.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dropout in RNNs.}{239}{subsection.12.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Teacher forcing and scheduled sampling.}{239}{figure.caption.58}\protected@file@percent }
\citation{Hochreiter1997,Gers2000}
\citation{Cho2014}
\citation{Hochreiter1997,Gers2000}
\citation{Cho2014}
\@writefile{lof}{\contentsline {figure}{\numberline {55}{\ignorespaces Schematic: Teacher forcing vs.\ inference in a sequence-to-sequence decoder. Gold arrows show supervised targets; orange arrows highlight autoregressive feedback that motivates scheduled sampling.\relax }}{240}{figure.caption.59}\protected@file@percent }
\newlabel{fig:lec7-teacher-forcing}{{55}{240}{Schematic: Teacher forcing vs.\ inference in a sequence-to-sequence decoder. Gold arrows show supervised targets; orange arrows highlight autoregressive feedback that motivates scheduled sampling.\relax }{figure.caption.59}{}}
\newlabel{fig:lec7-teacher-forcing@cref}{{[figure][55][]55}{[1][240][]240}}
\@writefile{toc}{\contentsline {paragraph}{Gated cells.}{240}{figure.caption.59}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {56}{\ignorespaces Schematic: Long Short-Term Memory (LSTM) cell \citep  {Hochreiter1997,Gers2000}.\relax }}{241}{figure.caption.60}\protected@file@percent }
\newlabel{fig:lec7-lstm}{{56}{241}{Schematic: Long Short-Term Memory (LSTM) cell \citep {Hochreiter1997,Gers2000}.\relax }{figure.caption.60}{}}
\newlabel{fig:lec7-lstm@cref}{{[figure][56][]56}{[1][240][]241}}
\@writefile{lof}{\contentsline {figure}{\numberline {57}{\ignorespaces Schematic: Gated Recurrent Unit (GRU) cell \citep  {Cho2014}.\relax }}{241}{figure.caption.61}\protected@file@percent }
\newlabel{fig:lec7-gru}{{57}{241}{Schematic: Gated Recurrent Unit (GRU) cell \citep {Cho2014}.\relax }{figure.caption.61}{}}
\newlabel{fig:lec7-gru@cref}{{[figure][57][]57}{[1][240][]241}}
\@writefile{toc}{\contentsline {paragraph}{Attention mechanisms.}{242}{figure.caption.61}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {58}{\ignorespaces Schematic: Attention heatmap for a translation model. Rows are target tokens (decoder steps) and columns are source tokens (encoder positions). Each cell is an attention weight; the dot in each row marks the source position receiving the most attention.\relax }}{243}{figure.caption.62}\protected@file@percent }
\newlabel{fig:lec7-attention}{{58}{243}{Schematic: Attention heatmap for a translation model. Rows are target tokens (decoder steps) and columns are source tokens (encoder positions). Each cell is an attention weight; the dot in each row marks the source position receiving the most attention.\relax }{figure.caption.62}{}}
\newlabel{fig:lec7-attention@cref}{{[figure][58][]58}{[1][242][]243}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.21}RNN Input-Output Configurations}{243}{subsection.12.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.22}Representing Words for RNN Inputs}{244}{subsection.12.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vocabulary Size and Word Representation}{244}{subsection.12.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One-Hot Encoding}{244}{subsection.12.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of One-Hot Encoding}{245}{subsection.12.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.23}Example: Sentiment Analysis with RNNs}{245}{subsection.12.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.24}Limitations of One-Hot Encoding in Natural Language Processing}{245}{subsection.12.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{245}{subsection.12.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Document similarity:}{246}{subsection.12.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary:}{246}{subsection.12.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.25}Feature-Based Word Representations}{246}{subsection.12.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{246}{subsection.12.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notes:}{247}{subsection.12.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{247}{subsection.12.25}\protected@file@percent }
\citation{Mikolov2013}
\@writefile{toc}{\contentsline {paragraph}{Limitations:}{248}{subsection.12.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.26}Towards Distributed Word Representations}{248}{subsection.12.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key idea:}{248}{subsection.12.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Methods to obtain distributed representations}{248}{subsection.12.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.27}Semantic Relationships in Word Embeddings}{248}{subsection.12.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Subword tokenization and OOV handling.}{248}{subsection.12.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Gender and Royalty Analogies}{249}{subsection.12.27}\protected@file@percent }
\newlabel{eq:king-queen-analogy}{{12.20}{249}{Example: Gender and Royalty Analogies}{equation.12.20}{}}
\newlabel{eq:king-queen-analogy@cref}{{[equation][20][12]12.20}{[1][249][]249}}
\@writefile{toc}{\contentsline {paragraph}{Empirical Validation}{249}{equation.12.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geographical and Cultural Clustering}{249}{equation.12.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {59}{\ignorespaces Schematic: Toy 2D projection of word embeddings showing neighboring clusters (countries vs. capitals). Light hulls highlight clusters; arrows show that country-to-capital displacement vectors align, a visual check on analogy structure.\relax }}{250}{figure.caption.63}\protected@file@percent }
\newlabel{fig:lec7-embedding-clusters}{{59}{250}{Schematic: Toy 2D projection of word embeddings showing neighboring clusters (countries vs. capitals). Light hulls highlight clusters; arrows show that country-to-capital displacement vectors align, a visual check on analogy structure.\relax }{figure.caption.63}{}}
\newlabel{fig:lec7-embedding-clusters@cref}{{[figure][59][]59}{[1][250][]250}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.28}Feature-Based Representation vs. One-Hot Encoding}{251}{subsection.12.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One-Hot Encoding}{251}{subsection.12.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feature-Based Embeddings}{251}{subsection.12.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Context Window Convention}{251}{subsection.12.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.29}Open Questions: Feature Discovery and Representation}{252}{subsection.12.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Self-supervised learning of embeddings}{252}{Item.97}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{252}{Item.97}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.30}Wrapping Up the Derivations}{253}{subsection.12.30}\protected@file@percent }
\newlabel{eq:rnn_output_prob}{{12.23}{253}{Wrapping Up the Derivations}{equation.12.23}{}}
\newlabel{eq:rnn_output_prob@cref}{{[equation][23][12]12.23}{[1][253][]253}}
\@writefile{toc}{\contentsline {paragraph}{Training Objective}{254}{equation.12.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Self-supervised nature of language modeling}{254}{equation.12.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feature Representations}{254}{equation.12.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of the Modeling Pipeline}{255}{equation.12.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notation note.}{255}{Item.103}\protected@file@percent }
\citation{Press2016}
\citation{JurafskyMartin2023}
\citation{Goodfellow2016}
\citation{Mikolov2010}
\citation{LevyGoldberg2014}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{256}{Item.103}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{256}{Item.103}\protected@file@percent }
\citation{Vaswani2017}
\@writefile{toc}{\contentsline {section}{\numberline {13}Transformers: Attention-Based Sequence Modeling}{257}{section.13}\protected@file@percent }
\newlabel{chap:transformers}{{13}{257}{Transformers: Attention-Based Sequence Modeling}{section.13}{}}
\newlabel{chap:transformers@cref}{{[section][13][]13}{[1][256][]257}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Why transformers after RNNs?}{258}{subsection.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Scaled Dot-Product Attention}{258}{subsection.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Multi-Head Attention (MHA)}{259}{subsection.13.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {60}{\ignorespaces Schematic: Reference schematic for the Transformer. Left: scaled dot-product attention. Center: multi-head concatenation with an output projection. Right: pre-LN encoder block combining attention, FFN, and residual connections; a post-LN variant simply moves each LayerNorm after its residual add (dotted alternative, not shown).\relax }}{259}{figure.caption.64}\protected@file@percent }
\newlabel{fig:lec13_transformer_block}{{60}{259}{Schematic: Reference schematic for the Transformer. Left: scaled dot-product attention. Center: multi-head concatenation with an output projection. Right: pre-LN encoder block combining attention, FFN, and residual connections; a post-LN variant simply moves each LayerNorm after its residual add (dotted alternative, not shown).\relax }{figure.caption.64}{}}
\newlabel{fig:lec13_transformer_block@cref}{{[figure][60][]60}{[1][259][]259}}
\@writefile{toc}{\contentsline {paragraph}{Micro attention example (2 tokens, causal mask).}{259}{figure.caption.65}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {61}{\ignorespaces Schematic: Transformer micro-views. Left: positional encodings (sinusoidal/rotary) add order information. Center: KV cache stores past keys/values so decoding a new token reuses prior context. Right: LoRA inserts low-rank adapters (B A) on top of a frozen weight matrix W for parameter-efficient tuning.\relax }}{260}{figure.caption.65}\protected@file@percent }
\newlabel{fig:lec13_micro_figures}{{61}{260}{Schematic: Transformer micro-views. Left: positional encodings (sinusoidal/rotary) add order information. Center: KV cache stores past keys/values so decoding a new token reuses prior context. Right: LoRA inserts low-rank adapters (B A) on top of a frozen weight matrix W for parameter-efficient tuning.\relax }{figure.caption.65}{}}
\newlabel{fig:lec13_micro_figures@cref}{{[figure][61][]61}{[1][259][]260}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4}Positional Information}{260}{subsection.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5}Masks and Training Objectives}{261}{subsection.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.6}Encoder/Decoder Stacks and Stabilizers}{261}{subsection.13.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Code--math dictionary.}{262}{equation.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.7}Long Contexts and Efficient Attention}{262}{subsection.13.7}\protected@file@percent }
\citation{Su2021RoPE}
\citation{Press2022ALiBi}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8}Fine-Tuning and Parameter-Efficient Adaptation}{263}{subsection.13.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.9}Decoding and Evaluation}{263}{subsection.13.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to use which.}{263}{subsection.13.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.10}Alignment (Brief)}{263}{subsection.13.10}\protected@file@percent }
\citation{Dao2022FlashAttention}
\citation{Gu2023Mamba}
\citation{Shazeer2017MoE}
\citation{Hu2022LoRA,Dettmers2023QLoRA}
\@writefile{lof}{\contentsline {figure}{\numberline {62}{\ignorespaces Schematic: Attention masks visualized as heatmaps (queries on rows, keys on columns). Left: padding mask zeroes attention into padded positions of a shorter sequence in a packed batch. Right: causal mask enforces autoregressive flow by blocking attention to future tokens.\relax }}{264}{figure.caption.66}\protected@file@percent }
\newlabel{fig:lec13-masks}{{62}{264}{Schematic: Attention masks visualized as heatmaps (queries on rows, keys on columns). Left: padding mask zeroes attention into padded positions of a shorter sequence in a packed batch. Right: causal mask enforces autoregressive flow by blocking attention to future tokens.\relax }{figure.caption.66}{}}
\newlabel{fig:lec13-masks@cref}{{[figure][62][]62}{[1][263][]264}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.11}Advanced attention and efficiency notes (2024 snapshot)}{264}{subsection.13.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.12}RNNs vs. Transformers: When and Why}{265}{subsection.13.12}\protected@file@percent }
\citation{Dosovitskiy2021}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{266}{subsection.13.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{267}{subsection.13.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Neural Network Applications in Natural Language Processing}{267}{section.14}\protected@file@percent }
\newlabel{chap:nlp}{{14}{267}{Neural Network Applications in Natural Language Processing}{section.14}{}}
\newlabel{chap:nlp@cref}{{[section][14][]14}{[1][267][]267}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Context and Motivation}{267}{subsection.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Problem Statement}{268}{subsection.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}Key Insight: Distributional Hypothesis}{268}{subsection.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{268}{subsection.14.3}\protected@file@percent }
\citation{Mikolov2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}Contextual Meaning and Feature Extraction}{269}{subsection.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5}Word2Vec: Two Architectures}{269}{subsection.14.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.1}Continuous Bag of Words (CBOW)}{270}{subsubsection.14.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{270}{subsubsection.14.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.5.2}skip-gram}{270}{subsubsection.14.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.6}Mathematical Formulation of CBOW}{271}{subsection.14.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.7}Neural Network Architecture for Word Embeddings}{271}{subsection.14.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Input Representation}{271}{subsection.14.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Network Structure}{271}{subsection.14.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass}{271}{subsection.14.7}\protected@file@percent }
\newlabel{eq:hidden_layer}{{14.1}{271}{Forward Pass}{equation.14.1}{}}
\newlabel{eq:hidden_layer@cref}{{[equation][1][14]14.1}{[1][271][]271}}
\@writefile{toc}{\contentsline {paragraph}{Output Layer}{272}{equation.14.1}\protected@file@percent }
\newlabel{eq:output_logits}{{14.2}{272}{Output Layer}{equation.14.2}{}}
\newlabel{eq:output_logits@cref}{{[equation][2][14]14.2}{[1][272][]272}}
\newlabel{eq:softmax}{{14.3}{272}{Output Layer}{equation.14.3}{}}
\newlabel{eq:softmax@cref}{{[equation][3][14]14.3}{[1][272][]272}}
\@writefile{toc}{\contentsline {paragraph}{Training Objective}{272}{equation.14.3}\protected@file@percent }
\newlabel{eq:cross_entropy}{{14.4}{272}{Training Objective}{equation.14.4}{}}
\newlabel{eq:cross_entropy@cref}{{[equation][4][14]14.4}{[1][272][]272}}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation and Weight Updates}{272}{equation.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.8}Context window and sequential input}{272}{subsection.14.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Input Sequence Processing}{273}{subsection.14.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dimensionality and Sparsity}{273}{subsection.14.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.9}Interpretation of the Weight Matrix \(W\)}{273}{subsection.14.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.10}Word Embeddings: Continuous Bag of Words (CBOW) and skip-gram models}{273}{subsection.14.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.10.1}Continuous Bag of Words (CBOW)}{273}{subsubsection.14.10.1}\protected@file@percent }
\newlabel{eq:cbow-softmax}{{14.6}{274}{Continuous Bag of Words (CBOW)}{equation.14.6}{}}
\newlabel{eq:cbow-softmax@cref}{{[equation][6][14]14.6}{[1][274][]274}}
\@writefile{toc}{\contentsline {paragraph}{Key Insight:}{274}{equation.14.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.10.2}skip-gram model}{275}{subsubsection.14.10.2}\protected@file@percent }
\newlabel{eq:skipgram-softmax}{{14.9}{275}{skip-gram model}{equation.14.9}{}}
\newlabel{eq:skipgram-softmax@cref}{{[equation][9][14]14.9}{[1][275][]275}}
\@writefile{toc}{\contentsline {paragraph}{Training Objective:}{275}{equation.14.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{275}{equation.14.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.10.3}Computational Challenges: Softmax Normalization}{275}{subsubsection.14.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Approximate Solutions:}{276}{equation.14.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.11}Efficient Training of Word Embeddings: Hierarchical Softmax and Negative Sampling}{276}{subsection.14.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Hierarchical Softmax}{277}{subsection.14.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Negative Sampling}{277}{subsection.14.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{277}{subsection.14.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Objective with Negative Sampling}{277}{subsection.14.11}\protected@file@percent }
\newlabel{eq:neg-sample-prob}{{14.11}{278}{Training Objective with Negative Sampling}{equation.14.11}{}}
\newlabel{eq:neg-sample-prob@cref}{{[equation][11][14]14.11}{[1][278][]278}}
\newlabel{eq:neg-sample-loss}{{14.12}{278}{Training Objective with Negative Sampling}{equation.14.12}{}}
\newlabel{eq:neg-sample-loss@cref}{{[equation][12][14]14.12}{[1][278][]278}}
\@writefile{toc}{\contentsline {paragraph}{Tiny worked example (skip-gram with \(k=2\)).}{278}{equation.14.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{278}{equation.14.12}\protected@file@percent }
\citation{Pennington2014}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation:}{279}{equation.14.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Connection to PMI (Levy \& Goldberg).}{279}{equation.14.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.12}Local Context vs. Global Matrix Factorization Approaches}{279}{subsection.14.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Local Context Window Methods}{279}{subsection.14.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Global Matrix Factorization Methods}{279}{subsection.14.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Co-occurrence Matrix}{280}{subsection.14.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.13}Global Word Vector Representations via Co-occurrence Statistics}{280}{subsection.14.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Setup:}{280}{subsection.14.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notation:}{280}{subsection.14.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Goal:}{280}{subsection.14.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {63}{\ignorespaces Schematic: Analogy geometry in embedding space. The classic offset ``v(king) - v(man) + v(woman) approx v(queen)'' forms a parallelogram; a similar gender direction also moves ``doctor'' toward ``nurse.'' Visualizing these displacement vectors (solid vs. dashed) makes the shared relational direction explicit. Points are shown after a 2D PCA projection, so directions are approximate rather than exact.\relax }}{281}{figure.caption.67}\protected@file@percent }
\newlabel{fig:lec8-embedding-clusters}{{63}{281}{Schematic: Analogy geometry in embedding space. The classic offset ``v(king) - v(man) + v(woman) approx v(queen)'' forms a parallelogram; a similar gender direction also moves ``doctor'' toward ``nurse.'' Visualizing these displacement vectors (solid vs. dashed) makes the shared relational direction explicit. Points are shown after a 2D PCA projection, so directions are approximate rather than exact.\relax }{figure.caption.67}{}}
\newlabel{fig:lec8-embedding-clusters@cref}{{[figure][63][]63}{[1][281][]281}}
\@writefile{toc}{\contentsline {paragraph}{Visualization.}{281}{subsection.14.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.13.1}Modeling Co-occurrence Probabilities}{281}{subsubsection.14.13.1}\protected@file@percent }
\newlabel{eq:cond_prob}{{14.13}{281}{Modeling Co-occurrence Probabilities}{equation.14.13}{}}
\newlabel{eq:cond_prob@cref}{{[equation][13][14]14.13}{[1][281][]281}}
\@writefile{toc}{\contentsline {paragraph}{Relating to word vectors:}{282}{equation.14.13}\protected@file@percent }
\newlabel{eq:exp_model}{{14.14}{282}{Relating to word vectors:}{equation.14.14}{}}
\newlabel{eq:exp_model@cref}{{[equation][14][14]14.14}{[1][282][]282}}
\newlabel{eq:log_prob}{{14.15}{282}{Relating to word vectors:}{equation.14.15}{}}
\newlabel{eq:log_prob@cref}{{[equation][15][14]14.15}{[1][282][]282}}
\@writefile{toc}{\contentsline {paragraph}{Derivation:}{282}{equation.14.15}\protected@file@percent }
\newlabel{eq:log_xik}{{14.17}{282}{Derivation:}{equation.14.17}{}}
\newlabel{eq:log_xik@cref}{{[equation][17][14]14.17}{[1][282][]282}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.13.2}Optimization Objective}{283}{subsubsection.14.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why weighting?}{283}{equation.14.18}\protected@file@percent }
\newlabel{eq:weighting_function}{{14.19}{283}{Why weighting?}{equation.14.19}{}}
\newlabel{eq:weighting_function@cref}{{[equation][19][14]14.19}{[1][283][]283}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.13.3}Interpretation and Remarks}{283}{subsubsection.14.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.14}Finalizing the Word Embedding Derivations}{283}{subsection.14.14}\protected@file@percent }
\newlabel{eq:cooccurrence_log}{{14.20}{283}{Finalizing the Word Embedding Derivations}{equation.14.20}{}}
\newlabel{eq:cooccurrence_log@cref}{{[equation][20][14]14.20}{[1][283][]283}}
\citation{Pennington2014}
\@writefile{toc}{\contentsline {paragraph}{Symmetry and Bias Terms}{284}{equation.14.20}\protected@file@percent }
\newlabel{eq:cooccurrence_log_bias}{{14.21}{284}{Symmetry and Bias Terms}{equation.14.21}{}}
\newlabel{eq:cooccurrence_log_bias@cref}{{[equation][21][14]14.21}{[1][284][]284}}
\@writefile{toc}{\contentsline {paragraph}{Objective Function and Optimization}{284}{equation.14.21}\protected@file@percent }
\newlabel{eq:objective}{{14.22}{284}{Objective Function and Optimization}{equation.14.22}{}}
\newlabel{eq:objective@cref}{{[equation][22][14]14.22}{[1][284][]284}}
\newlabel{eq:glove_weight}{{14.23}{284}{Objective Function and Optimization}{equation.14.23}{}}
\newlabel{eq:glove_weight@cref}{{[equation][23][14]14.23}{[1][284][]284}}
\@writefile{toc}{\contentsline {paragraph}{Singular Value Decomposition (SVD) Connection}{285}{equation.14.23}\protected@file@percent }
\newlabel{eq:svd}{{14.24}{285}{Singular Value Decomposition (SVD) Connection}{equation.14.24}{}}
\newlabel{eq:svd@cref}{{[equation][24][14]14.24}{[1][285][]285}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Limitations}{285}{equation.14.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.15}Bias in Natural Language Processing}{285}{subsection.14.15}\protected@file@percent }
\citation{Bolukbasi2016}
\@writefile{toc}{\contentsline {paragraph}{Sources of Bias}{286}{subsection.14.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact on Embeddings}{286}{subsection.14.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Debiasing Techniques}{286}{subsection.14.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cross-Lingual Challenges}{286}{subsection.14.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.16}Responsible deployment checklist (appendix)}{287}{subsection.14.16}\protected@file@percent }
\citation{Devlin2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.17}Contextual embeddings and transformers}{288}{subsection.14.17}\protected@file@percent }
\citation{Mikolov2013}
\citation{Pennington2014}
\citation{LevyGoldberg2014}
\citation{Devlin2019}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{289}{subsection.14.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{289}{subsection.14.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}Introduction to Soft Computing}{289}{section.15}\protected@file@percent }
\newlabel{chap:softcomp}{{15}{289}{Introduction to Soft Computing}{section.15}{}}
\newlabel{chap:softcomp@cref}{{[section][15][]15}{[1][289][]289}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Hard Computing: The Classical Paradigm}{290}{subsection.15.1}\protected@file@percent }
\citation{Zadeh1994,Zadeh1997}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Soft Computing: Motivation and Definition}{291}{subsection.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Why Soft Computing?}{292}{subsection.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Relationship Between Hard and Soft Computing}{292}{subsection.15.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5}Overview of Soft Computing Constituents}{292}{subsection.15.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6}Distinguishing Imprecision, Uncertainty, and Fuzziness}{293}{subsection.15.6}\protected@file@percent }
\newlabel{sec:imprecision-fuzziness}{{15.6}{293}{Distinguishing Imprecision, Uncertainty, and Fuzziness}{subsection.15.6}{}}
\newlabel{sec:imprecision-fuzziness@cref}{{[subsection][6][15]15.6}{[1][293][]293}}
\citation{Jang1993}
\citation{Dubois1988}
\citation{Herrera2008,Ishibuchi2007}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.7}Soft Computing: Motivation and Overview}{294}{subsection.15.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Schematic: Fuzzy vs.\ probabilistic reasoning at a glance.\relax }}{295}{table.caption.68}\protected@file@percent }
\newlabel{tab:fuzzy-vs-prob}{{5}{295}{Schematic: Fuzzy vs.\ probabilistic reasoning at a glance.\relax }{table.caption.68}{}}
\newlabel{tab:fuzzy-vs-prob@cref}{{[table][5][]5}{[1][294][]295}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Schematic: Boolean operators vs.\ fuzzy operators at a glance.\relax }}{295}{table.caption.69}\protected@file@percent }
\newlabel{tab:boolean-vs-fuzzy}{{6}{295}{Schematic: Boolean operators vs.\ fuzzy operators at a glance.\relax }{table.caption.69}{}}
\newlabel{tab:boolean-vs-fuzzy@cref}{{[table][6][]6}{[1][294][]295}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.8}Fuzzy Logic: Capturing Human Knowledge Linguistically}{295}{subsection.15.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fuzzy Rules and Approximate Reasoning}{296}{subsection.15.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages over Traditional Systems}{296}{equation.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.9}Comparison with Other Soft Computing Paradigms}{296}{subsection.15.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neural Networks}{296}{subsection.15.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Genetic Algorithms}{297}{equation.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Probabilistic Reasoning}{297}{equation.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.10}Zadeh's Insight and the Birth of Fuzzy Logic}{297}{subsection.15.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.11}Challenges in Fuzzy Logic Systems}{298}{subsection.15.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.12}Mathematical Languages as Foundations for Fuzzy Logic}{298}{subsection.15.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.12.1}Relational Algebra}{298}{subsubsection.15.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.12.2}Boolean Algebra}{299}{subsubsection.15.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.12.3}Predicate Algebra}{299}{subsubsection.15.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.12.4}Propositional Calculus}{300}{subsubsection.15.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Modus Ponens}{300}{equation.15.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Modus Tollens}{300}{equation.15.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hypothetical Syllogism}{301}{equation.15.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.13}Fuzzy Logic as a New Mathematical Language}{301}{subsection.15.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.14}Fuzzy Logic: Motivation and Intuition}{301}{subsection.15.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fuzzy truth values}{301}{subsection.15.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why fuzzy logic?}{301}{subsection.15.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.15}From Crisp Sets to Fuzzy Sets}{302}{subsection.15.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Crisp sets}{302}{subsection.15.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{302}{subsection.15.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fuzzy sets}{302}{subsection.15.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Thermostat at a glance.}{303}{subsection.15.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.16}Wrapping Up Fuzzy Sets and Fuzzy Logic}{303}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fuzzy Sets Recap}{304}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Universe of Discourse}{304}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fuzziness and Degrees of Truth}{304}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Height Classification}{304}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fuzzy Actions and Control}{304}{subsection.15.16}\protected@file@percent }
\citation{Zadeh1965}
\citation{DuboisPrade1980}
\citation{YenLangari1999}
\@writefile{toc}{\contentsline {paragraph}{Next Steps: Membership Functions and Fuzzy Inference Systems}{305}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{305}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{306}{subsection.15.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16}Fuzzy Sets and Membership Functions: Foundations and Representations}{306}{section.16}\protected@file@percent }
\newlabel{chap:fuzzysets}{{16}{306}{Fuzzy Sets and Membership Functions: Foundations and Representations}{section.16}{}}
\newlabel{chap:fuzzysets@cref}{{[section][16][]16}{[1][306][]306}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1}Recap: Fuzzy Sets and the Universe of Discourse}{307}{subsection.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2}Membership Functions: Definition and Interpretation}{307}{subsection.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{308}{subsection.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mathematical Representation:}{308}{subsection.16.2}\protected@file@percent }
\newlabel{eq:fuzzy_set_ordered_pairs}{{16.2}{308}{Mathematical Representation:}{equation.16.2}{}}
\newlabel{eq:fuzzy_set_ordered_pairs@cref}{{[equation][2][16]16.2}{[1][308][]308}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3}Discrete vs. Continuous Universes of Discourse}{308}{subsection.16.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.3.1}Discrete Universe}{308}{subsubsection.16.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{308}{equation.16.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.3.2}Continuous Universe}{308}{subsubsection.16.3.2}\protected@file@percent }
\newlabel{eq:continuous_fuzzy_set}{{16.4}{309}{Continuous Universe}{equation.16.4}{}}
\newlabel{eq:continuous_fuzzy_set@cref}{{[equation][4][16]16.4}{[1][308][]309}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{309}{equation.16.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example:}{309}{equation.16.4}\protected@file@percent }
\newlabel{eq:triangular_mf}{{16.5}{309}{Example:}{equation.16.5}{}}
\newlabel{eq:triangular_mf@cref}{{[equation][5][16]16.5}{[1][309][]309}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4}Crisp Sets versus Fuzzy Sets}{309}{subsection.16.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5}Membership Functions in Fuzzy Sets}{309}{subsection.16.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Triangular Membership Function}{310}{subsection.16.5}\protected@file@percent }
\newlabel{eq:triangular-mf}{{16.6}{310}{Triangular Membership Function}{equation.16.6}{}}
\newlabel{eq:triangular-mf@cref}{{[equation][6][16]16.6}{[1][310][]310}}
\@writefile{toc}{\contentsline {paragraph}{Trapezoidal Membership Function}{310}{equation.16.6}\protected@file@percent }
\newlabel{eq:trapezoidal-mf}{{16.7}{310}{Trapezoidal Membership Function}{equation.16.7}{}}
\newlabel{eq:trapezoidal-mf@cref}{{[equation][7][16]16.7}{[1][310][]310}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian Membership Function}{310}{equation.16.7}\protected@file@percent }
\newlabel{eq:gaussian-mf}{{16.8}{310}{Gaussian Membership Function}{equation.16.8}{}}
\newlabel{eq:gaussian-mf@cref}{{[equation][8][16]16.8}{[1][310][]310}}
\@writefile{toc}{\contentsline {paragraph}{Generalized Bell Membership Function}{311}{equation.16.8}\protected@file@percent }
\newlabel{eq:bell-mf}{{16.9}{311}{Generalized Bell Membership Function}{equation.16.9}{}}
\newlabel{eq:bell-mf@cref}{{[equation][9][16]16.9}{[1][311][]311}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6}Comparison of Membership Functions}{311}{subsection.16.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Grading System as Fuzzy Sets}{311}{subsection.16.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {64}{\ignorespaces Schematic: Trapezoidal membership functions for grades C and B with the overlapping region shaded. Scores near 78--82 partially satisfy both grade definitions.\relax }}{312}{figure.caption.70}\protected@file@percent }
\newlabel{fig:lec9-grade-trapezoids}{{64}{312}{Schematic: Trapezoidal membership functions for grades C and B with the overlapping region shaded. Scores near 78--82 partially satisfy both grade definitions.\relax }{figure.caption.70}{}}
\newlabel{fig:lec9-grade-trapezoids@cref}{{[figure][64][]64}{[1][312][]312}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7}Example: Overlapping weight labels}{313}{subsection.16.7}\protected@file@percent }
\newlabel{sec:weight-membership}{{16.7}{313}{Example: Overlapping weight labels}{subsection.16.7}{}}
\newlabel{sec:weight-membership@cref}{{[subsection][7][16]16.7}{[1][312][]313}}
\@writefile{toc}{\contentsline {paragraph}{Quick plotting snippet.}{313}{figure.caption.71}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {65}{\ignorespaces Overlapping membership functions for Small/Medium/Large labels}}{314}{figure.caption.71}\protected@file@percent }
\newlabel{fig:lec9-membership-overlap}{{65}{314}{Overlapping membership functions for Small/Medium/Large labels}{figure.caption.71}{}}
\newlabel{fig:lec9-membership-overlap@cref}{{[figure][65][]65}{[1][313][]314}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8}Fuzzy Sets: Core Concepts and Terminology}{314}{subsection.16.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Support Set}{314}{subsection.16.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Core Set}{314}{equation.16.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Normality}{315}{equation.16.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Crossover Points}{315}{equation.16.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Open and Closed Fuzzy Sets}{315}{equation.16.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9}Probability vs. Possibility}{315}{subsection.16.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.10}Fuzzy Set Operations}{316}{subsection.16.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Union}{316}{subsection.16.10}\protected@file@percent }
\newlabel{eq:fuzzy_union}{{16.13}{316}{Union}{equation.16.13}{}}
\newlabel{eq:fuzzy_union@cref}{{[equation][13][16]16.13}{[1][316][]316}}
\@writefile{toc}{\contentsline {paragraph}{Intersection}{317}{equation.16.13}\protected@file@percent }
\newlabel{eq:fuzzy_intersection}{{16.14}{317}{Intersection}{equation.16.14}{}}
\newlabel{eq:fuzzy_intersection@cref}{{[equation][14][16]16.14}{[1][316][]317}}
\@writefile{toc}{\contentsline {paragraph}{Complement}{317}{equation.16.14}\protected@file@percent }
\newlabel{eq:fuzzy_complement}{{16.15}{317}{Complement}{equation.16.15}{}}
\newlabel{eq:fuzzy_complement@cref}{{[equation][15][16]16.15}{[1][317][]317}}
\@writefile{toc}{\contentsline {paragraph}{Remarks}{317}{equation.16.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reminder on basic operators}{318}{equation.16.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11}Graphical Interpretation}{318}{subsection.16.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.12}Additional Fuzzy Set Operations}{318}{subsection.16.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algebraic Product}{318}{subsection.16.12}\protected@file@percent }
\newlabel{eq:algebraic_product}{{16.18}{318}{Algebraic Product}{equation.16.18}{}}
\newlabel{eq:algebraic_product@cref}{{[equation][18][16]16.18}{[1][318][]318}}
\@writefile{toc}{\contentsline {paragraph}{Scalar Multiplication}{318}{equation.16.18}\protected@file@percent }
\newlabel{eq:scalar_multiplication}{{16.19}{318}{Scalar Multiplication}{equation.16.19}{}}
\newlabel{eq:scalar_multiplication@cref}{{[equation][19][16]16.19}{[1][318][]318}}
\@writefile{toc}{\contentsline {paragraph}{Algebraic Sum}{318}{equation.16.19}\protected@file@percent }
\newlabel{eq:algebraic_sum}{{16.20}{318}{Algebraic Sum}{equation.16.20}{}}
\newlabel{eq:algebraic_sum@cref}{{[equation][20][16]16.20}{[1][318][]318}}
\@writefile{toc}{\contentsline {paragraph}{Difference}{319}{equation.16.20}\protected@file@percent }
\newlabel{eq:fuzzy_difference}{{16.21}{319}{Difference}{equation.16.21}{}}
\newlabel{eq:fuzzy_difference@cref}{{[equation][21][16]16.21}{[1][318][]319}}
\@writefile{toc}{\contentsline {paragraph}{Bounded Difference}{319}{equation.16.21}\protected@file@percent }
\newlabel{eq:bounded_difference}{{16.22}{319}{Bounded Difference}{equation.16.22}{}}
\newlabel{eq:bounded_difference@cref}{{[equation][22][16]16.22}{[1][319][]319}}
\@writefile{toc}{\contentsline {paragraph}{Remarks:}{319}{equation.16.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.13}Example: Union and Intersection of Fuzzy Sets}{319}{subsection.16.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.14}Cartesian Product of Fuzzy Sets}{319}{subsection.16.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition:}{319}{subsection.16.14}\protected@file@percent }
\newlabel{eq:cartesian_product}{{16.23}{319}{Definition:}{equation.16.23}{}}
\newlabel{eq:cartesian_product@cref}{{[equation][23][16]16.23}{[1][319][]319}}
\@writefile{toc}{\contentsline {paragraph}{Example:}{320}{equation.16.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.15}Properties of Fuzzy Set Operations}{320}{subsection.16.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Commutativity:}{320}{subsection.16.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Associativity:}{320}{equation.16.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Distributivity:}{320}{equation.16.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Identity Elements:}{321}{equation.16.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Involution:}{321}{equation.16.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{De Morgan's Laws:}{321}{equation.16.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.16}Fuzzy Set Operators}{321}{subsection.16.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples of Operators:}{321}{subsection.16.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.17}Complement Operators in Fuzzy Logic}{322}{subsection.16.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard Complement}{322}{subsection.16.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameterized Complement Operators}{322}{subsection.16.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Properties of Complement Operators}{322}{figure.caption.72}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {66}{\ignorespaces Schematic: Fuzzy AND surfaces comparing minimum versus product t\hyp  {}norms; analogous OR surfaces show similar differences. Choices here influence rule aggregation in Chapter\nobreakspace  {}\ref  {chap:fuzzyinference}.\relax }}{323}{figure.caption.72}\protected@file@percent }
\newlabel{fig:tnorm-surfaces}{{66}{323}{Schematic: Fuzzy AND surfaces comparing minimum versus product t\hyp {}norms; analogous OR surfaces show similar differences. Choices here influence rule aggregation in Chapter~\ref {chap:fuzzyinference}.\relax }{figure.caption.72}{}}
\newlabel{fig:tnorm-surfaces@cref}{{[figure][66][]66}{[1][322][]323}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.18}Triangular norms (t\hyp  {}norms)}{323}{subsection.16.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation}{323}{subsection.16.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{323}{subsection.16.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples of t\hyp  {}norms}{324}{Item.111}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{324}{Item.111}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.19}Triangular conorms (t\hyp  {}conorms / s\hyp  {}norms)}{325}{subsection.16.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{325}{subsection.16.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.20}T-Norms and S-Norms: Complementarity and Properties}{325}{subsection.16.20}\protected@file@percent }
\newlabel{eq:tnorm-snorm-complement}{{16.35}{326}{T-Norms and S-Norms: Complementarity and Properties}{equation.16.35}{}}
\newlabel{eq:tnorm-snorm-complement@cref}{{[equation][35][16]16.35}{[1][326][]326}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.21}Examples of common t\hyp  {}norm/s\hyp  {}norm pairs}{326}{subsection.16.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.22}Fuzzy Set Inclusion and Subset Relations}{326}{subsection.16.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition (Fuzzy Subset).}{327}{subsection.16.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{327}{subsection.16.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.23}Degree of Inclusion}{327}{subsection.16.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.24}Set Operations and Inclusion Properties}{327}{subsection.16.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.25}Grades of Inclusion and Equality in Fuzzy Sets}{328}{subsection.16.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Grade of Inclusion}{328}{subsection.16.25}\protected@file@percent }
\newlabel{eq:grade_inclusion}{{16.36}{328}{Grade of Inclusion}{equation.16.36}{}}
\newlabel{eq:grade_inclusion@cref}{{[equation][36][16]16.36}{[1][328][]328}}
\@writefile{toc}{\contentsline {paragraph}{Example}{329}{equation.16.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Grade of Equality}{329}{equation.16.36}\protected@file@percent }
\newlabel{eq:grade_equality}{{16.37}{329}{Grade of Equality}{equation.16.37}{}}
\newlabel{eq:grade_equality@cref}{{[equation][37][16]16.37}{[1][329][]329}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.26}Dilation and Contraction of Fuzzy Sets}{329}{subsection.16.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation}{329}{subsection.16.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definitions}{330}{subsection.16.26}\protected@file@percent }
\newlabel{eq:dilation}{{16.38}{330}{Definitions}{equation.16.38}{}}
\newlabel{eq:dilation@cref}{{[equation][38][16]16.38}{[1][330][]330}}
\newlabel{eq:contraction}{{16.39}{330}{Definitions}{equation.16.39}{}}
\newlabel{eq:contraction@cref}{{[equation][39][16]16.39}{[1][330][]330}}
\@writefile{toc}{\contentsline {paragraph}{Properties}{330}{equation.16.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.27}Closure of Membership Function Derivations}{330}{subsection.16.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.27.1}Generating New Membership Functions via Set Operations}{331}{subsubsection.16.27.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dilation (Expansion)}{331}{subsubsection.16.27.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contraction (Narrowing)}{331}{subsubsection.16.27.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Complement}{331}{subsubsection.16.27.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intersection}{331}{subsubsection.16.27.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Union}{332}{subsubsection.16.27.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.27.2}Examples of Constructed Membership Functions}{332}{subsubsection.16.27.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Remark on Normality}{332}{subsubsection.16.27.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.28}Implications for Fuzzy Inference Systems}{332}{subsection.16.28}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Schematic: Typical operator choices in fuzzy inference and their qualitative effects. Here the t-norm implements fuzzy AND, the s-norm implements fuzzy OR, and the implication shapes consequents.\relax }}{333}{table.caption.73}\protected@file@percent }
\newlabel{tab:fuzzy-operators}{{7}{333}{Schematic: Typical operator choices in fuzzy inference and their qualitative effects. Here the t-norm implements fuzzy AND, the s-norm implements fuzzy OR, and the implication shapes consequents.\relax }{table.caption.73}{}}
\newlabel{tab:fuzzy-operators@cref}{{[table][7][]7}{[1][333][]333}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.29}Worked Example: Mamdani Fuzzy Inference (End-to-End)}{334}{subsection.16.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Universes and membership functions}{334}{subsection.16.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rule base}{334}{subsection.16.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fuzzify input and compute firing strengths}{334}{Item.118}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Defuzzification (centroid)}{335}{Item.118}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {67}{\ignorespaces Schematic: End-to-end fuzzy inference example. (A) Consequent membership functions with clipping levels from firing strengths at T = 27 deg C. (B) Aggregated output set (max of truncated consequents) and a centroid marker near s* approx 0.58.\relax }}{336}{figure.caption.74}\protected@file@percent }
\newlabel{fig:fuzzy-end-to-end}{{67}{336}{Schematic: End-to-end fuzzy inference example. (A) Consequent membership functions with clipping levels from firing strengths at T = 27 deg C. (B) Aggregated output set (max of truncated consequents) and a centroid marker near s* approx 0.58.\relax }{figure.caption.74}{}}
\newlabel{fig:fuzzy-end-to-end@cref}{{[figure][67][]67}{[1][335][]336}}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{337}{figure.caption.74}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{337}{figure.caption.74}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17}Fuzzy Set Transformations Between Related Universes}{337}{section.17}\protected@file@percent }
\newlabel{chap:fuzzyrelations}{{17}{337}{Fuzzy Set Transformations Between Related Universes}{section.17}{}}
\newlabel{chap:fuzzyrelations@cref}{{[section][17][]17}{[1][337][]337}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1}Context and Motivation}{338}{subsection.17.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notation.}{338}{subsection.17.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2}Problem Statement}{339}{subsection.17.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3}Intuition and Challenges}{339}{subsection.17.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4}Formal Definition of the Transformed Membership Function}{339}{subsection.17.4}\protected@file@percent }
\newlabel{eq:membership-transform}{{17.1}{339}{Formal Definition of the Transformed Membership Function}{equation.17.1}{}}
\newlabel{eq:membership-transform@cref}{{[equation][1][17]17.1}{[1][339][]339}}
\@writefile{toc}{\contentsline {paragraph}{Remarks:}{340}{equation.17.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5}Interpretation}{340}{subsection.17.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6}Example Setup}{340}{subsection.17.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.7}Transformation of Fuzzy Sets Between Universes}{341}{subsection.17.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Mapping via \( y = x^2 \)}{341}{subsection.17.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {68}{\ignorespaces Schematic: Mapping a fuzzy set through the function ``y = x-squared''. The membership at an output value y is the supremum over all pre-images x that map to y; shared images such as x = +/-1 map to y = 1 using the maximum membership.\relax }}{342}{figure.caption.75}\protected@file@percent }
\newlabel{fig:lec10-extension}{{68}{342}{Schematic: Mapping a fuzzy set through the function ``y = x-squared''. The membership at an output value y is the supremum over all pre-images x that map to y; shared images such as x = +/-1 map to y = 1 using the maximum membership.\relax }{figure.caption.75}{}}
\newlabel{fig:lec10-extension@cref}{{[figure][68][]68}{[1][342][]342}}
\@writefile{toc}{\contentsline {paragraph}{Visual intuition.}{342}{subsection.17.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extension to Multiple Fuzzy Sets}{342}{figure.caption.75}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Membership Values in \( Y \)}{343}{figure.caption.75}\protected@file@percent }
\newlabel{eq:extension-two-var}{{17.2}{343}{Computing Membership Values in \( Y \)}{equation.17.2}{}}
\newlabel{eq:extension-two-var@cref}{{[equation][2][17]17.2}{[1][343][]343}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.8}Extension Principle Recap and Projection Operations}{344}{subsection.17.8}\protected@file@percent }
\newlabel{eq:extension_principle}{{17.3}{344}{Extension Principle Recap and Projection Operations}{equation.17.3}{}}
\newlabel{eq:extension_principle@cref}{{[equation][3][17]17.3}{[1][344][]344}}
\@writefile{lof}{\contentsline {figure}{\numberline {69}{\ignorespaces Schematic: Alpha-cuts under the non-monotone map ``y = x-squared''. A symmetric triangular fuzzy set on X maps to a right-skewed fuzzy set on Y. Each alpha-cut on A splits into two intervals whose images union to the output alpha-cut.\relax }}{345}{figure.caption.76}\protected@file@percent }
\newlabel{fig:alpha-cut-nonmonotone}{{69}{345}{Schematic: Alpha-cuts under the non-monotone map ``y = x-squared''. A symmetric triangular fuzzy set on X maps to a right-skewed fuzzy set on Y. Each alpha-cut on A splits into two intervals whose images union to the output alpha-cut.\relax }{figure.caption.76}{}}
\newlabel{fig:alpha-cut-nonmonotone@cref}{{[figure][69][]69}{[1][345][]345}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.9}Projection of Fuzzy Relations}{345}{subsection.17.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cartesian Product of Fuzzy Sets}{345}{subsection.17.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Schematic: Popular t\hyp  {}norms and their typical roles.\relax }}{346}{table.caption.77}\protected@file@percent }
\newlabel{tab:tnorms}{{8}{346}{Schematic: Popular t\hyp {}norms and their typical roles.\relax }{table.caption.77}{}}
\newlabel{tab:tnorms@cref}{{[table][8][]8}{[1][346][]346}}
\@writefile{toc}{\contentsline {paragraph}{Example}{346}{table.caption.77}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Projection of Fuzzy Relations}{347}{table.caption.77}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition (Projection onto $X$).}{347}{table.caption.77}\protected@file@percent }
\newlabel{eq:projection_x}{{17.5}{347}{Definition (Projection onto $X$)}{equation.17.5}{}}
\newlabel{eq:projection_x@cref}{{[equation][5][17]17.5}{[1][347][]347}}
\@writefile{toc}{\contentsline {paragraph}{Definition (Projection onto $Y$).}{347}{equation.17.5}\protected@file@percent }
\newlabel{eq:projection_y}{{17.6}{347}{Definition (Projection onto $Y$)}{equation.17.6}{}}
\newlabel{eq:projection_y@cref}{{[equation][6][17]17.6}{[1][347][]347}}
\@writefile{toc}{\contentsline {paragraph}{Total Projection}{347}{equation.17.6}\protected@file@percent }
\newlabel{eq:total_projection}{{17.7}{347}{Total Projection}{equation.17.7}{}}
\newlabel{eq:total_projection@cref}{{[equation][7][17]17.7}{[1][347][]347}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{347}{equation.17.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example (continued)}{347}{figure.caption.78}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {70}{\ignorespaces Schematic: Illustrative fuzzy relation table (left) together with its projections onto the error universe (middle) and the rate-of-change universe (right). These are the exact quantities used in the running thermostat example before composing rules.\relax }}{348}{figure.caption.78}\protected@file@percent }
\newlabel{fig:lec17_projection_matrix}{{70}{348}{Schematic: Illustrative fuzzy relation table (left) together with its projections onto the error universe (middle) and the rate-of-change universe (right). These are the exact quantities used in the running thermostat example before composing rules.\relax }{figure.caption.78}{}}
\newlabel{fig:lec17_projection_matrix@cref}{{[figure][70][]70}{[1][347][]348}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.10}Dimensional Extension and Projection in Fuzzy Set Operations}{348}{subsection.17.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cylindrical Extension}{348}{subsection.17.10}\protected@file@percent }
\newlabel{eq:cylindrical_extension}{{17.8}{348}{Cylindrical Extension}{equation.17.8}{}}
\newlabel{eq:cylindrical_extension@cref}{{[equation][8][17]17.8}{[1][348][]348}}
\@writefile{toc}{\contentsline {paragraph}{Projection}{349}{equation.17.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example}{349}{equation.17.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.11}Fuzzy Inference via Composition of Relations}{349}{subsection.17.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Setup}{349}{subsection.17.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Composition of Fuzzy Relations}{349}{subsection.17.11}\protected@file@percent }
\newlabel{eq:fuzzy_composition}{{17.9}{349}{Composition of Fuzzy Relations}{equation.17.9}{}}
\newlabel{eq:fuzzy_composition@cref}{{[equation][9][17]17.9}{[1][349][]349}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{350}{equation.17.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dimensional Considerations}{350}{equation.17.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example}{350}{equation.17.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.12}Recap and Motivation}{351}{subsection.17.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.13}Generalization of Fuzzy Relation Composition}{351}{subsection.17.13}\protected@file@percent }
\newlabel{eq:composition_general}{{17.10}{351}{Generalization of Fuzzy Relation Composition}{equation.17.10}{}}
\newlabel{eq:composition_general@cref}{{[equation][10][17]17.10}{[1][351][]351}}
\@writefile{toc}{\contentsline {paragraph}{Max--min Composition:}{352}{equation.17.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.14}Example Calculation of Composition}{352}{subsection.17.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.15}Properties of Fuzzy Relation Composition}{352}{subsection.17.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.16}Alternative Composition Operators}{353}{subsection.17.16}\protected@file@percent }
\citation{Zadeh1975}
\citation{BandlerKohout1980}
\citation{KlirYuan1995}
\citation{Dubois1988}
\citation{Klement2000}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{354}{subsection.17.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{354}{subsection.17.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {18}Fuzzy Inference Systems: Rule Composition and Output Calculation}{355}{section.18}\protected@file@percent }
\newlabel{chap:fuzzyinference}{{18}{355}{Fuzzy Inference Systems: Rule Composition and Output Calculation}{section.18}{}}
\newlabel{chap:fuzzyinference@cref}{{[section][18][]18}{[1][354][]355}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1}Context and Motivation}{356}{subsection.18.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2}Rule Antecedent Composition}{356}{subsection.18.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Membership values of antecedents:}{356}{subsection.18.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Aggregation operator:}{356}{equation.18.1}\protected@file@percent }
\citation{Klement2000,Dubois1988}
\newlabel{eq:antecedent_min}{{18.2}{357}{Aggregation operator:}{equation.18.2}{}}
\newlabel{eq:antecedent_min@cref}{{[equation][2][18]18.2}{[1][356][]357}}
\newlabel{eq:antecedent_prod}{{18.3}{357}{Aggregation operator:}{equation.18.3}{}}
\newlabel{eq:antecedent_prod@cref}{{[equation][3][18]18.3}{[1][356][]357}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3}Rule Consequent and Output Fuzzy Set}{357}{subsection.18.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implication operator:}{357}{subsection.18.3}\protected@file@percent }
\newlabel{eq:min_implication}{{18.4}{357}{Implication operator:}{equation.18.4}{}}
\newlabel{eq:min_implication@cref}{{[equation][4][18]18.4}{[1][357][]357}}
\newlabel{eq:prod_implication}{{18.5}{357}{Implication operator:}{equation.18.5}{}}
\newlabel{eq:prod_implication@cref}{{[equation][5][18]18.5}{[1][357][]357}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4}Aggregation of Multiple Rules}{358}{subsection.18.4}\protected@file@percent }
\newlabel{eq:output_aggregation}{{18.6}{358}{Aggregation of Multiple Rules}{equation.18.6}{}}
\newlabel{eq:output_aggregation@cref}{{[equation][6][18]18.6}{[1][358][]358}}
\@writefile{toc}{\contentsline {paragraph}{Other aggregations}{358}{equation.18.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5}Summary of the Fuzzy Inference Process}{358}{subsection.18.5}\protected@file@percent }
\newlabel{eq:centroid_defuzz}{{18.7}{359}{Summary of the Fuzzy Inference Process}{equation.18.7}{}}
\newlabel{eq:centroid_defuzz@cref}{{[equation][7][18]18.7}{[1][359][]359}}
\@writefile{toc}{\contentsline {paragraph}{Other defuzzifiers}{359}{equation.18.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Zero-mass fallback}{359}{equation.18.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computation note}{359}{equation.18.7}\protected@file@percent }
\citation{Mamdani1975}
\citation{TakagiSugeno1985}
\citation{Jang1993}
\citation{Klement2000,Dubois1988}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6}Mamdani vs.\ Sugeno/Takagi--Sugeno systems}{361}{subsection.18.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{362}{subsection.18.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{362}{subsection.18.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {19}Introduction to Evolutionary Computing}{363}{section.19}\protected@file@percent }
\newlabel{chap:evo}{{19}{363}{Introduction to Evolutionary Computing}{section.19}{}}
\newlabel{chap:evo@cref}{{[section][19][]19}{[1][362][]363}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1}Context and Motivation}{363}{subsection.19.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2}Philosophical and Historical Background}{364}{subsection.19.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Insight:}{364}{subsection.19.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3}Problem Setting: Optimization}{364}{subsection.19.3}\protected@file@percent }
\newlabel{eq:optimization_problem}{{19.1}{364}{Problem Setting: Optimization}{equation.19.1}{}}
\newlabel{eq:optimization_problem@cref}{{[equation][1][19]19.1}{[1][364][]364}}
\@writefile{toc}{\contentsline {paragraph}{Challenges:}{365}{equation.19.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.4}Illustrative Example}{365}{subsection.19.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Goal:}{365}{subsection.19.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.5}Why Not Brute Force?}{365}{subsection.19.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.6}Summary}{366}{subsection.19.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.7}Challenges in Continuous Optimization and Motivation for Evolutionary Computing}{366}{subsection.19.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Issues with Traditional Methods}{366}{subsection.19.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.8}Introduction to Evolutionary Computing}{367}{subsection.19.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Idea}{367}{subsection.19.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Genetic Algorithms (GAs)}{367}{subsection.19.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.9}Biological Inspiration: Evolutionary Concepts}{367}{subsection.19.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chromosomes and Genes}{367}{subsection.19.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cell Division: Mitosis vs. Meiosis}{367}{subsection.19.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Genetic Recombination and Variation}{368}{subsection.19.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inheritance and Heredity}{368}{subsection.19.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.10}Implications for Genetic Algorithms}{368}{subsection.19.10}\protected@file@percent }
\citation{DebAgrawal1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.11}Summary of Biological Mechanisms Modeled in GAs}{369}{subsection.19.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.12}Genetic Algorithms: Modeling Chromosomes}{370}{subsection.19.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chromosomes as Information Carriers}{370}{subsection.19.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inheritance and Crossover}{370}{subsection.19.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {71}{\ignorespaces Schematic: Evolutionary micro-operators. Left: fitter individuals get sampled more often (roulette/tournament). Middle: crossover splices parents by a mask (one-point shown). Right: constraint handling routes offspring through repair/penalty/feasibility before evaluation.\relax }}{371}{figure.caption.79}\protected@file@percent }
\newlabel{fig:lec11-ea-micro}{{71}{371}{Schematic: Evolutionary micro-operators. Left: fitter individuals get sampled more often (roulette/tournament). Middle: crossover splices parents by a mask (one-point shown). Right: constraint handling routes offspring through repair/penalty/feasibility before evaluation.\relax }{figure.caption.79}{}}
\newlabel{fig:lec11-ea-micro@cref}{{[figure][71][]71}{[1][371][]371}}
\@writefile{toc}{\contentsline {paragraph}{Modeling the Genetic Operations}{371}{figure.caption.79}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitness and Selection}{371}{equation.19.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Probabilistic Survival and Evolution}{372}{equation.19.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.13}Mapping Genetic Algorithms to Optimization Problems}{372}{subsection.19.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key GA Components in Optimization Terms}{372}{subsection.19.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitness as Objective Function Proxy}{373}{subsection.19.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.14}Encoding in Genetic Algorithms}{373}{subsection.19.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Genotype and Phenotype}{373}{subsection.19.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.14.1}Common Encoding Schemes}{374}{subsubsection.19.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Binary Encoding}{374}{subsubsection.19.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Floating-Point Encoding}{374}{subsubsection.19.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Gray Coding}{374}{subsubsection.19.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.14.2}Example: Binary Encoding of Parameters}{374}{subsubsection.19.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.14.3}Example Problem: Minimization with Constraints}{375}{subsubsection.19.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoding Strategy}{375}{subsubsection.19.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoding}{375}{subsubsection.19.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitness Evaluation}{375}{subsubsection.19.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.15}Population Initialization and Size}{376}{subsection.19.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Population Size}{376}{subsection.19.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example}{376}{subsection.19.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.16}Genetic Operators}{376}{subsection.19.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.16.1}Selection}{376}{subsubsection.19.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common Methods}{376}{subsubsection.19.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.16.2}Crossover}{376}{subsubsection.19.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Binary Crossover}{376}{subsubsection.19.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.17}Selection in Genetic Algorithms}{377}{subsection.19.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.17.1}Fitness and Selection Probability}{377}{subsubsection.19.17.1}\protected@file@percent }
\newlabel{eq:selection_probability}{{19.3}{377}{Fitness and Selection Probability}{equation.19.3}{}}
\newlabel{eq:selection_probability@cref}{{[equation][3][19]19.3}{[1][377][]377}}
\@writefile{toc}{\contentsline {paragraph}{Roulette Wheel Selection}{377}{equation.19.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example}{378}{equation.19.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.17.2}Ranking Selection}{378}{subsubsection.19.17.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Procedure}{378}{subsubsection.19.17.2}\protected@file@percent }
\newlabel{eq:linear_ranking}{{19.4}{378}{Procedure}{equation.19.4}{}}
\newlabel{eq:linear_ranking@cref}{{[equation][4][19]19.4}{[1][378][]378}}
\@writefile{toc}{\contentsline {paragraph}{Elitism}{379}{equation.19.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages}{379}{equation.19.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.18}Crossover Operator}{379}{subsection.19.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {19.18.1}One-Point Crossover}{379}{subsubsection.19.18.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.19}Crossover Operators in Genetic Algorithms}{380}{subsection.19.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Single-point crossover}{380}{subsection.19.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multi-point crossover}{380}{subsection.19.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Probabilistic nature of crossover}{381}{subsection.19.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.20}Mutation Operator}{381}{subsection.19.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Biological motivation}{381}{subsection.19.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Role in optimization}{381}{subsection.19.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation of mutation}{381}{subsection.19.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mutation operator formalization}{382}{subsection.19.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.21}Summary of Genetic Operators and Their Probabilities}{382}{subsection.19.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.22}Known Issues in Genetic Algorithms}{382}{subsection.19.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Premature Convergence}{382}{subsection.19.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mutation Interference}{383}{subsection.19.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deception}{383}{subsection.19.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitness Misinterpretation}{383}{subsection.19.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.23}Convergence Criteria}{383}{subsection.19.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {72}{\ignorespaces Schematic: Illustrative GA run showing the best and mean normalized fitness over 50 generations. Flat regions motivate ``no improvement'' stopping rules, while steady separation between best and mean indicates ongoing selection pressure.\relax }}{384}{figure.caption.80}\protected@file@percent }
\newlabel{fig:lec11-ga-progress}{{72}{384}{Schematic: Illustrative GA run showing the best and mean normalized fitness over 50 generations. Flat regions motivate ``no improvement'' stopping rules, while steady separation between best and mean indicates ongoing selection pressure.\relax }{figure.caption.80}{}}
\newlabel{fig:lec11-ga-progress@cref}{{[figure][72][]72}{[1][384][]384}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.24}Summary of Genetic Algorithm Workflow}{384}{subsection.19.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {73}{\ignorespaces Schematic: GA flowchart showing the iterative process: initialization leads to fitness evaluation and a termination check. If not terminated, the algorithm proceeds through selection, crossover, mutation, and replacement, which then feeds the next generation's fitness evaluation.\relax }}{385}{figure.caption.81}\protected@file@percent }
\newlabel{fig:lec11-ga-flow}{{73}{385}{Schematic: GA flowchart showing the iterative process: initialization leads to fitness evaluation and a termination check. If not terminated, the algorithm proceeds through selection, crossover, mutation, and replacement, which then feeds the next generation's fitness evaluation.\relax }{figure.caption.81}{}}
\newlabel{fig:lec11-ga-flow@cref}{{[figure][73][]73}{[1][385][]385}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Schematic: Toy GA generation on a bounded interval. One crossover and mutation illustrate how the fitness function guides selection before the next generation.\relax }}{386}{table.caption.82}\protected@file@percent }
\newlabel{tab:ga-toy}{{9}{386}{Schematic: Toy GA generation on a bounded interval. One crossover and mutation illustrate how the fitness function guides selection before the next generation.\relax }{table.caption.82}{}}
\newlabel{tab:ga-toy@cref}{{[table][9][]9}{[1][385][]386}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.25}Pseudocode Representation}{386}{subsection.19.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.26}Example: GA for a Constrained Optimization Problem}{386}{subsection.19.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GA Parameters:}{387}{subsection.19.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initialization:}{387}{subsection.19.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitness Evaluation:}{387}{subsection.19.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evolutionary Cycle:}{387}{subsection.19.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Remarks:}{387}{subsection.19.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reproducibility and fair comparison}{389}{subsection.19.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.27}Genetic Algorithms: Iterative Process and Convergence}{389}{subsection.19.27}\protected@file@percent }
\citation{Hansen2001}
\@writefile{toc}{\contentsline {paragraph}{Selection and Reproduction}{390}{subsection.19.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Crossover and Mutation}{390}{subsection.19.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evolution Over Generations}{390}{subsection.19.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.28}Beyond canonical GAs: real-coded strategies}{390}{subsection.19.28}\protected@file@percent }
\citation{Storn1997}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.29}Genetic Programming (GP)}{391}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Problem Setup}{391}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Representation of Programs}{391}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Genetic Operators in GP}{392}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitness Evaluation}{392}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example}{392}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recursive and Modular Programs}{392}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications}{393}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Robot Obstacle Avoidance}{393}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{393}{subsection.19.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.30}Wrapping Up Genetic Algorithms and Genetic Programming}{393}{subsection.19.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recap of Genetic Algorithms}{393}{subsection.19.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Genetic Programming: Structure over Parameters}{394}{subsection.19.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications and Insights}{394}{subsection.19.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Further Topics and Extensions}{394}{subsection.19.30}\protected@file@percent }
\citation{Deb2002}
\citation{Zitzler2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.31}Multi-objective search and NSGA-II}{395}{subsection.19.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Metrics and variants}{395}{subsection.19.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {74}{\ignorespaces Schematic: Sample Pareto front for two objectives. NSGA-II keeps all non-dominated points (blue) while pushing dominated solutions (orange) toward the front via selection, yielding a spread of trade-offs in one run.\relax }}{396}{figure.caption.83}\protected@file@percent }
\newlabel{fig:lec11-pareto}{{74}{396}{Schematic: Sample Pareto front for two objectives. NSGA-II keeps all non-dominated points (blue) while pushing dominated solutions (orange) toward the front via selection, yielding a spread of trade-offs in one run.\relax }{figure.caption.83}{}}
\newlabel{fig:lec11-pareto@cref}{{[figure][74][]74}{[1][395][]396}}
\citation{Holland1975}
\citation{Koza1992}
\citation{Goldberg1989}
\citation{Deb2001Book}
\citation{Mitchell1998}
\@writefile{toc}{\contentsline {paragraph}{Where we head next.}{397}{figure.caption.83}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{References.}{397}{figure.caption.83}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Key Takeaways}{398}{figure.caption.83}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {75}{\ignorespaces Map of model families}}{402}{figure.caption.84}\protected@file@percent }
\newlabel{fig:big-picture-map}{{75}{402}{Map of model families}{figure.caption.84}{}}
\newlabel{fig:big-picture-map@cref}{{[figure][75][]75}{[1][401][]402}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Schematic: Big-picture view of model families across the taxonomy and learning paradigms. Each entry represents a family introduced in the book; supervision labels indicate the dominant training signal rather than strict exclusivity.\relax }}{403}{table.caption.85}\protected@file@percent }
\newlabel{tab:big-picture-models}{{10}{403}{Schematic: Big-picture view of model families across the taxonomy and learning paradigms. Each entry represents a family introduced in the book; supervision labels indicate the dominant training signal rather than strict exclusivity.\relax }{table.caption.85}{}}
\newlabel{tab:big-picture-models@cref}{{[table][10][]10}{[1][401][]403}}
\citation{RussellNorvig2021}
\citation{PooleMackworth2017}
\citation{IEEEEthicallyAligned2019}
\citation{CoxRaja2011}
\citation{Hill1998}
\citation{Amodei2016}
\citation{Bishop1995}
\citation{Goodfellow2016}
\citation{Haykin2009}
\citation{Zadeh1965}
\citation{KlirYuan1995}
\citation{DeJong2006}
\citation{DudaHartStork2001}
\citation{Goldberg1989}
\citation{Krizhevsky2012}
\citation{Mikolov2013}
\citation{LevyGoldberg2014}
\bibstyle{plainnat-ece657}
\bibdata{refs}
\bibcite{AmitGutfreundSompolinsky1985}{{1}{1985}{{Amit et~al.}}{{Amit, Gutfreund, and Sompolinsky}}}
\bibcite{Amodei2016}{{2}{2016}{{Amodei et~al.}}{{Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}}}}
\bibcite{Arkin1998}{{3}{1998}{{Arkin}}{{}}}
\bibcite{Ba2016}{{4}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{BandlerKohout1980}{{5}{1980}{{Bandler and Kohout}}{{}}}
\bibcite{Belkin2019}{{6}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma, and Mandal}}}
\bibcite{Bengio2015}{{7}{2015}{{Bengio et~al.}}{{Bengio, Vinyals, Jaitly, and Shazeer}}}
\bibcite{Bengio1994}{{8}{1994}{{Bengio et~al.}}{{Bengio, Simard, and Frasconi}}}
\bibcite{Bishop1995}{{9}{1995}{{Bishop}}{{}}}
\bibcite{Bolukbasi2016}{{10}{2016}{{Bolukbasi et~al.}}{{Bolukbasi, Chang, Zou, Saligrama, and Kalai}}}
\bibcite{Bronstein2005}{{11}{2005}{{Bronstein}}{{}}}
\bibcite{Brooks1986}{{12}{1986}{{Brooks}}{{}}}
\bibcite{Chen1999LinearSystemTheoryDesign}{{13}{1999}{{Chen}}{{}}}
\bibcite{Cho2014}{{14}{2014}{{Cho et~al.}}{{Cho, van Merri{\"e}nboer, Bahdanau, and Bengio}}}
\bibcite{Cortes1995}{{15}{1995}{{Cortes and Vapnik}}{{}}}
\bibcite{CottrellFort1986}{{16}{1986}{{Cottrell and Fort}}{{}}}
\bibcite{CoxRaja2011}{{17}{2011}{{Cox and Raja}}{{}}}
\bibcite{Cybenko1989}{{18}{1989}{{Cybenko}}{{}}}
\bibcite{Dao2022FlashAttention}{{19}{2022}{{Dao et~al.}}{{Dao, Fu, Ermon, Rudra, and R{\'e}}}}
\bibcite{Deb2001Book}{{20}{2001}{{Deb}}{{}}}
\bibcite{DebAgrawal1995}{{21}{1995}{{Deb and Agrawal}}{{}}}
\bibcite{Deb2002}{{22}{2002}{{Deb et~al.}}{{Deb, Pratap, Agarwal, and Meyarivan}}}
\bibcite{Dettmers2023QLoRA}{{23}{2023}{{Dettmers et~al.}}{{Dettmers, Belkada, and Zettlemoyer}}}
\bibcite{Devlin2019}{{24}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{Dosovitskiy2021}{{25}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, et~al.}}}
\bibcite{DuboisPrade1980}{{26}{1980}{{Dubois and Prade}}{{}}}
\bibcite{Dubois1988}{{27}{1988}{{Dubois and Prade}}{{}}}
\bibcite{DudaHartStork2001}{{28}{2001}{{Duda et~al.}}{{Duda, Hart, and Stork}}}
\bibcite{Elman1990}{{29}{1990}{{Elman}}{{}}}
\bibcite{ErwinObermayerSchulten1992}{{30}{1992}{{Erwin et~al.}}{{Erwin, Obermayer, and Schulten}}}
\bibcite{Fritzke1994GrowingNeuralGas}{{31}{1994}{{Fritzke}}{{}}}
\bibcite{Gal2016}{{32}{2016}{{Gal and Ghahramani}}{{}}}
\bibcite{Gers2000}{{33}{2000}{{Gers et~al.}}{{Gers, Schmidhuber, and Cummins}}}
\bibcite{Goldberg1989}{{34}{1989}{{Goldberg}}{{}}}
\bibcite{Goodfellow2016}{{35}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, and Courville}}}
\bibcite{Gu2023Mamba}{{36}{2023}{{Gu et~al.}}{{Gu, Johnson, and Dao}}}
\bibcite{Guo2017}{{37}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{Hansen2001}{{38}{2001}{{Hansen and Ostermeier}}{{}}}
\bibcite{HastieTibshiraniFriedman2009}{{39}{2009}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{Haykin2009}{{40}{2009}{{Haykin}}{{}}}
\bibcite{Haykin2013AdaptiveFilterTheory}{{41}{2013}{{Haykin}}{{}}}
\bibcite{Herrera2008}{{42}{2008}{{Herrera and Lozano}}{{}}}
\bibcite{Hill1998}{{43}{1998}{{Hill}}{{}}}
\bibcite{Hochreiter1997}{{44}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{Hoffmann2022}{{45}{2022}{{Hoffmann et~al.}}{{Hoffmann, Borgeaud, Mensch, et~al.}}}
\bibcite{Holland1975}{{46}{1975}{{Holland}}{{}}}
\bibcite{Hopfield1982}{{47}{1982}{{Hopfield}}{{}}}
\bibcite{Hu2022LoRA}{{48}{2022}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}}}
\bibcite{IEEEEthicallyAligned2019}{{49}{2019}{{IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems}}{{}}}
\bibcite{Ishibuchi2007}{{50}{2007}{{Ishibuchi and Nakashima}}{{}}}
\bibcite{Jang1993}{{51}{1993}{{Jang}}{{}}}
\bibcite{DeJong2006}{{52}{2006}{{Jong}}{{}}}
\bibcite{JurafskyMartin2023}{{53}{2023}{{Jurafsky and Martin}}{{}}}
\bibcite{Kailath1980}{{54}{1980}{{Kailath}}{{}}}
\bibcite{Kaplan2020}{{55}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, Henighan, et~al.}}}
\bibcite{Kingma2015}{{56}{2015}{{Kingma and Ba}}{{}}}
\bibcite{Klement2000}{{57}{2000}{{Klement et~al.}}{{Klement, Mesiar, and Pap}}}
\bibcite{KlirYuan1995}{{58}{1995}{{Klir and Yuan}}{{}}}
\bibcite{Kohonen1982}{{59}{1982}{{Kohonen}}{{}}}
\bibcite{Kohonen2001}{{60}{2001}{{Kohonen}}{{}}}
\bibcite{Koza1992}{{61}{1992}{{Koza}}{{}}}
\bibcite{Krizhevsky2012}{{62}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{Krotov2016}{{63}{2016}{{Krotov and Hopfield}}{{}}}
\bibcite{Krotov2020}{{64}{2020}{{Krotov and Hopfield}}{{}}}
\bibcite{Krueger2016}{{65}{2017}{{Krueger et~al.}}{{Krueger, Maharaj, Tiedemann, et~al.}}}
\bibcite{LevyGoldberg2014}{{66}{2014}{{Levy and Goldberg}}{{}}}
\bibcite{Loshchilov2019}{{67}{2019}{{Loshchilov and Hutter}}{{}}}
\bibcite{MacQueen1967}{{68}{1967}{{MacQueen}}{{}}}
\bibcite{Mamdani1975}{{69}{1975}{{Mamdani and Assilian}}{{}}}
\bibcite{MartinetzBerkovichSchulten1993}{{70}{1993}{{Martinetz et~al.}}{{Martinetz, Berkovich, and Schulten}}}
\bibcite{McCullochPitts1943}{{71}{1943}{{McCulloch and Pitts}}{{}}}
\bibcite{McEliece1987}{{72}{1987}{{McEliece et~al.}}{{McEliece, Posner, Rodemich, and Venkatesh}}}
\bibcite{Micchelli1986}{{73}{1986}{{Micchelli}}{{}}}
\bibcite{Mikolov2010}{{74}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi{\'a}t, Burget, {\v C}ernock{\'y}, and Khudanpur}}}
\bibcite{Mikolov2013}{{75}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{Mitchell1998}{{76}{1998}{{Mitchell}}{{}}}
\bibcite{Ogata2010}{{77}{2010}{{Ogata}}{{}}}
\bibcite{ParkSandberg1991}{{78}{1991}{{Park and Sandberg}}{{}}}
\bibcite{Pennington2014}{{79}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{Platt1999}{{80}{1999}{{Platt}}{{}}}
\bibcite{PoggioGirosi1990}{{81}{1990}{{Poggio and Girosi}}{{}}}
\bibcite{PooleMackworth2017}{{82}{2017}{{Poole and Mackworth}}{{}}}
\bibcite{Press2016}{{83}{2017}{{Press and Wolf}}{{}}}
\bibcite{Press2022ALiBi}{{84}{2022}{{Press et~al.}}{{Press, Smith, and Lewis}}}
\bibcite{Ramsauer2021}{{85}{2021}{{Ramsauer et~al.}}{{Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich, Adler, Gruber, Holzleitner, Pavlovic, Sandve, Deiseroth, and Hochreiter}}}
\bibcite{Risch1969}{{86}{1969}{{Risch}}{{}}}
\bibcite{Rosenblatt1958}{{87}{1958}{{Rosenblatt}}{{}}}
\bibcite{Rumelhart1986}{{88}{1986}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{RussellNorvig2021}{{89}{2021}{{Russell and Norvig}}{{}}}
\bibcite{SchreierScharf2010}{{90}{2010}{{Schreier and Scharf}}{{}}}
\bibcite{Semeniuta2016}{{91}{2016}{{Semeniuta et~al.}}{{Semeniuta, Severyn, and Barth}}}
\bibcite{Shazeer2017MoE}{{92}{2017}{{Shazeer et~al.}}{{Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean}}}
\bibcite{Storn1997}{{93}{1997}{{Storn and Price}}{{}}}
\bibcite{Su2021RoPE}{{94}{2021}{{Su et~al.}}{{Su, Lu, Pan, Wen, and Liu}}}
\bibcite{TakagiSugeno1985}{{95}{1985}{{Takagi and Sugeno}}{{}}}
\bibcite{Vaswani2017}{{96}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, ukasz Kaiser, and Polosukhin}}}
\bibcite{WidrowHoff1960}{{97}{1960}{{Widrow and Hoff}}{{}}}
\bibcite{WidrowStearns1985}{{98}{1985}{{Widrow and Stearns}}{{}}}
\bibcite{WillshawVonDerMalsburg1976}{{99}{1976}{{Willshaw and {von der Malsburg}}}{{}}}
\bibcite{YenLangari1999}{{100}{1999}{{Yen and Langari}}{{}}}
\bibcite{Zadeh1965}{{101}{1965}{{Zadeh}}{{}}}
\bibcite{Zadeh1975}{{102}{1975}{{Zadeh}}{{}}}
\bibcite{Zadeh1994}{{103}{1994}{{Zadeh}}{{}}}
\bibcite{Zadeh1997}{{104}{1997}{{Zadeh}}{{}}}
\bibcite{Zitzler2002}{{105}{2002}{{Zitzler et~al.}}{{Zitzler, Laumanns, and Thiele}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Linear Systems Primer}{413}{appendix.A}\protected@file@percent }
\newlabel{app:linear_systems}{{A}{413}{Linear Systems Primer}{appendix.A}{}}
\newlabel{app:linear_systems@cref}{{[appendix][1][2147483647]A}{[1][413][]413}}
\newlabel{eq:state_space}{{A.2}{414}{}{equation.A.2}{}}
\newlabel{eq:state_space@cref}{{[equation][2][2147483647,1]A.2}{[1][414][]414}}
\@writefile{toc}{\contentsline {paragraph}{Homogeneous solution.}{415}{equation.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forced response.}{415}{equation.A.5}\protected@file@percent }
\newlabel{eq:state_solution}{{A.6}{415}{Forced response}{equation.A.6}{}}
\newlabel{eq:state_solution@cref}{{[equation][6][2147483647,1]A.6}{[1][415][]415}}
\@writefile{toc}{\contentsline {paragraph}{Transfer function.}{415}{equation.A.6}\protected@file@percent }
\newlabel{eq:transfer_function}{{A.10}{415}{Transfer function}{equation.A.10}{}}
\newlabel{eq:transfer_function@cref}{{[equation][10][2147483647,1]A.10}{[1][415][]415}}
\citation{Kailath1980}
\citation{Chen1999LinearSystemTheoryDesign}
\citation{Ogata2010}
\@writefile{toc}{\contentsline {section}{\numberline {B}Kernel Methods and Support Vector Machines}{416}{appendix.B}\protected@file@percent }
\newlabel{app:kernels}{{B}{416}{Kernel Methods and Support Vector Machines}{appendix.B}{}}
\newlabel{app:kernels@cref}{{[appendix][2][2147483647]B}{[1][416][]416}}
\newlabel{eq:krr}{{B.1}{416}{}{equation.B.1}{}}
\newlabel{eq:krr@cref}{{[equation][1][2147483647,2]B.1}{[1][416][]416}}
\citation{Cortes1995}
\newlabel{eq:svm_kernel_decision}{{B.4}{417}{}{equation.B.4}{}}
\newlabel{eq:svm_kernel_decision@cref}{{[equation][4][2147483647,2]B.4}{[1][417][]417}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Course Logistics}{418}{appendix.C}\protected@file@percent }
\newlabel{app:course_logistics}{{C}{418}{Course Logistics}{appendix.C}{}}
\newlabel{app:course_logistics@cref}{{[appendix][3][2147483647]C}{[1][417][]418}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Using this book in ECE\nobreakspace  {}657}{418}{subsection.C.1}\protected@file@percent }
\newlabel{app:ece657_using_this_book}{{C.1}{418}{Using this book in ECE~657}{subsection.C.1}{}}
\newlabel{app:ece657_using_this_book@cref}{{[subappendix][1][2147483647,3]C.1}{[1][418][]418}}
\gdef \@abspage@last{419}

\contentsline {figure}{\numberline {1}{\ignorespaces Schematic: Roadmap (core supervised path; SOM/fuzzy; optimization/evolutionary).\relax }}{45}{figure.caption.1}%
\contentsline {figure}{\numberline {2}{\ignorespaces Schematic: Transformation tree for the running example integral; badges \textbf {[S]}/\textbf {[H]} mark safe vs.\ heuristic moves; the dashed branch mirrors the sine substitution.\relax }}{56}{figure.caption.2}%
\contentsline {figure}{\numberline {3}{\ignorespaces Schematic: Underfitting and overfitting as a function of model complexity. Training error typically decreases with complexity, while validation error often has a U-shape. Regularization and model selection aim to operate near the minimum of the validation curve.\relax }}{67}{figure.caption.4}%
\contentsline {figure}{\numberline {4}{\ignorespaces Schematic: Why L1 promotes sparsity. Minimizing loss subject to an L2 constraint tends to hit a smooth boundary; an L1 constraint has corners aligned with coordinate axes, so tangency often occurs at a point where some coordinates are exactly zero.\relax }}{70}{figure.caption.5}%
\contentsline {figure}{\numberline {5}{\ignorespaces Schematic: A typical lasso path as the regularization strength increases. Coefficients shrink, and some become exactly zero, yielding sparse models.\relax }}{70}{figure.caption.6}%
\contentsline {figure}{\numberline {6}{\ignorespaces Schematic: Classification losses as functions of the signed margin z.\relax }}{72}{figure.caption.8}%
\contentsline {figure}{\numberline {7}{\ignorespaces Schematic: Regression losses versus prediction error. The Huber loss transitions from quadratic to linear to reduce sensitivity to outliers.\relax }}{72}{figure.caption.9}%
\contentsline {figure}{\numberline {8}{\ignorespaces Schematic: Dataset partitioning into training, validation, and test segments. Any resampling scheme should preserve disjoint evaluation data; when classes are imbalanced, shuffle within strata so each split reflects the overall class mix.\relax }}{74}{figure.caption.10}%
\contentsline {figure}{\numberline {9}{\ignorespaces Schematic: Mini ERM pipeline (split once, iterate train/validate, then test only the best model on the held-out set).\relax }}{74}{figure.caption.11}%
\contentsline {figure}{\numberline {10}{\ignorespaces Schematic: Learning curves reveal under/overfitting: the validation curve flattens while additional data continue to decrease training error only marginally. A shaded patience window marks when early stopping would halt if no validation improvement occurs.\relax }}{75}{figure.caption.12}%
\contentsline {figure}{\numberline {11}{\ignorespaces Calibration and capacity diagnostics (reliability and double descent)}}{76}{figure.caption.13}%
\contentsline {figure}{\numberline {12}{\ignorespaces Schematic: Ridge regularization shrinks parameter norms as the penalty strength increases.\relax }}{77}{figure.caption.14}%
\contentsline {figure}{\numberline {13}{\ignorespaces Schematic: Synthetic binary dataset.}}{84}{figure.caption.15}%
\contentsline {figure}{\numberline {14}{\ignorespaces Schematic: Bayes-optimal boundary for two Gaussian classes with equal covariances and similar priors (LDA setting), which yields a linear separator. Unequal covariances produce a quadratic boundary. We place the boundary near the equal-posterior line (vertical, pink); left/right regions correspond to predicted classes R0 and R1.\relax }}{84}{figure.caption.16}%
\contentsline {figure}{\numberline {15}{\ignorespaces Schematic: The sigmoid maps logits to probabilities (left). The binary cross\hyp {}entropy (negative log\hyp {}likelihood) penalizes confident wrong predictions sharply (middle). Regularization typically shrinks parameter norms as the penalty strength increases (right).\relax }}{87}{figure.caption.17}%
\contentsline {figure}{\numberline {16}{\ignorespaces Schematic: Gradient-descent iterates contracting toward the minimizer of a convex quadratic cost. Ellipses are level sets; arrows show the ``steepest descent along contours'' direction.\relax }}{87}{figure.caption.18}%
\contentsline {figure}{\numberline {17}{\ignorespaces Schematic: Illustrative logistic-regression boundary. The dashed line marks the linear decision boundary at probability 0.5; labeled contours show how the posterior varies smoothly with margin, enabling calibrated decisions and adjustable thresholds.\relax }}{88}{figure.caption.19}%
\contentsline {figure}{\numberline {18}{\ignorespaces Schematic: MAP estimates interpolate between the prior mean and the data-driven MLE. As the sample size grows, the MAP curve approaches the true mean.\relax }}{89}{figure.caption.20}%
\contentsline {figure}{\numberline {19}{\ignorespaces Schematic: ROC and PR curves with an explicit operating point. Left: ROC curve with iso-cost lines; right: PR curve with a class-prevalence baseline and iso-F1 contours. Together they visualize threshold trade-offs and calibration quality.\relax }}{90}{figure.caption.21}%
\contentsline {figure}{\numberline {20}{\ignorespaces Schematic: Confusion matrix for a three-class classifier; diagonals dominate, indicating strong accuracy with modest confusion between classes B and C.\relax }}{90}{figure.caption.22}%
\contentsline {figure}{\numberline {21}{\ignorespaces Schematic: Perceptron geometry. Points on either side of the separating hyperplane receive different labels, and the signed distance to the boundary controls both the class prediction and the magnitude of the update during learning. Compare to \Cref {fig:lec2-logistic-boundary} in \Cref {chap:logistic}: both share a linear separator, but logistic smooths the boundary into calibrated probabilities.\relax }}{105}{figure.caption.24}%
\contentsline {figure}{\numberline {22}{\ignorespaces Schematic: The minimal neural network used in this chapter is a two-neuron chain. The first unit produces an intermediate signal, and the second unit maps that signal to the final output.\relax }}{110}{figure.caption.25}%
\contentsline {figure}{\numberline {23}{\ignorespaces Schematic: Think of performance as a surface over the weights. Gradient descent moves in one vector step (blue), whereas coordinate-wise updates can zig-zag (orange).\relax }}{113}{figure.caption.26}%
\contentsline {figure}{\numberline {24}{\ignorespaces Schematic: Hard thresholds block gradient-based learning because the derivative is zero almost everywhere. A smooth activation like the sigmoid provides informative derivatives across a wide range of inputs.\relax }}{113}{figure.caption.27}%
\contentsline {figure}{\numberline {25}{\ignorespaces Computational graph for backpropagation (reverse-mode AD)}}{129}{figure.caption.28}%
\contentsline {figure}{\numberline {26}{\ignorespaces Schematic: Forward (blue) and backward (orange) flows for a two-layer MLP. The cached activations and the layerwise error terms (deltas) are exactly the quantities carried along these arrows.\relax }}{138}{figure.caption.29}%
\contentsline {figure}{\numberline {27}{\ignorespaces Schematic: Canonical activation functions on a common axis. Solid curves show the activation; dashed curves show its derivative.\relax }}{140}{figure.caption.30}%
\contentsline {figure}{\numberline {28}{\ignorespaces Schematic: How the width parameter (sigma) influences decision boundaries on a 2D toy dataset. Too-large sigma underfits, intermediate sigma captures the boundary, too-small sigma overfits with fragmented regions. Use \Cref {chap:supervised}'s validation curves to pick model size and regularization.\relax }}{144}{figure.caption.31}%
\contentsline {figure}{\numberline {29}{\ignorespaces Schematic: RBFN architecture. Inputs feed fixed radial units parameterized by centers and widths; a linear readout with weights (and bias) is trained by a regression or classification loss. Only the output weights are typically learned; centers/widths come from k-means or spacing heuristics.\relax }}{145}{figure.caption.32}%
\contentsline {figure}{\numberline {30}{\ignorespaces Schematic: Localized Gaussian basis functions (dashed) and their weighted sum (solid). Overlapping bumps allow RBF networks to interpolate complex signals smoothly.\relax }}{146}{figure.caption.33}%
\contentsline {figure}{\numberline {31}{\ignorespaces Schematic: Center placement and overlap. Top: K-means prototypes roughly tile the data manifold, giving even overlap; bottom: random centers can leave gaps or excessive overlap, influencing the width (sigma) choice and conditioning.\relax }}{147}{figure.caption.34}%
\contentsline {figure}{\numberline {32}{\ignorespaces Schematic: Primal (finite basis) vs.\ dual (kernel ridge) viewpoints. Using as many centers as data points recovers the dual form; using fewer centers corresponds to a Nystr\"om approximation. The same trade-off appears in kernel methods through the choice of kernel and effective rank.\relax }}{154}{figure.caption.35}%
\contentsline {figure}{\numberline {33}{\ignorespaces Schematic: RBFN decision boundary on the XOR toy for a model with 4 centers, width sigma = 0.8, and ridge lambda = 1e-3. Shading indicates the predicted class under a 0.5 threshold; the black contour marks the 0.5 boundary. Training points are overlaid (class 0: open circles; class 1: filled squares). See \Cref {fig:rbf_sigma_sweep} for how sigma changes this boundary.\relax }}{156}{figure.caption.36}%
\contentsline {figure}{\numberline {34}{\ignorespaces Schematic: Learning-rate scheduling intuition. On a smooth objective (left), large initial steps quickly cover ground and roughly align prototypes, while a decaying step-size refines the solution near convergence. Right: common exponential and multiplicative decays used in SOM training.\relax }}{162}{figure.caption.37}%
\contentsline {figure}{\numberline {35}{\ignorespaces Schematic: Classical MDS intuition. Projecting a cube onto a plane via an orthogonal map yields a square (left), whereas an oblique projection along a body diagonal produces a hexagon (right). The local adjacency of vertices is preserved even though metric structure is distorted.\relax }}{165}{figure.caption.38}%
\contentsline {figure}{\numberline {36}{\ignorespaces Schematic: Gaussian neighborhood weights in SOM training. Early iterations use a broad kernel so many neighbors adapt; later iterations shrink the neighborhood width sigma(t) so only units near the BMU update.\relax }}{170}{figure.caption.39}%
\contentsline {figure}{\numberline {37}{\ignorespaces Schematic: Bias--variance trade-off when sweeping SOM capacity (number of units or kernel width). The optimum appears near the knee where bias and variance intersect.\relax }}{177}{figure.caption.40}%
\contentsline {figure}{\numberline {38}{\ignorespaces Schematic: Regularization smooths the loss surface. Coupling neighboring prototypes (right) yields wider, flatter basins than the jagged unregularized landscape (left).\relax }}{178}{figure.caption.41}%
\contentsline {figure}{\numberline {39}{\ignorespaces Schematic: Quantization error combined with an entropy-style regularizer (modern SOM variant; for example, a negative sum of p log p over unit usage). Valleys arise when prototypes cover the space evenly; ridges highlight collapse or poor topological preservation.\relax }}{179}{figure.caption.42}%
\contentsline {figure}{\numberline {40}{\ignorespaces Schematic: Validation curves used to identify an early\hyp {}stopping knee. When both quantization and topographic errors flatten (shaded band), further training risks map drift.\relax }}{180}{figure.caption.43}%
\contentsline {figure}{\numberline {41}{\ignorespaces Schematic: Voronoi-like regions induced by SOM prototypes (left) and the corresponding softmax confidence after shrinking the neighborhood kernel (right). Softer updates blur the decision frontiers and reduce jagged mappings between adjacent neurons.\relax }}{181}{figure.caption.44}%
\contentsline {figure}{\numberline {42}{\ignorespaces Schematic: Left: a 5-by-5 SOM lattice with best matching unit (blue) and neighbors within the Gaussian-kernel radius (green). Right: a toy U-Matrix (colormap chosen to remain interpretable in grayscale) showing average distances between neighboring codebook vectors; higher distances indicate likely cluster boundaries.\relax }}{182}{figure.caption.45}%
\contentsline {figure}{\numberline {43}{\ignorespaces Schematic: Component planes for three features on a trained SOM (toy data). Each plane maps one feature's value across the map; aligned bright/dark regions across planes reveal correlated features, complementing the U-Matrix in \Cref {fig:lec5-som-lattice-umatrix}. Interpret brightness comparatively within a plane rather than as an absolute calibrated scale.\relax }}{183}{figure.caption.46}%
\contentsline {figure}{\numberline {44}{\ignorespaces Schematic: SOM lattice with the best-matching unit (BMU) highlighted in blue and a dashed neighborhood radius indicating which prototype vectors receive cooperative updates.\relax }}{184}{figure.caption.47}%
\contentsline {figure}{\numberline {45}{\ignorespaces Schematic: Hopfield energy decreases monotonically under asynchronous updates. Starting from a noisy probe state s(0), successive single-neuron flips move downhill until the stored memory s(2) is recovered.\relax }}{197}{figure.caption.49}%
\contentsline {figure}{\numberline {46}{\ignorespaces Dropout effect on training/validation curves (validation flattening)}}{217}{figure.caption.50}%
\contentsline {figure}{\numberline {47}{\ignorespaces Schematic: Batch normalization transforms per-channel activations toward zero mean and unit variance prior to the learned affine re-scaling, stabilizing training.\relax }}{218}{figure.caption.51}%
\contentsline {figure}{\numberline {48}{\ignorespaces Schematic: Representative training curves for SGD with momentum versus Adam on the same CNN.\relax }}{219}{figure.caption.52}%
\contentsline {figure}{\numberline {49}{\ignorespaces Schematic: Decision boundaries for logistic regression (left) versus a shallow MLP (right). Linear models carve a single hyperplane, whereas hidden units can warp the boundary to follow non-convex manifolds such as the moons dataset.\relax }}{226}{figure.caption.53}%
\contentsline {figure}{\numberline {50}{\ignorespaces Schematic: Binary cross-entropy geometry (left), effect of learning-rate schedules on loss (middle), and the typical training/validation divergence that motivates early stopping (right).\relax }}{227}{figure.caption.54}%
\contentsline {figure}{\numberline {51}{\ignorespaces Schematic: Unrolling an RNN reveals repeated application of the same parameters across time steps. This view motivates backpropagation through time (BPTT), which accumulates gradients through every copy before updating the shared weights.\relax }}{233}{figure.caption.55}%
\contentsline {figure}{\numberline {52}{\ignorespaces Schematic: Backpropagation through time (BPTT): unrolled forward pass (black) and backward gradients (pink) through time.\relax }}{237}{figure.caption.56}%
\contentsline {figure}{\numberline {53}{\ignorespaces Schematic: Vanishing (blue) versus exploding (orange) gradients on a log scale. The gray strip highlights the stability band; the inset reminds readers that repeated Jacobian products either shrink gradients (thin blue arrows) or amplify them (thick orange arrows).\relax }}{238}{figure.caption.57}%
\contentsline {figure}{\numberline {54}{\ignorespaces Schematic: Gradient norms (left) explode without clipping (orange) but remain bounded when the global norm is clipped at tau (green). Training loss (right) stabilizes as a result.\relax }}{239}{figure.caption.58}%
\contentsline {figure}{\numberline {55}{\ignorespaces Schematic: Teacher forcing vs.\ inference in a sequence-to-sequence decoder. Gold arrows show supervised targets; orange arrows highlight autoregressive feedback that motivates scheduled sampling.\relax }}{240}{figure.caption.59}%
\contentsline {figure}{\numberline {56}{\ignorespaces Schematic: Long Short-Term Memory (LSTM) cell \citep {Hochreiter1997,Gers2000}.\relax }}{241}{figure.caption.60}%
\contentsline {figure}{\numberline {57}{\ignorespaces Schematic: Gated Recurrent Unit (GRU) cell \citep {Cho2014}.\relax }}{241}{figure.caption.61}%
\contentsline {figure}{\numberline {58}{\ignorespaces Schematic: Attention heatmap for a translation model. Rows are target tokens (decoder steps) and columns are source tokens (encoder positions). Each cell is an attention weight; the dot in each row marks the source position receiving the most attention.\relax }}{243}{figure.caption.62}%
\contentsline {figure}{\numberline {59}{\ignorespaces Schematic: Toy 2D projection of word embeddings showing neighboring clusters (countries vs. capitals). Light hulls highlight clusters; arrows show that country-to-capital displacement vectors align, a visual check on analogy structure.\relax }}{250}{figure.caption.63}%
\contentsline {figure}{\numberline {60}{\ignorespaces Schematic: Reference schematic for the Transformer. Left: scaled dot-product attention. Center: multi-head concatenation with an output projection. Right: pre-LN encoder block combining attention, FFN, and residual connections; a post-LN variant simply moves each LayerNorm after its residual add (dotted alternative, not shown).\relax }}{259}{figure.caption.64}%
\contentsline {figure}{\numberline {61}{\ignorespaces Schematic: Transformer micro-views. Left: positional encodings (sinusoidal/rotary) add order information. Center: KV cache stores past keys/values so decoding a new token reuses prior context. Right: LoRA inserts low-rank adapters (B A) on top of a frozen weight matrix W for parameter-efficient tuning.\relax }}{260}{figure.caption.65}%
\contentsline {figure}{\numberline {62}{\ignorespaces Schematic: Attention masks visualized as heatmaps (queries on rows, keys on columns). Left: padding mask zeroes attention into padded positions of a shorter sequence in a packed batch. Right: causal mask enforces autoregressive flow by blocking attention to future tokens.\relax }}{264}{figure.caption.66}%
\contentsline {figure}{\numberline {63}{\ignorespaces Schematic: Analogy geometry in embedding space. The classic offset ``v(king) - v(man) + v(woman) approx v(queen)'' forms a parallelogram; a similar gender direction also moves ``doctor'' toward ``nurse.'' Visualizing these displacement vectors (solid vs. dashed) makes the shared relational direction explicit. Points are shown after a 2D PCA projection, so directions are approximate rather than exact.\relax }}{281}{figure.caption.67}%
\contentsline {figure}{\numberline {64}{\ignorespaces Schematic: Trapezoidal membership functions for grades C and B with the overlapping region shaded. Scores near 78--82 partially satisfy both grade definitions.\relax }}{312}{figure.caption.70}%
\contentsline {figure}{\numberline {65}{\ignorespaces Overlapping membership functions for Small/Medium/Large labels}}{314}{figure.caption.71}%
\contentsline {figure}{\numberline {66}{\ignorespaces Schematic: Fuzzy AND surfaces comparing minimum versus product t\hyp {}norms; analogous OR surfaces show similar differences. Choices here influence rule aggregation in Chapter\nobreakspace {}\ref {chap:fuzzyinference}.\relax }}{323}{figure.caption.72}%
\contentsline {figure}{\numberline {67}{\ignorespaces Schematic: End-to-end fuzzy inference example. (A) Consequent membership functions with clipping levels from firing strengths at T = 27 deg C. (B) Aggregated output set (max of truncated consequents) and a centroid marker near s* approx 0.58.\relax }}{336}{figure.caption.74}%
\contentsline {figure}{\numberline {68}{\ignorespaces Schematic: Mapping a fuzzy set through the function ``y = x-squared''. The membership at an output value y is the supremum over all pre-images x that map to y; shared images such as x = +/-1 map to y = 1 using the maximum membership.\relax }}{342}{figure.caption.75}%
\contentsline {figure}{\numberline {69}{\ignorespaces Schematic: Alpha-cuts under the non-monotone map ``y = x-squared''. A symmetric triangular fuzzy set on X maps to a right-skewed fuzzy set on Y. Each alpha-cut on A splits into two intervals whose images union to the output alpha-cut.\relax }}{345}{figure.caption.76}%
\contentsline {figure}{\numberline {70}{\ignorespaces Schematic: Illustrative fuzzy relation table (left) together with its projections onto the error universe (middle) and the rate-of-change universe (right). These are the exact quantities used in the running thermostat example before composing rules.\relax }}{348}{figure.caption.78}%
\contentsline {figure}{\numberline {71}{\ignorespaces Schematic: Evolutionary micro-operators. Left: fitter individuals get sampled more often (roulette/tournament). Middle: crossover splices parents by a mask (one-point shown). Right: constraint handling routes offspring through repair/penalty/feasibility before evaluation.\relax }}{371}{figure.caption.79}%
\contentsline {figure}{\numberline {72}{\ignorespaces Schematic: Illustrative GA run showing the best and mean normalized fitness over 50 generations. Flat regions motivate ``no improvement'' stopping rules, while steady separation between best and mean indicates ongoing selection pressure.\relax }}{384}{figure.caption.80}%
\contentsline {figure}{\numberline {73}{\ignorespaces Schematic: GA flowchart showing the iterative process: initialization leads to fitness evaluation and a termination check. If not terminated, the algorithm proceeds through selection, crossover, mutation, and replacement, which then feeds the next generation's fitness evaluation.\relax }}{385}{figure.caption.81}%
\contentsline {figure}{\numberline {74}{\ignorespaces Schematic: Sample Pareto front for two objectives. NSGA-II keeps all non-dominated points (blue) while pushing dominated solutions (orange) toward the front via selection, yielding a spread of trade-offs in one run.\relax }}{396}{figure.caption.83}%
\contentsline {figure}{\numberline {75}{\ignorespaces Map of model families}}{402}{figure.caption.84}%

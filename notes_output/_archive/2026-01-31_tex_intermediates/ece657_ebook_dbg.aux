\relax 
\providecommand\hyper@newdestlabel[2]{}
\ifx\rEfLiNK\UnDef\gdef \rEfLiNK#1#2{#2}\fi
\newlabel{chap:intro}{{\rEfLiNK{x6-80001}{\csname :autoref\endcsname{section}1}}{\rEfLiNK{x6-80001}{\csname :autoref\endcsname{section}19}}{\rEfLiNK{x6-80001}{\csname :autoref\endcsname{section}About This Book}}{section.1}{}}
\newlabel{chap:intro@cref}{{[section][1][]1}{[1][19][]19}{}{}{}}
\citation{Risch1969}
\citation{RussellNorvig2021}
\citation{PooleMackworth2017}
\citation{Brooks1986,Arkin1998}
\newlabel{par:intelligent-systems}{{\rEfLiNK{x6-180001.3}{\csname :autoref\endcsname{subsection}1.3}}{\rEfLiNK{x6-180001.3}{\csname :autoref\endcsname{subsection}27}}{\rEfLiNK{x6-180001.3}{\csname :autoref\endcsname{subsection}Intelligent Systems}}{subsection.1}{}}
\newlabel{par:intelligent-systems@cref}{{[subsection][3][1]1.3}{[1][27][]27}{}{}{}}
\newlabel{eq:intelligent_mapping}{{\rEfLiNK{x6-30001r2}{\csname :autoref\endcsname{equation}1.2}}{\rEfLiNK{x6-30001r2}{\csname :autoref\endcsname{equation}39}}{\rEfLiNK{x6-30001r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:intelligent_mapping@cref}{{[equation][2][1]1.2}{[1][39][]39}{}{}{}}
\newlabel{subsec:levels}{{\rEfLiNK{x6-360001.7}{\csname :autoref\endcsname{subsection}1.7}}{\rEfLiNK{x6-360001.7}{\csname :autoref\endcsname{subsection}43}}{\rEfLiNK{x6-360001.7}{\csname :autoref\endcsname{subsection}Levels, Meta-cognition, and Safety}}{subsection.1}{}}
\newlabel{subsec:levels@cref}{{[subsection][7][1]1.7}{[1][43][]43}{}{}{}}
\newlabel{fig:roadmap}{{\rEfLiNK{x6-41001r1}{\csname :autoref\endcsname{figure}1}}{\rEfLiNK{x6-41001r1}{\csname :autoref\endcsname{figure}47}}{\rEfLiNK{x6-41001r1.9}{\csname :autoref\endcsname{figure}Schematic: Roadmap (core supervised path; SOM/fuzzy; optimization/evolutionary).}}{figure.1}{}}
\newlabel{fig:roadmap@cref}{{[figure][1][]1}{[1][47][]47}{}{}{}}
\newlabel{chap:symbolic}{{\rEfLiNK{x7-450002}{\csname :autoref\endcsname{section}2}}{\rEfLiNK{x7-450002}{\csname :autoref\endcsname{section}54}}{\rEfLiNK{x7-450002}{\csname :autoref\endcsname{section}Symbolic Integration and Problem-Solving Strategies}}{section.1}{}}
\newlabel{chap:symbolic@cref}{{[section][2][]2}{[1][54][]54}{}{}{}}
\citation{Risch1969}
\newlabel{eq:original_integral}{{\rEfLiNK{x7-57001r1}{\csname :autoref\endcsname{equation}2.1}}{\rEfLiNK{x7-57001r1}{\csname :autoref\endcsname{equation}70}}{\rEfLiNK{x7-57001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:original_integral@cref}{{[equation][1][2]2.1}{[1][70][]70}{}{}{}}
\newlabel{eq:sec4_integral}{{\rEfLiNK{x7-58001r2}{\csname :autoref\endcsname{equation}2.2}}{\rEfLiNK{x7-58001r2}{\csname :autoref\endcsname{equation}72}}{\rEfLiNK{x7-58001r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sec4_integral@cref}{{[equation][2][2]2.2}{[1][72][]72}{}{}{}}
\citation{Bronstein2005,Risch1969}
\newlabel{fig:lec3_transform_tree}{{\rEfLiNK{x7-66001r2}{\csname :autoref\endcsname{figure}2}}{\rEfLiNK{x7-66001r2}{\csname :autoref\endcsname{figure}79}}{\rEfLiNK{x7-66001r}{\csname :autoref\endcsname{figure}Schematic: Transformation tree for the running example integral; badges \textbf  {[S]}/\textbf  {[H]} mark safe vs.\relax \leavevmode \special {t4ht@+&{35}x00A0{59}}xheuristic moves; the dashed branch mirrors the sine substitution.}}{figure.1}{}}
\newlabel{fig:lec3_transform_tree@cref}{{[figure][2][]2}{[1][79][]79}{}{}{}}
\newlabel{chap:supervised}{{\rEfLiNK{x8-790003}{\csname :autoref\endcsname{section}3}}{\rEfLiNK{x8-790003}{\csname :autoref\endcsname{section}101}}{\rEfLiNK{x8-790003}{\csname :autoref\endcsname{section}Supervised Learning Foundations}}{section.1}{}}
\newlabel{chap:supervised@cref}{{[section][3][]3}{[1][101][]101}{}{}{}}
\newlabel{fig:lec1_fit_regimes}{{\rEfLiNK{x8-84001r3}{\csname :autoref\endcsname{figure}3}}{\rEfLiNK{x8-84001r3}{\csname :autoref\endcsname{figure}111}}{\rEfLiNK{x8-84001r}{\csname :autoref\endcsname{figure}Schematic: Underfitting and overfitting as a function of model complexity. Training error typically decreases with complexity, while validation error often has a U-shape. Regularization and model selection aim to operate near the minimum of the validation curve.}}{figure.1}{}}
\newlabel{fig:lec1_fit_regimes@cref}{{[figure][3][]3}{[1][111][]111}{}{}{}}
\newlabel{fig:lec1_l1_l2_geometry}{{\rEfLiNK{x8-86001r4}{\csname :autoref\endcsname{figure}4}}{\rEfLiNK{x8-86001r4}{\csname :autoref\endcsname{figure}118}}{\rEfLiNK{x8-86001r}{\csname :autoref\endcsname{figure}Schematic: Why L1 promotes sparsity. Minimizing loss subject to an L2 constraint tends to hit a smooth boundary; an L1 constraint has corners aligned with coordinate axes, so tangency often occurs at a point where some coordinates are exactly zero.}}{figure.1}{}}
\newlabel{fig:lec1_l1_l2_geometry@cref}{{[figure][4][]4}{[1][118][]118}{}{}{}}
\newlabel{fig:lec1_lasso_path}{{\rEfLiNK{x8-86002r5}{\csname :autoref\endcsname{figure}5}}{\rEfLiNK{x8-86002r5}{\csname :autoref\endcsname{figure}121}}{\rEfLiNK{x8-86002r}{\csname :autoref\endcsname{figure}Schematic: A typical lasso path as the regularization strength increases. Coefficients shrink, and some become exactly zero, yielding sparse models.}}{figure.1}{}}
\newlabel{fig:lec1_lasso_path@cref}{{[figure][5][]5}{[1][121][]121}{}{}{}}
\newlabel{fig:lec1_class_losses}{{\rEfLiNK{x8-88004r6}{\csname :autoref\endcsname{figure}6}}{\rEfLiNK{x8-88004r6}{\csname :autoref\endcsname{figure}130}}{\rEfLiNK{x8-88004r3.5}{\csname :autoref\endcsname{figure}Schematic: Classification losses as functions of the signed margin z.}}{figure.1}{}}
\newlabel{fig:lec1_class_losses@cref}{{[figure][6][]6}{[1][130][]130}{}{}{}}
\newlabel{fig:lec1_reg_losses}{{\rEfLiNK{x8-88005r7}{\csname :autoref\endcsname{figure}7}}{\rEfLiNK{x8-88005r7}{\csname :autoref\endcsname{figure}133}}{\rEfLiNK{x8-88005r3.5}{\csname :autoref\endcsname{figure}Schematic: Regression losses versus prediction error. The Huber loss transitions from quadratic to linear to reduce sensitivity to outliers.}}{figure.1}{}}
\newlabel{fig:lec1_reg_losses@cref}{{[figure][7][]7}{[1][133][]133}{}{}{}}
\newlabel{sec:lec1_model_selection}{{\rEfLiNK{x8-890003.6}{\csname :autoref\endcsname{subsection}3.6}}{\rEfLiNK{x8-890003.6}{\csname :autoref\endcsname{subsection}134}}{\rEfLiNK{x8-890003.6}{\csname :autoref\endcsname{subsection}Model Selection, Splits, and Learning Curves}}{subsection.1}{}}
\newlabel{sec:lec1_model_selection@cref}{{[subsection][6][3]3.6}{[1][134][]134}{}{}{}}
\newlabel{fig:lec1_splits}{{\rEfLiNK{x8-89001r8}{\csname :autoref\endcsname{figure}8}}{\rEfLiNK{x8-89001r8}{\csname :autoref\endcsname{figure}138}}{\rEfLiNK{x8-89001r3.6}{\csname :autoref\endcsname{figure}Schematic: Dataset partitioning into training, validation, and test segments. Any resampling scheme should preserve disjoint evaluation data; when classes are imbalanced, shuffle within strata so each split reflects the overall class mix.}}{figure.1}{}}
\newlabel{fig:lec1_splits@cref}{{[figure][8][]8}{[1][138][]138}{}{}{}}
\newlabel{fig:lec1_pipeline}{{\rEfLiNK{x8-89002r9}{\csname :autoref\endcsname{figure}9}}{\rEfLiNK{x8-89002r9}{\csname :autoref\endcsname{figure}141}}{\rEfLiNK{x8-89002r3.6}{\csname :autoref\endcsname{figure}Schematic: Mini ERM pipeline (split once, iterate train/validate, then test only the best model on the held-out set).}}{figure.1}{}}
\newlabel{fig:lec1_pipeline@cref}{{[figure][9][]9}{[1][141][]141}{}{}{}}
\newlabel{fig:lec1_learning_curves}{{\rEfLiNK{x8-89003r10}{\csname :autoref\endcsname{figure}10}}{\rEfLiNK{x8-89003r10}{\csname :autoref\endcsname{figure}144}}{\rEfLiNK{x8-89003r3.6}{\csname :autoref\endcsname{figure}Schematic: Learning curves reveal under/overfitting: the validation curve flattens while additional data continue to decrease training error only marginally. A shaded patience window marks when early stopping would halt if no validation improvement occurs.}}{figure.1}{}}
\newlabel{fig:lec1_learning_curves@cref}{{[figure][10][]10}{[1][144][]144}{}{}{}}
\citation{Belkin2019}
\citation{Kaplan2020,Hoffmann2022}
\newlabel{fig:lec1-calibration-double-descent}{{\rEfLiNK{x8-89004r11}{\csname :autoref\endcsname{figure}11}}{\rEfLiNK{x8-89004r11}{\csname :autoref\endcsname{figure}153}}{\rEfLiNK{x8-89004r3.6}{\csname :autoref\endcsname{figure}Calibration and capacity diagnostics (reliability and double descent)}}{figure.1}{}}
\newlabel{fig:lec1-calibration-double-descent@cref}{{[figure][11][]11}{[1][153][]153}{}{}{}}
\newlabel{fig:lec1_ridge}{{\rEfLiNK{x8-89005r12}{\csname :autoref\endcsname{figure}12}}{\rEfLiNK{x8-89005r12}{\csname :autoref\endcsname{figure}156}}{\rEfLiNK{x8-89005r3.6}{\csname :autoref\endcsname{figure}Schematic: Ridge regularization shrinks parameter norms as the penalty strength increases.}}{figure.1}{}}
\newlabel{fig:lec1_ridge@cref}{{[figure][12][]12}{[1][156][]156}{}{}{}}
\newlabel{sec:linear_regression_closed}{{\rEfLiNK{x8-900003.7}{\csname :autoref\endcsname{subsection}3.7}}{\rEfLiNK{x8-900003.7}{\csname :autoref\endcsname{subsection}157}}{\rEfLiNK{x8-900003.7}{\csname :autoref\endcsname{subsection}Linear regression: a first full case study}}{subsection.1}{}}
\newlabel{sec:linear_regression_closed@cref}{{[subsection][7][3]3.7}{[1][157][]157}{}{}{}}
\newlabel{eq:linear_model}{{\rEfLiNK{x8-91001r8}{\csname :autoref\endcsname{equation}3.8}}{\rEfLiNK{x8-91001r8}{\csname :autoref\endcsname{equation}158}}{\rEfLiNK{x8-91001r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:linear_model@cref}{{[equation][8][3]3.8}{[1][158][]158}{}{}{}}
\newlabel{chap:logistic}{{\rEfLiNK{x9-990004}{\csname :autoref\endcsname{section}4}}{\rEfLiNK{x9-990004}{\csname :autoref\endcsname{section}167}}{\rEfLiNK{x9-990004}{\csname :autoref\endcsname{section}Classification and Logistic Regression}}{section.1}{}}
\newlabel{chap:logistic@cref}{{[section][4][]4}{[1][167][]167}{}{}{}}
\newlabel{eq:bayes_theorem}{{\rEfLiNK{x9-102001r1}{\csname :autoref\endcsname{equation}4.1}}{\rEfLiNK{x9-102001r1}{\csname :autoref\endcsname{equation}172}}{\rEfLiNK{x9-102001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:bayes_theorem@cref}{{[equation][1][4]4.1}{[1][172][]172}{}{}{}}
\newlabel{fig:lec1_dataset}{{\rEfLiNK{x9-104001r13}{\csname :autoref\endcsname{figure}13}}{\rEfLiNK{x9-104001r13}{\csname :autoref\endcsname{figure}176}}{\rEfLiNK{x9-104001r}{\csname :autoref\endcsname{figure}Schematic: Synthetic binary dataset.}}{figure.1}{}}
\newlabel{fig:lec1_dataset@cref}{{[figure][13][]13}{[1][176][]176}{}{}{}}
\newlabel{fig:lec1_bayes}{{\rEfLiNK{x9-104002r14}{\csname :autoref\endcsname{figure}14}}{\rEfLiNK{x9-104002r14}{\csname :autoref\endcsname{figure}179}}{\rEfLiNK{x9-104002r}{\csname :autoref\endcsname{figure}Schematic: Bayes-optimal boundary for two Gaussian classes with equal covariances and similar priors (LDA setting), which yields a linear separator. Unequal covariances produce a quadratic boundary. We place the boundary near the equal-posterior line (vertical, pink); left/right regions correspond to predicted classes R0 and R1.}}{figure.1}{}}
\newlabel{fig:lec1_bayes@cref}{{[figure][14][]14}{[1][179][]179}{}{}{}}
\newlabel{eq:logit_linear}{{\rEfLiNK{x9-108001r2}{\csname :autoref\endcsname{equation}4.2}}{\rEfLiNK{x9-108001r2}{\csname :autoref\endcsname{equation}182}}{\rEfLiNK{x9-108001r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:logit_linear@cref}{{[equation][2][4]4.2}{[1][182][]182}{}{}{}}
\newlabel{eq:logistic_probability}{{\rEfLiNK{x9-108002r3}{\csname :autoref\endcsname{equation}4.3}}{\rEfLiNK{x9-108002r3}{\csname :autoref\endcsname{equation}183}}{\rEfLiNK{x9-108002r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:logistic_probability@cref}{{[equation][3][4]4.3}{[1][182][]183}{}{}{}}
\newlabel{sec:lec2_logistic_likelihood}{{\rEfLiNK{x9-1090004.4.1}{\csname :autoref\endcsname{subsubsection}4.4.1}}{\rEfLiNK{x9-1090004.4.1}{\csname :autoref\endcsname{subsubsection}185}}{\rEfLiNK{x9-1090004.4.1}{\csname :autoref\endcsname{subsubsection}Likelihood, loss, and gradient}}{subsubsection.1}{}}
\newlabel{sec:lec2_logistic_likelihood@cref}{{[subsubsection][1][4,4]4.4.1}{[1][185][]185}{}{}{}}
\newlabel{eq:lec2_bernoulli_likelihood}{{\rEfLiNK{x9-109001r4}{\csname :autoref\endcsname{equation}4.4}}{\rEfLiNK{x9-109001r4}{\csname :autoref\endcsname{equation}185}}{\rEfLiNK{x9-109001r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:lec2_bernoulli_likelihood@cref}{{[equation][4][4]4.4}{[1][185][]185}{}{}{}}
\newlabel{eq:lec2_loglik}{{\rEfLiNK{x9-109002r5}{\csname :autoref\endcsname{equation}4.5}}{\rEfLiNK{x9-109002r5}{\csname :autoref\endcsname{equation}185}}{\rEfLiNK{x9-109002r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:lec2_loglik@cref}{{[equation][5][4]4.5}{[1][185][]185}{}{}{}}
\newlabel{eq:lec2_logistic_grad}{{\rEfLiNK{x9-109003r6}{\csname :autoref\endcsname{equation}4.6}}{\rEfLiNK{x9-109003r6}{\csname :autoref\endcsname{equation}186}}{\rEfLiNK{x9-109003r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:lec2_logistic_grad@cref}{{[equation][6][4]4.6}{[1][186][]186}{}{}{}}
\newlabel{fig:lec2_sigmoid_bce}{{\rEfLiNK{x9-109004r15}{\csname :autoref\endcsname{figure}15}}{\rEfLiNK{x9-109004r15}{\csname :autoref\endcsname{figure}188}}{\rEfLiNK{x9-109004r4.4.1}{\csname :autoref\endcsname{figure}Schematic: The sigmoid maps logits to probabilities (left). The binary cross-{}entropy (negative log-{}likelihood) penalizes confident wrong predictions sharply (middle). Regularization typically shrinks parameter norms as the penalty strength increases (right).}}{figure.1}{}}
\newlabel{fig:lec2_sigmoid_bce@cref}{{[figure][15][]15}{[1][188][]188}{}{}{}}
\newlabel{fig:lec1_gd}{{\rEfLiNK{x9-110001r16}{\csname :autoref\endcsname{figure}16}}{\rEfLiNK{x9-110001r16}{\csname :autoref\endcsname{figure}191}}{\rEfLiNK{x9-110001r}{\csname :autoref\endcsname{figure}Schematic: Gradient-descent iterates contracting toward the minimizer of a convex quadratic cost. Ellipses are level sets; arrows show the ``steepest descent along contours'' direction.}}{figure.1}{}}
\newlabel{fig:lec1_gd@cref}{{[figure][16][]16}{[1][191][]191}{}{}{}}
\newlabel{fig:lec2-logistic-boundary}{{\rEfLiNK{x9-111001r17}{\csname :autoref\endcsname{figure}17}}{\rEfLiNK{x9-111001r17}{\csname :autoref\endcsname{figure}194}}{\rEfLiNK{x9-111001r}{\csname :autoref\endcsname{figure}Schematic: Illustrative logistic-regression boundary. The dashed line marks the linear decision boundary at probability 0.5; labeled contours show how the posterior varies smoothly with margin, enabling calibrated decisions and adjustable thresholds.}}{figure.1}{}}
\newlabel{fig:lec2-logistic-boundary@cref}{{[figure][17][]17}{[1][194][]194}{}{}{}}
\newlabel{fig:lec1_mle_map}{{\rEfLiNK{x9-112001r18}{\csname :autoref\endcsname{figure}18}}{\rEfLiNK{x9-112001r18}{\csname :autoref\endcsname{figure}197}}{\rEfLiNK{x9-112001r4.5}{\csname :autoref\endcsname{figure}Schematic: MAP estimates interpolate between the prior mean and the data-driven MLE. As the sample size grows, the MAP curve approaches the true mean.}}{figure.1}{}}
\newlabel{fig:lec1_mle_map@cref}{{[figure][18][]18}{[1][197][]197}{}{}{}}
\newlabel{fig:lec1-roc-pr}{{\rEfLiNK{x9-113001r19}{\csname :autoref\endcsname{figure}19}}{\rEfLiNK{x9-113001r19}{\csname :autoref\endcsname{figure}200}}{\rEfLiNK{x9-113001r4.6}{\csname :autoref\endcsname{figure}Schematic: ROC and PR curves with an explicit operating point. Left: ROC curve with iso-cost lines; right: PR curve with a class-prevalence baseline and iso-F1 contours. Together they visualize threshold trade-offs and calibration quality.}}{figure.1}{}}
\newlabel{fig:lec1-roc-pr@cref}{{[figure][19][]19}{[1][200][]200}{}{}{}}
\newlabel{fig:lec1_confusion}{{\rEfLiNK{x9-113002r20}{\csname :autoref\endcsname{figure}20}}{\rEfLiNK{x9-113002r20}{\csname :autoref\endcsname{figure}205}}{\rEfLiNK{x9-113002r4.6}{\csname :autoref\endcsname{figure}Schematic: Confusion matrix for a three-class classifier; diagonals dominate, indicating strong accuracy with modest confusion between classes B and C.}}{figure.1}{}}
\newlabel{fig:lec1_confusion@cref}{{[figure][20][]20}{[1][205][]205}{}{}{}}
\citation{Platt1999,Guo2017}
\newlabel{chap:perceptron}{{\rEfLiNK{x10-1160005}{\csname :autoref\endcsname{section}5}}{\rEfLiNK{x10-1160005}{\csname :autoref\endcsname{section}217}}{\rEfLiNK{x10-1160005}{\csname :autoref\endcsname{section}Introduction to Neural Networks}}{section.1}{}}
\newlabel{chap:perceptron@cref}{{[section][5][]5}{[1][217][]217}{}{}{}}
\newlabel{eq:feedforward}{{\rEfLiNK{x10-125002r2}{\csname :autoref\endcsname{equation}5.2}}{\rEfLiNK{x10-125002r2}{\csname :autoref\endcsname{equation}224}}{\rEfLiNK{x10-125002r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:feedforward@cref}{{[equation][2][5]5.2}{[1][224][]224}{}{}{}}
\newlabel{eq:weighted_sum}{{\rEfLiNK{x10-136001r3}{\csname :autoref\endcsname{equation}5.3}}{\rEfLiNK{x10-136001r3}{\csname :autoref\endcsname{equation}233}}{\rEfLiNK{x10-136001r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:weighted_sum@cref}{{[equation][3][5]5.3}{[1][232][]233}{}{}{}}
\newlabel{eq:threshold_output}{{\rEfLiNK{x10-136002r4}{\csname :autoref\endcsname{equation}5.4}}{\rEfLiNK{x10-136002r4}{\csname :autoref\endcsname{equation}233}}{\rEfLiNK{x10-136002r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:threshold_output@cref}{{[equation][4][5]5.4}{[1][233][]233}{}{}{}}
\newlabel{eq:neuron_output}{{\rEfLiNK{x10-140001r6}{\csname :autoref\endcsname{equation}5.6}}{\rEfLiNK{x10-140001r6}{\csname :autoref\endcsname{equation}236}}{\rEfLiNK{x10-140001r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:neuron_output@cref}{{[equation][6][5]5.6}{[1][236][]236}{}{}{}}
\newlabel{eq:mp-neuron}{{\rEfLiNK{x10-141001r8}{\csname :autoref\endcsname{equation}5.8}}{\rEfLiNK{x10-141001r8}{\csname :autoref\endcsname{equation}237}}{\rEfLiNK{x10-141001r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:mp-neuron@cref}{{[equation][8][5]5.8}{[1][237][]237}{}{}{}}
\newlabel{sec:perceptron}{{\rEfLiNK{x10-1440005.10}{\csname :autoref\endcsname{subsection}5.10}}{\rEfLiNK{x10-1440005.10}{\csname :autoref\endcsname{subsection}239}}{\rEfLiNK{x10-1440005.10}{\csname :autoref\endcsname{subsection}From MP Neuron to Perceptron and Beyond}}{subsection.1}{}}
\newlabel{sec:perceptron@cref}{{[subsection][10][5]5.10}{[1][239][]239}{}{}{}}
\newlabel{eq:perceptron}{{\rEfLiNK{x10-145001r9}{\csname :autoref\endcsname{equation}5.9}}{\rEfLiNK{x10-145001r9}{\csname :autoref\endcsname{equation}239}}{\rEfLiNK{x10-145001r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:perceptron@cref}{{[equation][9][5]5.9}{[1][239][]239}{}{}{}}
\newlabel{fig:lec3-perceptron-geometry}{{\rEfLiNK{x10-147004r21}{\csname :autoref\endcsname{figure}21}}{\rEfLiNK{x10-147004r21}{\csname :autoref\endcsname{figure}248}}{\rEfLiNK{x10-147004r}{\csname :autoref\endcsname{figure}Schematic: Perceptron geometry. Points on either side of the separating hyperplane receive different labels, and the signed distance to the boundary controls both the class prediction and the magnitude of the update during learning. Compare to \Cref  {fig:lec2-logistic-boundary} in \Cref  {chap:logistic}: both share a linear separator, but logistic smooths the boundary into calibrated probabilities.}}{figure.1}{}}
\newlabel{fig:lec3-perceptron-geometry@cref}{{[figure][21][]21}{[1][248][]248}{}{}{}}
\citation{McCullochPitts1943}
\citation{Rosenblatt1958}
\citation{WidrowHoff1960}
\citation{Rumelhart1986}
\newlabel{chap:mlp}{{\rEfLiNK{x11-1520006}{\csname :autoref\endcsname{section}6}}{\rEfLiNK{x11-1520006}{\csname :autoref\endcsname{section}259}}{\rEfLiNK{x11-1520006}{\csname :autoref\endcsname{section}Multi-Layer Perceptrons: Challenges and Foundations}}{section.1}{}}
\newlabel{chap:mlp@cref}{{[section][6][]6}{[1][259][]259}{}{}{}}
\newlabel{sec:mlp-limitations}{{\rEfLiNK{x11-1530006.1}{\csname :autoref\endcsname{subsection}6.1}}{\rEfLiNK{x11-1530006.1}{\csname :autoref\endcsname{subsection}264}}{\rEfLiNK{x11-1530006.1}{\csname :autoref\endcsname{subsection}From a single unit to the smallest network}}{subsection.1}{}}
\newlabel{sec:mlp-limitations@cref}{{[subsection][1][6]6.1}{[1][264][]264}{}{}{}}
\newlabel{eq:perceptron_forward}{{\rEfLiNK{x11-155001r1}{\csname :autoref\endcsname{equation}6.1}}{\rEfLiNK{x11-155001r1}{\csname :autoref\endcsname{equation}264}}{\rEfLiNK{x11-155001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:perceptron_forward@cref}{{[equation][1][6]6.1}{[1][264][]264}{}{}{}}
\newlabel{eq:two_neuron_forward}{{\rEfLiNK{x11-155003r3}{\csname :autoref\endcsname{equation}6.3}}{\rEfLiNK{x11-155003r3}{\csname :autoref\endcsname{equation}265}}{\rEfLiNK{x11-155003r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:two_neuron_forward@cref}{{[equation][3][6]6.3}{[1][265][]265}{}{}{}}
\newlabel{fig:mlp_minimal_chain}{{\rEfLiNK{x11-155004r22}{\csname :autoref\endcsname{figure}22}}{\rEfLiNK{x11-155004r22}{\csname :autoref\endcsname{figure}267}}{\rEfLiNK{x11-155004r}{\csname :autoref\endcsname{figure}Schematic: The minimal neural network used in this chapter is a two-neuron chain. The first unit produces an intermediate signal, and the second unit maps that signal to the final output.}}{figure.1}{}}
\newlabel{fig:mlp_minimal_chain@cref}{{[figure][22][]22}{[1][267][]267}{}{}{}}
\newlabel{eq:performance}{{\rEfLiNK{x11-157001r4}{\csname :autoref\endcsname{equation}6.4}}{\rEfLiNK{x11-157001r4}{\csname :autoref\endcsname{equation}272}}{\rEfLiNK{x11-157001r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:performance@cref}{{[equation][4][6]6.4}{[1][272][]272}{}{}{}}
\newlabel{eq:gd_update}{{\rEfLiNK{x11-160001r5}{\csname :autoref\endcsname{equation}6.5}}{\rEfLiNK{x11-160001r5}{\csname :autoref\endcsname{equation}275}}{\rEfLiNK{x11-160001r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:gd_update@cref}{{[equation][5][6]6.5}{[1][275][]275}{}{}{}}
\newlabel{eq:vectorized_update}{{\rEfLiNK{x11-160002r6}{\csname :autoref\endcsname{equation}6.6}}{\rEfLiNK{x11-160002r6}{\csname :autoref\endcsname{equation}276}}{\rEfLiNK{x11-160002r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:vectorized_update@cref}{{[equation][6][6]6.6}{[1][275][]276}{}{}{}}
\newlabel{fig:mlp_gd_surface}{{\rEfLiNK{x11-161001r23}{\csname :autoref\endcsname{figure}23}}{\rEfLiNK{x11-161001r23}{\csname :autoref\endcsname{figure}278}}{\rEfLiNK{x11-161001r}{\csname :autoref\endcsname{figure}Schematic: Think of performance as a surface over the weights. Gradient descent moves in one vector step (blue), whereas coordinate-wise updates can zig-zag (orange).}}{figure.1}{}}
\newlabel{fig:mlp_gd_surface@cref}{{[figure][23][]23}{[1][278][]278}{}{}{}}
\newlabel{fig:mlp_step_vs_sigmoid}{{\rEfLiNK{x11-163001r24}{\csname :autoref\endcsname{figure}24}}{\rEfLiNK{x11-163001r24}{\csname :autoref\endcsname{figure}281}}{\rEfLiNK{x11-163001r}{\csname :autoref\endcsname{figure}Schematic: Hard thresholds block gradient-based learning because the derivative is zero almost everywhere. A smooth activation like the sigmoid provides informative derivatives across a wide range of inputs.}}{figure.1}{}}
\newlabel{fig:mlp_step_vs_sigmoid@cref}{{[figure][24][]24}{[1][281][]281}{}{}{}}
\newlabel{eq:sigmoid}{{\rEfLiNK{x11-164001r7}{\csname :autoref\endcsname{equation}6.7}}{\rEfLiNK{x11-164001r7}{\csname :autoref\endcsname{equation}282}}{\rEfLiNK{x11-164001r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sigmoid@cref}{{[equation][7][6]6.7}{[1][282][]282}{}{}{}}
\newlabel{eq:sigmoid_derivative}{{\rEfLiNK{x11-164002r8}{\csname :autoref\endcsname{equation}6.8}}{\rEfLiNK{x11-164002r8}{\csname :autoref\endcsname{equation}282}}{\rEfLiNK{x11-164002r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:sigmoid_derivative@cref}{{[equation][8][6]6.8}{[1][282][]282}{}{}{}}
\newlabel{eq:grad_w2}{{\rEfLiNK{x11-167001r9}{\csname :autoref\endcsname{equation}6.9}}{\rEfLiNK{x11-167001r9}{\csname :autoref\endcsname{equation}286}}{\rEfLiNK{x11-167001r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:grad_w2@cref}{{[equation][9][6]6.9}{[1][286][]286}{}{}{}}
\newlabel{eq:grad_b2}{{\rEfLiNK{x11-167002r10}{\csname :autoref\endcsname{equation}6.10}}{\rEfLiNK{x11-167002r10}{\csname :autoref\endcsname{equation}287}}{\rEfLiNK{x11-167002r10}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:grad_b2@cref}{{[equation][10][6]6.10}{[1][286][]287}{}{}{}}
\newlabel{eq:grad_w1}{{\rEfLiNK{x11-168002r12}{\csname :autoref\endcsname{equation}6.12}}{\rEfLiNK{x11-168002r12}{\csname :autoref\endcsname{equation}287}}{\rEfLiNK{x11-168002r12}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:grad_w1@cref}{{[equation][12][6]6.12}{[1][287][]287}{}{}{}}
\newlabel{eq:grad_b1}{{\rEfLiNK{x11-168003r13}{\csname :autoref\endcsname{equation}6.13}}{\rEfLiNK{x11-168003r13}{\csname :autoref\endcsname{equation}288}}{\rEfLiNK{x11-168003r13}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:grad_b1@cref}{{[equation][13][6]6.13}{[1][287][]288}{}{}{}}
\newlabel{eq:delta2}{{\rEfLiNK{x11-169001r14}{\csname :autoref\endcsname{equation}6.14}}{\rEfLiNK{x11-169001r14}{\csname :autoref\endcsname{equation}288}}{\rEfLiNK{x11-169001r14}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:delta2@cref}{{[equation][14][6]6.14}{[1][288][]288}{}{}{}}
\newlabel{eq:delta1}{{\rEfLiNK{x11-169002r15}{\csname :autoref\endcsname{equation}6.15}}{\rEfLiNK{x11-169002r15}{\csname :autoref\endcsname{equation}289}}{\rEfLiNK{x11-169002r15}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:delta1@cref}{{[equation][15][6]6.15}{[1][288][]289}{}{}{}}
\newlabel{eq:delta_recursion}{{\rEfLiNK{x11-170001r16}{\csname :autoref\endcsname{equation}6.16}}{\rEfLiNK{x11-170001r16}{\csname :autoref\endcsname{equation}291}}{\rEfLiNK{x11-170001r16}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:delta_recursion@cref}{{[equation][16][6]6.16}{[1][291][]291}{}{}{}}
\newlabel{chap:backprop}{{\rEfLiNK{x12-1740007}{\csname :autoref\endcsname{section}7}}{\rEfLiNK{x12-1740007}{\csname :autoref\endcsname{section}298}}{\rEfLiNK{x12-1740007}{\csname :autoref\endcsname{section}Backpropagation Learning in Multi-Layer Perceptrons}}{section.1}{}}
\newlabel{chap:backprop@cref}{{[section][7][]7}{[1][298][]298}{}{}{}}
\newlabel{eq:forward_z}{{\rEfLiNK{x12-176001r1}{\csname :autoref\endcsname{equation}7.1}}{\rEfLiNK{x12-176001r1}{\csname :autoref\endcsname{equation}303}}{\rEfLiNK{x12-176001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:forward_z@cref}{{[equation][1][7]7.1}{[1][303][]303}{}{}{}}
\newlabel{eq:forward_a}{{\rEfLiNK{x12-176002r2}{\csname :autoref\endcsname{equation}7.2}}{\rEfLiNK{x12-176002r2}{\csname :autoref\endcsname{equation}303}}{\rEfLiNK{x12-176002r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:forward_a@cref}{{[equation][2][7]7.2}{[1][303][]303}{}{}{}}
\newlabel{eq:error_function}{{\rEfLiNK{x12-177001r3}{\csname :autoref\endcsname{equation}7.3}}{\rEfLiNK{x12-177001r3}{\csname :autoref\endcsname{equation}304}}{\rEfLiNK{x12-177001r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:error_function@cref}{{[equation][3][7]7.3}{[1][304][]304}{}{}{}}
\newlabel{eq:chain_rule_weight}{{\rEfLiNK{x12-182001r7}{\csname :autoref\endcsname{equation}7.7}}{\rEfLiNK{x12-182001r7}{\csname :autoref\endcsname{equation}311}}{\rEfLiNK{x12-182001r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:chain_rule_weight@cref}{{[equation][7][7]7.7}{[1][311][]311}{}{}{}}
\newlabel{eq:weight_gradient}{{\rEfLiNK{x12-182002r8}{\csname :autoref\endcsname{equation}7.8}}{\rEfLiNK{x12-182002r8}{\csname :autoref\endcsname{equation}312}}{\rEfLiNK{x12-182002r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:weight_gradient@cref}{{[equation][8][7]7.8}{[1][312][]312}{}{}{}}
\newlabel{eq:delta_output}{{\rEfLiNK{x12-184003r11}{\csname :autoref\endcsname{equation}7.11}}{\rEfLiNK{x12-184003r11}{\csname :autoref\endcsname{equation}314}}{\rEfLiNK{x12-184003r11}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:delta_output@cref}{{[equation][11][7]7.11}{[1][314][]314}{}{}{}}
\newlabel{eq:delta_hidden_chain}{{\rEfLiNK{x12-185003r14}{\csname :autoref\endcsname{equation}7.14}}{\rEfLiNK{x12-185003r14}{\csname :autoref\endcsname{equation}315}}{\rEfLiNK{x12-185003r14}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:delta_hidden_chain@cref}{{[equation][14][7]7.14}{[1][315][]315}{}{}{}}
\newlabel{eq:delta_hidden}{{\rEfLiNK{x12-185004r15}{\csname :autoref\endcsname{equation}7.15}}{\rEfLiNK{x12-185004r15}{\csname :autoref\endcsname{equation}316}}{\rEfLiNK{x12-185004r15}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:delta_hidden@cref}{{[equation][15][7]7.15}{[1][316][]316}{}{}{}}
\newlabel{eq:chain_rule}{{\rEfLiNK{x12-188002r17}{\csname :autoref\endcsname{equation}7.17}}{\rEfLiNK{x12-188002r17}{\csname :autoref\endcsname{equation}318}}{\rEfLiNK{x12-188002r17}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:chain_rule@cref}{{[equation][17][7]7.17}{[1][318][]318}{}{}{}}
\newlabel{eq:dE_do}{{\rEfLiNK{x12-189001r18}{\csname :autoref\endcsname{equation}7.18}}{\rEfLiNK{x12-189001r18}{\csname :autoref\endcsname{equation}318}}{\rEfLiNK{x12-189001r18}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:dE_do@cref}{{[equation][18][7]7.18}{[1][318][]318}{}{}{}}
\newlabel{eq:do_da}{{\rEfLiNK{x12-190001r19}{\csname :autoref\endcsname{equation}7.19}}{\rEfLiNK{x12-190001r19}{\csname :autoref\endcsname{equation}319}}{\rEfLiNK{x12-190001r19}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:do_da@cref}{{[equation][19][7]7.19}{[1][319][]319}{}{}{}}
\newlabel{eq:da_dw}{{\rEfLiNK{x12-191001r20}{\csname :autoref\endcsname{equation}7.20}}{\rEfLiNK{x12-191001r20}{\csname :autoref\endcsname{equation}320}}{\rEfLiNK{x12-191001r20}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:da_dw@cref}{{[equation][20][7]7.20}{[1][320][]320}{}{}{}}
\newlabel{eq:dE_dw}{{\rEfLiNK{x12-192001r21}{\csname :autoref\endcsname{equation}7.21}}{\rEfLiNK{x12-192001r21}{\csname :autoref\endcsname{equation}320}}{\rEfLiNK{x12-192001r21}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:dE_dw@cref}{{[equation][21][7]7.21}{[1][320][]320}{}{}{}}
\newlabel{eq:error_signal_output}{{\rEfLiNK{x12-192002r22}{\csname :autoref\endcsname{equation}7.22}}{\rEfLiNK{x12-192002r22}{\csname :autoref\endcsname{equation}321}}{\rEfLiNK{x12-192002r22}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:error_signal_output@cref}{{[equation][22][7]7.22}{[1][321][]321}{}{}{}}
\newlabel{eq:dE_dw_delta}{{\rEfLiNK{x12-192003r23}{\csname :autoref\endcsname{equation}7.23}}{\rEfLiNK{x12-192003r23}{\csname :autoref\endcsname{equation}321}}{\rEfLiNK{x12-192003r23}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:dE_dw_delta@cref}{{[equation][23][7]7.23}{[1][321][]321}{}{}{}}
\newlabel{eq:weight_update_output}{{\rEfLiNK{x12-192004r24}{\csname :autoref\endcsname{equation}7.24}}{\rEfLiNK{x12-192004r24}{\csname :autoref\endcsname{equation}322}}{\rEfLiNK{x12-192004r24}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:weight_update_output@cref}{{[equation][24][7]7.24}{[1][321][]322}{}{}{}}
\newlabel{eq:error_signal_hidden}{{\rEfLiNK{x12-193001r25}{\csname :autoref\endcsname{equation}7.25}}{\rEfLiNK{x12-193001r25}{\csname :autoref\endcsname{equation}322}}{\rEfLiNK{x12-193001r25}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:error_signal_hidden@cref}{{[equation][25][7]7.25}{[1][322][]322}{}{}{}}
\newlabel{eq:weight_update_hidden}{{\rEfLiNK{x12-193002r26}{\csname :autoref\endcsname{equation}7.26}}{\rEfLiNK{x12-193002r26}{\csname :autoref\endcsname{equation}323}}{\rEfLiNK{x12-193002r26}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:weight_update_hidden@cref}{{[equation][26][7]7.26}{[1][323][]323}{}{}{}}
\newlabel{fig:backprop-computational-graph}{{\rEfLiNK{x12-194001r25}{\csname :autoref\endcsname{figure}25}}{\rEfLiNK{x12-194001r25}{\csname :autoref\endcsname{figure}328}}{\rEfLiNK{x12-194001r7.10}{\csname :autoref\endcsname{figure}Computational graph for backpropagation (reverse-mode AD)}}{figure.1}{}}
\newlabel{fig:backprop-computational-graph@cref}{{[figure][25][]25}{[1][328][]328}{}{}{}}
\newlabel{eq:weight_update}{{\rEfLiNK{x12-198001r31}{\csname :autoref\endcsname{equation}7.31}}{\rEfLiNK{x12-198001r31}{\csname :autoref\endcsname{equation}335}}{\rEfLiNK{x12-198001r31}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:weight_update@cref}{{[equation][31][7]7.31}{[1][335][]335}{}{}{}}
\newlabel{eq:grad_W}{{\rEfLiNK{x12-216003r34}{\csname :autoref\endcsname{equation}7.34}}{\rEfLiNK{x12-216003r34}{\csname :autoref\endcsname{equation}350}}{\rEfLiNK{x12-216003r34}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:grad_W@cref}{{[equation][34][7]7.34}{[1][349][]350}{}{}{}}
\newlabel{eq:grad_b}{{\rEfLiNK{x12-216004r35}{\csname :autoref\endcsname{equation}7.35}}{\rEfLiNK{x12-216004r35}{\csname :autoref\endcsname{equation}350}}{\rEfLiNK{x12-216004r35}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:grad_b@cref}{{[equation][35][7]7.35}{[1][349][]350}{}{}{}}
\newlabel{fig:lec4_backprop_flow}{{\rEfLiNK{x12-216005r26}{\csname :autoref\endcsname{figure}26}}{\rEfLiNK{x12-216005r26}{\csname :autoref\endcsname{figure}352}}{\rEfLiNK{x12-216005r}{\csname :autoref\endcsname{figure}Schematic: Forward (blue) and backward (orange) flows for a two-layer MLP. The cached activations and the layerwise error terms (deltas) are exactly the quantities carried along these arrows; backward signals are computed with the next-layer weights and the activation derivative.}}{figure.1}{}}
\newlabel{fig:lec4_backprop_flow@cref}{{[figure][26][]26}{[1][352][]352}{}{}{}}
\newlabel{fig:lec4-activations}{{\rEfLiNK{x12-219001r27}{\csname :autoref\endcsname{figure}27}}{\rEfLiNK{x12-219001r27}{\csname :autoref\endcsname{figure}356}}{\rEfLiNK{x12-219001r}{\csname :autoref\endcsname{figure}Schematic: Canonical activation functions on a common axis. Solid curves show the activation; dashed curves show its derivative.}}{figure.1}{}}
\newlabel{fig:lec4-activations@cref}{{[figure][27][]27}{[1][356][]356}{}{}{}}
\citation{Rumelhart1986}
\citation{Haykin2009}
\newlabel{chap:rbf}{{\rEfLiNK{x13-2230008}{\csname :autoref\endcsname{section}8}}{\rEfLiNK{x13-2230008}{\csname :autoref\endcsname{section}364}}{\rEfLiNK{x13-2230008}{\csname :autoref\endcsname{section}Radial Basis Function Networks (RBFNs)}}{section.1}{}}
\newlabel{chap:rbf@cref}{{[section][8][]8}{[1][364][]364}{}{}{}}
\newlabel{fig:rbf_architecture_weights}{{\rEfLiNK{x13-225001r28}{\csname :autoref\endcsname{figure}28}}{\rEfLiNK{x13-225001r28}{\csname :autoref\endcsname{figure}374}}{\rEfLiNK{x13-225001r8.2}{\csname :autoref\endcsname{figure}Schematic: RBFN architecture. Inputs feed fixed radial units parameterized by centers and widths; a linear readout with weights (and bias) is trained by a regression or classification loss. Only the output weights are typically learned; centers/widths come from k-means or spacing heuristics.}}{figure.1}{}}
\newlabel{fig:rbf_architecture_weights@cref}{{[figure][28][]28}{[1][374][]374}{}{}{}}
\newlabel{fig:rbf_gaussian_bumps}{{\rEfLiNK{x13-226001r29}{\csname :autoref\endcsname{figure}29}}{\rEfLiNK{x13-226001r29}{\csname :autoref\endcsname{figure}377}}{\rEfLiNK{x13-226001r}{\csname :autoref\endcsname{figure}Schematic: Localized Gaussian basis functions (dashed) and their weighted sum (solid). Overlapping bumps allow RBF networks to interpolate complex signals smoothly.}}{figure.1}{}}
\newlabel{fig:rbf_gaussian_bumps@cref}{{[figure][29][]29}{[1][377][]377}{}{}{}}
\newlabel{fig:rbf_centres}{{\rEfLiNK{x13-226002r30}{\csname :autoref\endcsname{figure}30}}{\rEfLiNK{x13-226002r30}{\csname :autoref\endcsname{figure}380}}{\rEfLiNK{x13-226002r}{\csname :autoref\endcsname{figure}Schematic: Center placement and overlap. Top: K-means prototypes roughly tile the data manifold, giving even overlap; bottom: random centers can leave gaps or excessive overlap, influencing the width (sigma) choice and conditioning.}}{figure.1}{}}
\newlabel{fig:rbf_centres@cref}{{[figure][30][]30}{[1][380][]380}{}{}{}}
\newlabel{eq:rbfn_output}{{\rEfLiNK{x13-227001r1}{\csname :autoref\endcsname{equation}8.1}}{\rEfLiNK{x13-227001r1}{\csname :autoref\endcsname{equation}382}}{\rEfLiNK{x13-227001r1}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:rbfn_output@cref}{{[equation][1][8]8.1}{[1][382][]382}{}{}{}}
\newlabel{eq:gaussian_rbf}{{\rEfLiNK{x13-229001r2}{\csname :autoref\endcsname{equation}8.2}}{\rEfLiNK{x13-229001r2}{\csname :autoref\endcsname{equation}383}}{\rEfLiNK{x13-229001r2}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:gaussian_rbf@cref}{{[equation][2][8]8.2}{[1][383][]383}{}{}{}}
\citation{ParkSandberg1991,Micchelli1986}
\newlabel{eq:cost_function}{{\rEfLiNK{x13-237001r3}{\csname :autoref\endcsname{equation}8.3}}{\rEfLiNK{x13-237001r3}{\csname :autoref\endcsname{equation}387}}{\rEfLiNK{x13-237001r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:cost_function@cref}{{[equation][3][8]8.3}{[1][387][]387}{}{}{}}
\newlabel{eq:normal_eq_weights}{{\rEfLiNK{x13-238001r4}{\csname :autoref\endcsname{equation}8.4}}{\rEfLiNK{x13-238001r4}{\csname :autoref\endcsname{equation}388}}{\rEfLiNK{x13-238001r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:normal_eq_weights@cref}{{[equation][4][8]8.4}{[1][388][]388}{}{}{}}
\newlabel{fig:rbf_sigma_sweep}{{\rEfLiNK{x13-246001r31}{\csname :autoref\endcsname{figure}31}}{\rEfLiNK{x13-246001r31}{\csname :autoref\endcsname{figure}396}}{\rEfLiNK{x13-246001r8.10}{\csname :autoref\endcsname{figure}Schematic: How the width parameter (sigma) influences decision boundaries on a 2D toy dataset. Too-large sigma underfits, intermediate sigma captures the boundary, too-small sigma overfits with fragmented regions. Use \Cref  {chap:supervised}'s validation curves to pick model size and regularization.}}{figure.1}{}}
\newlabel{fig:rbf_sigma_sweep@cref}{{[figure][31][]31}{[1][396][]396}{}{}{}}
\newlabel{eq:rbf_weight_training}{{\rEfLiNK{x13-251001r5}{\csname :autoref\endcsname{equation}8.5}}{\rEfLiNK{x13-251001r5}{\csname :autoref\endcsname{equation}400}}{\rEfLiNK{x13-251001r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:rbf_weight_training@cref}{{[equation][5][8]8.5}{[1][400][]400}{}{}{}}
\newlabel{fig:rbf_primal_dual}{{\rEfLiNK{x13-251002r32}{\csname :autoref\endcsname{figure}32}}{\rEfLiNK{x13-251002r32}{\csname :autoref\endcsname{figure}405}}{\rEfLiNK{x13-251002r}{\csname :autoref\endcsname{figure}Schematic: Primal (finite basis) vs.\relax \leavevmode \special {t4ht@+&{35}x00A0{59}}xdual (kernel ridge) viewpoints. Using as many centers as data points recovers the dual form; using fewer centers corresponds to a Nystr\"om approximation. The same trade-off appears in kernel methods through the choice of kernel and effective rank.}}{figure.1}{}}
\newlabel{fig:rbf_primal_dual@cref}{{[figure][32][]32}{[1][405][]405}{}{}{}}
\newlabel{fig:rbf_boundary}{{\rEfLiNK{x13-254001r33}{\csname :autoref\endcsname{figure}33}}{\rEfLiNK{x13-254001r33}{\csname :autoref\endcsname{figure}411}}{\rEfLiNK{x13-254001r}{\csname :autoref\endcsname{figure}Schematic: RBFN decision boundary on the XOR toy for a model with 4 centers, width sigma = 0.8, and ridge lambda = 1e-3. Shading indicates the predicted class under a 0.5 threshold; the black contour marks the 0.5 boundary. Training points are overlaid (class 0: open circles; class 1: filled squares). See \Cref  {fig:rbf_sigma_sweep} for how sigma changes this boundary.}}{figure.1}{}}
\newlabel{fig:rbf_boundary@cref}{{[figure][33][]33}{[1][411][]411}{}{}{}}
\citation{Micchelli1986,ParkSandberg1991}
\newlabel{eq:mse_cost}{{\rEfLiNK{x13-259001r6}{\csname :autoref\endcsname{equation}8.6}}{\rEfLiNK{x13-259001r6}{\csname :autoref\endcsname{equation}414}}{\rEfLiNK{x13-259001r6}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:mse_cost@cref}{{[equation][6][8]8.6}{[1][414][]414}{}{}{}}
\newlabel{eq:wiener_hopf}{{\rEfLiNK{x13-259002r7}{\csname :autoref\endcsname{equation}8.7}}{\rEfLiNK{x13-259002r7}{\csname :autoref\endcsname{equation}416}}{\rEfLiNK{x13-259002r7}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:wiener_hopf@cref}{{[equation][7][8]8.7}{[1][415][]416}{}{}{}}
\newlabel{eq:wiener_solution}{{\rEfLiNK{x13-259003r8}{\csname :autoref\endcsname{equation}8.8}}{\rEfLiNK{x13-259003r8}{\csname :autoref\endcsname{equation}416}}{\rEfLiNK{x13-259003r8}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:wiener_solution@cref}{{[equation][8][8]8.8}{[1][416][]416}{}{}{}}
\newlabel{eq:wiener_freq}{{\rEfLiNK{x13-263001r9}{\csname :autoref\endcsname{equation}8.9}}{\rEfLiNK{x13-263001r9}{\csname :autoref\endcsname{equation}418}}{\rEfLiNK{x13-263001r9}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:wiener_freq@cref}{{[equation][9][8]8.9}{[1][417][]418}{}{}{}}
\citation{Haykin2013AdaptiveFilterTheory}
\citation{WidrowStearns1985}
\citation{SchreierScharf2010}
\citation{Micchelli1986}
\citation{ParkSandberg1991}
\citation{PoggioGirosi1990}
\citation{Bishop1995}
\citation{HastieTibshiraniFriedman2009}
\newlabel{chap:som}{{\rEfLiNK{x14-2680009}{\csname :autoref\endcsname{section}9}}{\rEfLiNK{x14-2680009}{\csname :autoref\endcsname{section}423}}{\rEfLiNK{x14-2680009}{\csname :autoref\endcsname{section}Introduction to Self-Organizing Networks\\and Unsupervised Learning}}{section.1}{}}
\newlabel{chap:som@cref}{{[section][9][]9}{[1][423][]423}{}{}{}}
\newlabel{fig:lec5-learning-rate}{{\rEfLiNK{x14-269001r34}{\csname :autoref\endcsname{figure}34}}{\rEfLiNK{x14-269001r34}{\csname :autoref\endcsname{figure}433}}{\rEfLiNK{x14-269001r9.1}{\csname :autoref\endcsname{figure}Schematic: Learning-rate scheduling intuition. On a smooth objective (left), large initial steps quickly cover ground and roughly align prototypes, while a decaying step-size refines the solution near convergence. Right: common exponential and multiplicative decays used in SOM training.}}{figure.1}{}}
\newlabel{fig:lec5-learning-rate@cref}{{[figure][34][]34}{[1][433][]433}{}{}{}}
\newlabel{fig:lec5-mds-projection}{{\rEfLiNK{x14-274001r35}{\csname :autoref\endcsname{figure}35}}{\rEfLiNK{x14-274001r35}{\csname :autoref\endcsname{figure}439}}{\rEfLiNK{x14-274001r}{\csname :autoref\endcsname{figure}Schematic: Classical MDS intuition. Projecting a cube onto a plane via an orthogonal map yields a square (left), whereas an oblique projection along a body diagonal produces a hexagon (right). The local adjacency of vertices is preserved even though metric structure is distorted.}}{figure.1}{}}
\newlabel{fig:lec5-mds-projection@cref}{{[figure][35][]35}{[1][439][]439}{}{}{}}
\citation{WillshawVonDerMalsburg1976}
\citation{Kohonen1982}
\citation{Kohonen2001}
\citation{Kohonen2001}
\newlabel{eq:bmu}{{\rEfLiNK{x14-282003r3}{\csname :autoref\endcsname{equation}9.3}}{\rEfLiNK{x14-282003r3}{\csname :autoref\endcsname{equation}446}}{\rEfLiNK{x14-282003r3}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:bmu@cref}{{[equation][3][9]9.3}{[1][446][]446}{}{}{}}
\newlabel{eq:som_update}{{\rEfLiNK{x14-282006r4}{\csname :autoref\endcsname{equation}9.4}}{\rEfLiNK{x14-282006r4}{\csname :autoref\endcsname{equation}447}}{\rEfLiNK{x14-282006r4}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:som_update@cref}{{[equation][4][9]9.4}{[1][447][]447}{}{}{}}
\newlabel{eq:gaussian_neighborhood_short}{{\rEfLiNK{x14-285001r5}{\csname :autoref\endcsname{equation}9.5}}{\rEfLiNK{x14-285001r5}{\csname :autoref\endcsname{equation}449}}{\rEfLiNK{x14-285001r5}{\csname :autoref\endcsname{equation}equation}}{equation.1}{}}
\newlabel{eq:gaussian_neighborhood_short@cref}{{[equation][5][9]9.5}{[1][448][]449}{}{}{}}
\newlabel{fig:lec5-gaussian-neighborhood}{{\rEfLiNK{x14-285002r36}{\csname :autoref\endcsname{figure}36}}{\rEfLiNK{x14-285002r36}{\csname :autoref\endcsname{figure}451}}{\rEfLiNK{x14-285002r}{\csname :autoref\endcsname{figure}Schematic: Gaussian neighborhood weights in SOM training. Early iterations us
% Chapter 10
\section{Hopfield Networks: Introduction and Context}\label{chap:hopfield}

\Cref{chap:som} focused on self-organizing maps and unsupervised feature maps; we now transition to another unsupervised/energy-based model: the \emph{Hopfield network}, a recurrent system that stores patterns as attractors. \Cref{fig:roadmap} marks this as the energy-based branch. The debug mindset carries over, but the diagnostics change: SOMs are easiest to audit through map geometry (U-matrix/component planes), while Hopfield recall is easiest to audit through an energy trace and overlap with the intended memory.

\begin{tcolorbox}[summarybox, title={Learning Outcomes}]
\begin{itemize}
    \item Interpret Hopfield networks as energy-minimizing recurrent systems and derive their asynchronous update rule.
    \item Quantify capacity, recall dynamics, and pitfalls (spurious memories, bias encodings) using simple analytical bounds.
    \item Relate Hopfield updates to modern energy-based models and attention mechanisms to build intuition for later chapters.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Design motif}]
Constrain recurrence so the dynamics become a descent process: symmetric weights and an energy function turn ``feedback'' into ``stable memory.''
\end{tcolorbox}

\subsection{From Feedforward to Recurrent Neural Networks}
\label{sec:hopfield_from_feedforward_to_recurrent_neural_networks}

Feedforward networks compute a single forward map: once an input has passed through the layers, the computation ends. That is the right abstraction for static input\(\rightarrow\)output tasks, but it is not a natural model of memory: nothing inside the network persists unless you explicitly feed it back in.

Recurrent networks add that feedback. Cycles let the current state influence the next state, which is exactly what you want for sequences and recall---and also exactly what makes the dynamics harder to reason about. Hopfield's contribution is to keep the recurrence, but constrain it so the system settles instead of wandering.

\paragraph{Challenges with general recurrent networks}
The moment you add feedback, you inherit a dynamical system. That buys you memory, but it also means the network can (i) fail to settle and instead oscillate, (ii) end up in different states from tiny changes in the starting point, and (iii) become harder to train because learning signals must propagate through time (and that path can vanish or explode). Historically, these issues pushed many applications toward feedforward models unless recurrence was the cleanest abstraction for the task.

\begin{tcolorbox}[summarybox, title={Then vs.\ now: energy-based memory in context}]
Classical Hopfield networks are the clean case: binary states, symmetric weights, and an explicit Lyapunov energy give you a convergence guarantee.

The intuition that survives is the one we care about: memory is an energy landscape, and recall is descent toward an attractor (pattern completion).

Modern systems borrow the geometry without necessarily inheriting the guarantee. Many ``memory'' mechanisms are differentiable, learned from data, and content-addressable at scale; the symmetry that makes Hopfield proofs work is not always present. Keep the attractor picture, but do not over-assume the math.
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Author's note: stabilizing recurrence}]
General recurrent networks can behave unpredictably because feedback can create cycles that oscillate or amplify small differences. Hopfield's key move was to restrict the architecture so the dynamics become a descent process: symmetric weights and no self-loops allow the network to be assigned an energy function that decreases under asynchronous updates. That single design choice turns ``recurrent'' from ``chaotic'' into ``stable memory.''
\end{tcolorbox}

\subsection{Hopfield's breakthrough}
\label{sec:hopfield_hopfield_s_breakthrough_1982}

In 1982, John Hopfield introduced a special class of recurrent networks by putting engineering constraints on the feedback loop \citep{Hopfield1982}. The constraints are simple enough to implement, and each one buys you a piece of the convergence story.

First, make the weights symmetric and remove self-loops:
\begin{equation}
    w_{ij} = w_{ji} \quad \forall i, j,
    \label{eq:auto_hopfield_591d3daa63}
\end{equation}
\begin{equation}
    w_{ii} = 0 \quad \forall i.
    \label{eq:auto_hopfield_bd57392190}
\end{equation}
Second, keep neuron states binary, \(s_i \in \{+1,-1\}\), so the network evolves by flipping bits rather than flowing through a continuous activation curve.

Under these choices you can define a Lyapunov energy \(E(\mathbf{s})\) (a scalar function that decreases under the update) and choose an asynchronous (one\hyp{}neuron\hyp{}at\hyp{}a\hyp{}time) update rule so that each flip never increases \(E\). That is the key move: the network becomes a descent process in a finite state space, so it must settle at a fixed point. Those fixed points are the stored patterns (and their complements), plus whatever spurious attractors appear when the network is heavily loaded.

\subsection{Network Architecture and Dynamics}
\label{sec:hopfield_network_architecture_and_dynamics}

Consider a Hopfield network with \(N\) neurons. The state vector is \(\mathbf{s} = (s_1, s_2, \ldots, s_N)^T\), where each \(s_i \in \{+1, -1\}\). The symmetric weight matrix \(\mathbf{W} = [w_{ij}]\) satisfies \(w_{ij} = w_{ji}\) and \(w_{ii} = 0\).
Throughout this discussion \(w_{ij}\) denotes the weight applied to state \(s_j\) when computing the input to neuron \(i\), so column indices correspond to presynaptic neurons.

The \emph{local field} or \emph{input energy} to neuron \(i\) is defined as
\begin{equation}
    h_i(t) = \sum_{j=1}^N w_{ij} s_j(t).
\label{eq:auto_hopfield_5a6eb676df}
\end{equation}
The scalar \(h_i(t)\) therefore represents the total input (or \emph{local field}) accumulated at neuron \(i\) before thresholding during iteration \(t\).

The neuron updates its state according to the sign of \(h_i(t)\) relative to a threshold \(\theta_i\):
\begin{equation}
    s_i(t+1) =
    \begin{cases}
        +1, & h_i(t) \geq \theta_i, \\
        -1, & h_i(t) < \theta_i,
    \end{cases}
    \label{eq:hopfield_update_rule}
\end{equation}

Typically, thresholds \(\theta_i\) are set to zero or learned as part of the model.

\paragraph{Interpretation:} The neuron "fires" (state \(+1\)) if the weighted sum of inputs exceeds the threshold; otherwise, it remains "off" (state \(-1\)). This binary update rule contrasts with the continuous activation functions used in feedforward networks.

\subsection{Encoding conventions}
\label{sec:hopfield_encoding_conventions}

Two binary encodings are common. We primarily use \(s_i \in \{-1,+1\}\) because it simplifies the energy function, but many software libraries work with \(x_i \in \{0,1\}\). Define \(\mathbf{s} = 2\mathbf{x} - \mathbf{1}\) and \(\mathbf{x} = (\mathbf{s}+\mathbf{1})/2\); then
\[
E_{\pm1}(\mathbf{s}) = -\frac{1}{2}\sum_{i\neq j} w_{ij} s_i s_j + \sum_i \theta_i s_i
\]
and
\[
E_{01}(\mathbf{x}) = -\frac{1}{2}\sum_{i\neq j} w'_{ij} x_i x_j + \sum_i \theta'_i x_i + \text{const},
\]
with \(w'_{ij}=4w_{ij}\) and \(\theta'_i = 2\theta_i + 2\sum_{j\neq i} w_{ij}\) under the sign convention in \(E_{\pm1}\). The additive constant does not affect which states minimize the energy or the update dynamics. This table summarizes the correspondence:

\begin{center}
\begin{tabular}{@{}p{0.32\linewidth}p{0.28\linewidth}p{0.28\linewidth}@{}}
\toprule
 & \(\{-1,+1\}\) encoding & \(\{0,1\}\) encoding \\
\midrule
State variable & \(s_i \in \{-1,+1\}\) & \(x_i = (s_i+1)/2\) \\
Energy & \(E_{\pm1}(\mathbf{s})\) & \(E_{01}(\mathbf{x}) = E_{\pm1}(2\mathbf{x}-\mathbf{1})\) \\
Update rule & \(s_i \leftarrow \operatorname{sign}(h_i - \theta_i)\) & \(x_i \leftarrow \mathbf{1}[h'_i - \theta'_i > 0]\) \\
\bottomrule
\end{tabular}
\end{center}

Whenever an equation later in the chapter uses \(s_i\) you can translate it to \(x_i\) via this affine mapping; we call out both forms only when the distinction matters. As a concrete example, the pattern \(\mathbf{x}=[1,0,1,0]\) in the \(\{0,1\}\) encoding maps to \(\mathbf{s}=2\mathbf{x}-\mathbf{1}=[+1,-1,+1,-1]\); conversely \(\mathbf{s}=[-1,+1,+1]\) corresponds to \(\mathbf{x}=[0,1,1]\).

\subsection{Energy Function and Stability}
\label{sec:hopfield_energy_function_and_stability}

Hopfield defined an energy function \(E: \{-1, +1\}^N \to \mathbb{R}\) associated with the network state \(\mathbf{s}\):
\begin{equation}
    E(\mathbf{s}) = -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N w_{ij} s_i s_j + \sum_{i=1}^N \theta_i s_i.
    \label{eq:energy_function}
\end{equation}
Because the weights are symmetric and satisfy \(w_{ii}=0\), the double sum may equivalently be written as \(\sum_{i<j} w_{ij} s_i s_j\); the \(\tfrac{1}{2}\) factor explicitly prevents counting each unordered pair twice, so removing it would scale the energy by two. Thresholds \(\theta_i\) act like biases; many texts write the second term as \(-\sum_i b_i s_i\) with \(b_i = \theta_i\).
% Chapter 10 (continued)

\begin{tcolorbox}[summarybox, title={Engineering lens: energy as an objective (not a metaphor)}]
It helps to read \(E(\mathbf{s})\) the same way you read a loss in supervised learning: a scalar objective defined over a space of states. Here the states are discrete (\(\mathbf{s}\in\{-1,+1\}^N\)), so the ``optimization'' is not gradient descent; it is a sequence of coordinate updates (flip one bit at a time) that never increase \(E\) under Hopfield's symmetry constraints.

This is why the update rule matters. The weights define the energy landscape, and the asynchronous update is a descent procedure on that landscape. Local minima are stable memories; other minima (spurious attractors) are the price of capacity.
\end{tcolorbox}

\paragraph{Hopfield network states and the energy view}
\label{sec:hopfield_hopfield_network_states_and_energy_function}

Encoding choices and the corresponding energy forms were laid out in \Cref{sec:hopfield_encoding_conventions} and \Cref{sec:hopfield_energy_function_and_stability}. The key continuity idea is that Hopfield dynamics are not ``mysterious recurrence'': with symmetric weights, the update rule becomes a descent process on \(E(\cdot)\). We make this concrete in three passes: define what ``stable'' means, walk one tiny numeric trace, then prove the one-step claim (\(\Delta E\le 0\)) that guarantees convergence under asynchronous updates.

\subsection{Energy Minimization and Stable States}
\label{sec:hopfield_energy_minimization_and_stable_states}

The fundamental goal in Hopfield networks is to find a state \(\mathbf{s}\) that minimizes the energy \(E\). Such states correspond to stable equilibria or attractors of the network dynamics.

\paragraph{State update dynamics:}
The network updates neuron states using the sign rule in \eqref{eq:hopfield_update_rule}; for \(\{0,1\}\) encodings use the corresponding thresholded version. The pseudo-code below makes the asynchronous sweep explicit for later convergence arguments.

\begin{tcolorbox}[summarybox, title={Asynchronous Hopfield update (pseudo-code)}]
\begin{enumerate}
    \item Initialize \(\mathbf{s}\) (e.g., noisy probe), set max sweeps \(T\).
    \item For \(t=1,\dots, T\):
    \begin{enumerate}
        \item Pick a neuron index \(i\) (random order or cyclic sweep).
        \item Compute \(h_i = \sum_j w_{ij} s_j - \theta_i\).
        \item Update \(s_i \leftarrow \operatorname{sign}(h_i)\).
    \end{enumerate}
    \item Stop early if a full sweep causes no flips; else continue.
\end{enumerate}
Each single-neuron update satisfies \(\Delta E \le 0\) by \eqref{eq:deltaE}, so the loop converges to a local minimum of \eqref{eq:energy_function}.
\end{tcolorbox}

For the formal monotonicity proof and the synchronous-update caveat, see \Cref{sec:hopfield_async_vs_sync}.

\begin{tcolorbox}[summarybox, title={Implementation checklist (debugging a Hopfield net)}]
\begin{itemize}
    \item \textbf{Weight constraints:} enforce \(W=W^\top\) and \(w_{ii}=0\). If either is violated, the classical energy argument does not apply.
    \item \textbf{Energy definition:} implement \eqref{eq:energy_function} with the \(\tfrac{1}{2}\) factor (or equivalently sum over \(i<j\)). Use one sign convention for thresholds/biases and stick to it.
    \item \textbf{Update scheme:} test with asynchronous single-neuron updates first. If you switch to synchronous updates, short cycles are possible (see \Cref{sec:hopfield_async_vs_sync}).
    \item \textbf{Sign at zero:} choose a deterministic convention for \(\operatorname{sign}(0)\) (e.g., keep the old state, or return \(+1\)) so runs are reproducible.
    \item \textbf{Diagnostics:} on a tiny network where you know the stored patterns, plot \(E(\mathbf{s}(t))\) and overlap \(m^{(\mu)}(\mathbf{s}(t))\). Under asynchronous updates and symmetric weights, \(E\) should never increase.
\end{itemize}
\end{tcolorbox}

\subsection{Example: Energy Calculation and State Updates}
\label{sec:hopfield_example_energy_calculation_and_state_updates}

Consider a Hopfield network with three neurons, bipolar states \(s_i \in \{-1,+1\}\), zero thresholds, and the symmetric weight matrix
\[
W = \begin{bmatrix}
0 & 3 & -4 \\
3 & 0 & 2 \\
-4 & 2 & 0
\end{bmatrix}.
\]

Let the initial state be \(\mathbf{s} = (1,\,1,\,-1)\). Using the energy definition with the \(\frac{1}{2}\) factor to avoid double counting, we obtain
\begin{align}
    E(\mathbf{s}) &= -\frac{1}{2} \sum_{i=1}^3 \sum_{j=1}^3 w_{ij} s_i s_j \nonumber \\
    &= -\frac{1}{2} \Big[ 2 \cdot 3 \cdot (1)(1) + 2 \cdot (-4) \cdot (1)(-1) + 2 \cdot 2 \cdot (1)(-1) \Big] = -5.
    \label{eq:auto:lecture_5_part_ii:1}
\end{align}

\paragraph{State update attempts:}
One way to check whether \(\mathbf{s}=(1,1,-1)\) is stable is to try each single-neuron flip and recompute \(E\). Flipping \(s_1\) gives \(E(-1,1,-1)=9\); flipping \(s_2\) gives \(E(1,-1,-1)=-3\); and flipping \(s_3\) gives \(E(1,1,1)=-1\). Each move raises the energy relative to \(-5\), so no asynchronous update would accept it: the state is a local minimum.

For clarity, \Cref{tab:hopfield-deltaE} reports the energy change for each single flip relative to the current state.
\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Flip & New state & \(\Delta E\) & Accept? \\
        \midrule
        \(s_1 \leftarrow -1\) & \((-1,\,1,\,-1)\) & \(+14\) & No \\
        \(s_2 \leftarrow -1\) & \((1,\,-1,\,-1)\) & \(+2\) & No \\
        \(s_3 \leftarrow +1\) & \((1,\,1,\,1)\) & \(+4\) & No \\
        \bottomrule
    \end{tabular}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Single-neuron flips from \((1,1,-1)\); all raise the energy, so the state is a local minimum.}
    \label{tab:hopfield-deltaE}
\end{table}

% QC-BEGIN: hopfield_energy_example
% W 0 3 -4
% W 3 0 2
% W -4 2 0
% theta 0 0 0
% s0 1 1 -1 E -5
% flip_s1 -1 1 -1 E 9
% flip_s2 1 -1 -1 E -3
% flip_s3 1 1 1 E -1
% noisy 1 1 1 update_i 3 s1 1 1 -1 E -5
% QC-END: hopfield_energy_example


Because every single-neuron flip raises the energy, the state \((1,1,-1)\) is a stable local minimum for this network.
If the network is perturbed slightly (for instance, by flipping \(s_3\) to \(+1\) to create the noisy pattern \((1,1,1)\)), the next asynchronous update flips it back. In this state the local field at neuron 3 is \(h_3=-4(1)+2(1)=-2\), so the update rule sets \(s_3\leftarrow -1\), returning to \((1,1,-1)\). The energy drops from \(-1\) to \(-5\), which is the basic content-addressable recall behavior.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.72\linewidth,
            height=4.2cm,
            ymin=-6.5, ymax=0.5,
            ytick={-6,-4,-2,0},
            symbolic x coords={s0, s1},
            xticklabels={$\,\mathbf{s}^{(0)}$,$\,\mathbf{s}^{(1)}$},
            xtick=data,
            xlabel={Update step},
            ylabel={$E(\mathbf{s})$},
            axis lines=left,
            enlarge x limits=0.1
        ]
            \addplot+[cbBlue, thick, mark=*, mark size=2pt] coordinates {(s0,-1) (s1,-5)};
            \addplot[cbPink, -{Latex[length=2mm]}, thick] coordinates {(s0,-1) (s1,-5)};
            \node[cbPink, font=\scriptsize, anchor=west] at (rel axis cs:0.42,0.75) {one asynchronous flip};
            \node[cbBlue, font=\scriptsize, anchor=south west] at (axis cs: s0,-1) {noisy start $\mathbf{s}^{(0)}$};
        \end{axis}
    \end{tikzpicture}
    % Avoid inline math in captions; it wraps poorly in some EPUB renderers.
    \caption{Hopfield energy decreases monotonically under asynchronous updates. Starting from a noisy probe \(\mathbf{s}^{(0)}\), single-neuron flips move downhill until a stable memory is recovered.}
    \label{fig:hopfield-energy-descent}
\end{figure}


% Chapter 10 (continued)

As \Cref{fig:hopfield-energy-descent} shows, the energy trace never increases when single neurons flip asynchronously, so the descent depicted here is exactly what the formal convergence proof below captures.

\subsection{Energy Function and Convergence of Hopfield Networks}
\label{sec:hopfield_energy_function_and_convergence_of_hopfield_networks}

The figure and pseudo-code above already suggest the main claim: under the symmetry constraints that make \(E(\cdot)\) well defined, asynchronous updates are guaranteed to move ``downhill'' (or stay flat) in energy. This is the formal reason Hopfield networks converge to a fixed point: the state space is finite, and you cannot decrease a bounded quantity forever.

In this subsection we prove the monotonicity step that the rest of the convergence story rests on. The only assumptions you should keep in view are the ones you would implement anyway: symmetric weights with zero diagonal, and an asynchronous update rule (one neuron at a time). When these assumptions are violated (e.g., synchronous updates or asymmetric weights), the energy argument no longer guarantees convergence.

We use the energy in \eqref{eq:energy_function}; the goal is to show that each single-neuron update never increases it.

\paragraph{Goal:} Show that asynchronous updates of neuron states always decrease (or leave unchanged) the energy \( E \), guaranteeing convergence to a local minimum.

\subsubsection{Energy Change Upon Updating a Single Neuron}
\label{sec:hopfield_energy_change_upon_updating_a_single_neuron_sub}

Consider updating neuron \( i \) from old state \( s_i^{\text{old}} \) to new state \( s_i^{\text{new}} \). All other neuron states \( s_j \) for \( j \neq i \) remain fixed. The change in energy is
\begin{equation}
    \Delta E = E_{\text{new}} - E_{\text{old}}.
\label{eq:auto_hopfield_927711ec51}
\end{equation}

Using \eqref{eq:energy_function}, write out the energies explicitly:
\begin{align}
    E_{\text{old}} &= -\frac{1}{2} \sum_{k=1}^N \sum_{l=1}^N w_{kl} s_k^{\text{old}} s_l^{\text{old}} + \sum_{k=1}^N \theta_k s_k^{\text{old}}, \\
    E_{\text{new}} &= -\frac{1}{2} \sum_{k=1}^N \sum_{l=1}^N w_{kl} s_k^{\text{new}} s_l^{\text{new}} + \sum_{k=1}^N \theta_k s_k^{\text{new}}.
    \label{eq:auto:lecture_5_part_ii:2}
\end{align}

Since only \( s_i \) changes, and weights are symmetric with zero diagonal \( w_{ii} = 0 \), the difference simplifies to
\begin{align}
    \Delta E &= E_{\text{new}} - E_{\text{old}} \nonumber \\
    &= -\frac{1}{2} \sum_{j=1}^N \left( w_{ij} s_i^{\text{new}} s_j + w_{ji} s_j s_i^{\text{new}} \right) + \theta_i s_i^{\text{new}} \nonumber \\
    &\quad + \frac{1}{2} \sum_{j=1}^N \left( w_{ij} s_i^{\text{old}} s_j + w_{ji} s_j s_i^{\text{old}} \right) - \theta_i s_i^{\text{old}} \nonumber \\
    &= - \sum_{j=1}^N w_{ij} s_j \left( s_i^{\text{new}} - s_i^{\text{old}} \right) + \theta_i \left( s_i^{\text{new}} - s_i^{\text{old}} \right) \nonumber \\
    &= - \left( s_i^{\text{new}} - s_i^{\text{old}} \right) \left( \sum_{j=1}^N w_{ij} s_j - \theta_i \right).
    \label{eq:deltaE}
\end{align}

Define the \emph{local field} \( h_i \) at neuron \( i \) as
\begin{equation}
    h_i = \sum_{j=1}^N w_{ij} s_j - \theta_i.
    \label{eq:local_field}
\end{equation}

Then,
\[
    \Delta E = - (s_i^{\text{new}} - s_i^{\text{old}}) h_i.
\]

\paragraph{Numeric check (single flip).} With two neurons, weights \(w_{12}=w_{21}=1\), thresholds \( \theta_i=0\), and current state \((s_1, s_2) = (1,-1)\), the local field at neuron 1 is \(h_1 = 1\cdot (-1) = -1\). The update rule sets \(s_1^{\text{new}} = \operatorname{sign}(h_1) = -1\), so \(s_1^{\text{new}} - s_1^{\text{old}} = -2\) and \(\Delta E = -(-2)(-1) = -2 < 0\), confirming the energy drop predicted by \eqref{eq:deltaE}.
\begin{tcolorbox}[summarybox, title={Modern Hopfield views and attention}]
Recent work \citep{Krotov2016,Krotov2020,Ramsauer2021} revisits Hopfield networks as dense
associative memories with continuous states and softmax interactions that are closely related to
Transformer attention. In this view the stored patterns play the role of keys and values, the
current state or query probes the landscape, and the update rule resembles a softmax-weighted
average over memories, minimizing an energy of the form \(\log \sum_\mu \exp(\beta \mathbf{s}^\top \mathbf{m}^\mu)\) with inverse temperature \(\beta\). The useful connection for us is the geometry: content-addressable lookup as motion in an energy landscape. Do not confuse that bridge with the classical guarantee above: the Lyapunov proof in this chapter relies on binary states, symmetric weights, and asynchronous flips.
\end{tcolorbox}

\paragraph{Continuous Hopfield networks (bridge).}
Modern extensions replace the binary sign activation with a smooth, often softmax-like update that keeps neuron states continuous. Storing \(P\) real-valued patterns \(\{\mathbf{m}^\mu\}\), a common update takes the form
\[
\mathbf{s}^{(t+1)} = \operatorname{softmax}\!\left(\beta M^\top \mathbf{s}^{(t)}\right) M,
\]
where \(M\) stacks the memory vectors and \(\beta\) controls sharpness. Read this as ``soft'' associative recall: the update produces a weighted combination of stored patterns, and that picture lines up closely with attention in \Cref{chap:transformers}. Unlike the classical binary case, you should not assume strict monotone descent unless the model is built to have a Lyapunov function.

Combining the sign update in \eqref{eq:hopfield_update_rule} with \eqref{eq:deltaE} yields \(\Delta E \le 0\) for each asynchronous flip, so we omit the redundant case split here.
% Chapter 10: Hopfield Network Weight Update and Capacity

\subsection{Asynchronous vs. Synchronous Updates in Hopfield Networks}
\label{sec:hopfield_async_vs_sync}

Recall from the previous discussion that the Hopfield network energy function decreases monotonically with each asynchronous update of a single neuron state. This guarantees convergence to a local minimum of the energy landscape. In contrast, fully synchronous updates (flipping all neurons at once) can lead to oscillations or short cycles rather than convergence, which is why the classical convergence proofs assume asynchronous updates.

\paragraph{Why asynchronous updates?}
With symmetric weights you can still write the same energy \(E(\mathbf{s})\), but the classic monotonicity proof is for \emph{single-neuron} (asynchronous) updates. If you update all neurons simultaneously, two-cycles can appear.

For a concrete example, take two neurons with \(\theta_1=\theta_2=0\) and \(w_{12}=w_{21}=1\). Start from \((s_1,s_2)=(1,-1)\). A synchronous update computes both new states from the old pair:
\[
s_1 \leftarrow \operatorname{sign}(w_{12}s_2)=\operatorname{sign}(-1)=-1,\qquad
s_2 \leftarrow \operatorname{sign}(w_{21}s_1)=\operatorname{sign}(1)=1,
\]
so the state becomes \((-1,1)\). Repeating the synchronous update maps \((-1,1)\) back to \((1,-1)\). The network therefore oscillates in a 2-cycle, even though the energy
\[
E(s_1,s_2) = -\frac{1}{2}\sum_{i,j} w_{ij}s_is_j = -w_{12}s_1s_2
\]
stays constant across the two states.

To ensure the classical convergence guarantee, use \emph{asynchronous} updates (one neuron at a time) under symmetric weights with zero diagonal. Synchronous updates can still work in practice, but the energy argument no longer rules out short cycles.

\subsection{Storage Capacity of Hopfield Networks}
\label{sec:hopfield_storage_capacity_of_hopfield_networks}

A key question is: \emph{How many memories can a Hopfield network reliably store and recall?}
\[
p \approx 0.138\, n,
\]
is the classical rule-of-thumb for the number of random patterns that can be stored with low error in a network of \(n\) neurons \citep{McEliece1987}. The important point is not the constant itself; it is that the capacity is only a small fraction of \(n\). Both \(\mathbf{\xi}^\mu\) and its complement \(-\mathbf{\xi}^\mu\) are fixed points, and odd mixtures of stored patterns become spurious states as \(p/n\) grows. This low capacity is why Hopfield networks are not used as practical storage devices despite their elegant associative-memory behavior.

\paragraph{Stochastic updates (bridge to Boltzmann machines).} A stochastic variant replaces the hard sign in \eqref{eq:hopfield_update_rule} with probabilistic flips (e.g., Gibbs sampling). With symmetric weights this defines a Boltzmann distribution whose energy matches \eqref{eq:energy_function}, linking Hopfield recall to the Boltzmann/energy-based models that underlie modern probabilistic neural networks.

\subsection{Improving Storage Capacity via Weight Updates}
\label{sec:hopfield_improving_storage_capacity_via_weight_updates}

Is it possible to improve storage by choosing the weights to ``bake in'' the memories directly? That is exactly what the classical Hebbian construction does: use stored patterns to set \(W\), then let the state updates perform recall.

\paragraph{Hebbian learning rule:}
Given \( p \) stored patterns \(\{\mathbf{\xi}^1, \mathbf{\xi}^2, \ldots, \mathbf{\xi}^p\}\), each \(\mathbf{\xi}^\mu = (\xi_1^\mu, \xi_2^\mu, \ldots, \xi_n^\mu)\) with \( \xi_i^\mu \in \{+1, -1\} \), the weights are set by:
\begin{equation}
w_{ij} = \frac{1}{n} \sum_{\mu=1}^p \xi_i^\mu \xi_j^\mu, \quad w_{ii} = 0.
\label{eq:hebbian_weights}
\end{equation}

This is the classical Hebbian learning rule for Hopfield networks.

\paragraph{What this formula is doing:}
Setting \(w_{ii}=0\) removes self-feedback. The factor \(1/n\) keeps weight magnitudes \(O(1)\) as the network grows. The sum itself is an outer-product accumulation: it encodes pairwise correlations across the stored patterns.

\subsection{Example: Weight Calculation for a Single Pattern}
\label{sec:hopfield_example_weight_calculation_for_a_single_pattern}

Consider a fundamental memory pattern:
\[
\mathbf{\xi} = (1, 1, 1, -1),
\]
with no thresholds (\( \theta_i = 0 \)).

\paragraph{Step 1: Compute outer product}
Form the matrix \( \mathbf{B} = \mathbf{\xi} \mathbf{\xi}^\top \). Each entry \(B_{ij} = \xi_i \xi_j\) captures the pairwise correlation between neurons \(i\) and \(j\).
\[
\mathbf{B}
=
\mathbf{\xi}\mathbf{\xi}^\top
=
\begin{bmatrix}
1 & 1 & 1 & -1 \\
1 & 1 & 1 & -1 \\
1 & 1 & 1 & -1 \\
-1 & -1 & -1 & 1
\end{bmatrix}.
\]

\paragraph{Step 2: Remove diagonal terms}
Zero the diagonal entries to obtain the weight matrix \( \mathbf{W} \) with \(w_{ii} = 0\). The off-diagonal values remain the same as in \(\mathbf{B}\), encoding the pairwise interactions required to store the memory pattern.
Including the \(1/n\) normalization from \eqref{eq:hebbian_weights} (here \(n=4\)) gives
\[
\mathbf{W}
=
\frac{1}{4}\Big(\mathbf{B}-\operatorname{diag}(\mathbf{B})\Big)
=
\frac{1}{4}
\begin{bmatrix}
0 & 1 & 1 & -1 \\
1 & 0 & 1 & -1 \\
1 & 1 & 0 & -1 \\
-1 & -1 & -1 & 0
\end{bmatrix}.
\]

\paragraph{Sanity check: the stored pattern is a fixed point}
With zero thresholds, the local field is \(\mathbf{h}=\mathbf{W}\mathbf{\xi}\). For this one-pattern example,
\[
\mathbf{h}
=
\begin{bmatrix}
3/4 \\
3/4 \\
3/4 \\
-3/4
\end{bmatrix},
\]
so \(\operatorname{sign}(\mathbf{h})=\mathbf{\xi}\). That is the basic content-addressable ``snap-back'': if you start near \(\mathbf{\xi}\), asynchronous updates push you toward it.

\begin{tcolorbox}[summarybox, title={Numeric recall trace (two asynchronous flips)}]
Start from a probe with two wrong bits, \(\mathbf{s}^{(0)}=(1,-1,-1,-1)\). With \(\theta_i=0\), the local field is \(h_i=\sum_j w_{ij}s_j\), and the update rule \eqref{eq:hopfield_update_rule} sets \(s_i\leftarrow \operatorname{sign}(h_i)\). One possible asynchronous update order (neuron 2, then neuron 3) gives:
\begin{center}
\begin{tabular}{@{}lccc@{}}
\toprule
Step & Update & Local field & New state \\
\midrule
0 & -- & -- & \((1,-1,-1,-1)\) \\
1 & \(i=2\) & \(h_2=0.25\) & \((1,1,-1,-1)\) \\
2 & \(i=3\) & \(h_3=0.75\) & \((1,1,1,-1)=\mathbf{\xi}\) \\
\bottomrule
\end{tabular}
\end{center}
The energy drops along the way: \(E=0.5 \rightarrow 0 \rightarrow -1.5\).
\end{tcolorbox}

% QC-BEGIN: hopfield_single_pattern_weights
% xi 1 1 1 -1
% W 0 0.25 0.25 -0.25
% W 0.25 0 0.25 -0.25
% W 0.25 0.25 0 -0.25
% W -0.25 -0.25 -0.25 0
% h 0.75 0.75 0.75 -0.75
% recall_s0 1 -1 -1 -1 E 0.5
% recall_step 1 update_i 2 h 0.25 s 1 1 -1 -1 E 0.0
% recall_step 2 update_i 3 h 0.75 s 1 1 1 -1 E -1.5
% QC-END: hopfield_single_pattern_weights

\subsection{Finalizing the Hopfield Network Derivation and Discussion}
\label{sec:hopfield_finalizing_the_hopfield_network_derivation_and_discussion}

We have already defined the Hebbian weights in \eqref{eq:hebbian_weights}, the update rule in \eqref{eq:hopfield_update_rule}, and the energy in \eqref{eq:energy_function}. Here we focus on what those ingredients imply for retrieval basins and limitations in practice.

\paragraph{Memory Retrieval and Basins of Attraction}

The stored patterns \(\{\mathbf{\xi}^\mu\}\) correspond to local minima of the energy landscape. Starting from an initial state \(\mathbf{s}(0)\) that is a noisy or partial version of a stored pattern, the network dynamics converge to the closest attractor, ideally retrieving the original memory or its complement \(-\mathbf{\xi}^\mu\).

For example, if the initial state is corrupted, the network will iteratively update states to reduce energy until it reaches a stable point:
\[
\mathbf{s}(\infty) \in \{\mathbf{\xi}^\mu, -\mathbf{\xi}^\mu\}.
\]
The same story can be read visually as motion toward a basin of attraction in an energy landscape (\Cref{fig:hopfield-basins-schematic}).

\begin{tcolorbox}[summarybox, title={Terminology and diagnostics}]
\textbf{Attractor.} A stable fixed point of the update rule: once you reach it, further asynchronous updates do not change the state.

\textbf{Basin of attraction.} The set of initial states that converge to a given attractor under the chosen update scheme.

\textbf{Spurious state.} An attractor that is not one of the intended stored patterns (nor its complement). Spurious states are real local minima of \(E(\cdot)\) created by interference among memories.

\textbf{Overlap (a simple health check).} For a stored pattern \(\mathbf{\xi}^\mu\), define
\[
m^{(\mu)}(\mathbf{s}) = \frac{1}{N} (\mathbf{\xi}^\mu)^\top \mathbf{s}.
\]
It ranges from \(+1\) (exact match) to \(-1\) (exact complement). Plot \(E(\mathbf{s}(t))\) and \(m^{(\mu)}(\mathbf{s}(t))\) versus update step to see whether recall is behaving or drifting into a spurious basin.
\end{tcolorbox}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.78\linewidth,
            height=4.2cm,
            xmin=-2.4, xmax=2.4,
            ymin=-1.2, ymax=1.8,
            axis lines=left,
            xlabel={State coordinate (schematic)},
            ylabel={Energy},
            ytick=\empty,
            xtick=\empty,
        ]
            \addplot[cbBlue, thick, samples=200, domain=-2.4:2.4]
                {0.25*(x^4) - 0.9*(x^2) + 0.4*sin(deg(2*x)) + 0.4};
            \node[font=\scriptsize, anchor=north] at (axis cs:-1.25,-0.5) {memory basin};
            \node[font=\scriptsize, anchor=north] at (axis cs:1.2,-0.55) {spurious basin};
            \addplot[cbPink, -{Latex[length=2mm]}, thick] coordinates {(-0.2,1.1) (-1.05,-0.35)};
            \node[cbPink, font=\scriptsize, anchor=west] at (axis cs:-0.15,1.1) {noisy probe};
        \end{axis}
    \end{tikzpicture}
    \caption{Energy-landscape intuition (schematic). Hopfield recall is a descent process toward a nearby basin (a stored memory), but other minima can exist and act as spurious attractors when the network is heavily loaded.}
    \label{fig:hopfield-basins-schematic}
\end{figure}

\paragraph{Limitations (what breaks first)}
Hopfield networks are a beautiful model to reason about, but several limitations show up quickly in practice:
\begin{itemize}
    \item \textbf{Capacity:} with the classical Hebbian construction, reliable storage for random patterns scales like \(P_{\max}\approx 0.138\,N\) for large \(N\). Past that loading level, retrieval errors and spurious minima become common.
    \item \textbf{Spurious attractors:} even below capacity, the energy landscape can contain minima that are not stored patterns (or their complements). A noisy probe can ``snap'' to one of these unintended fixed points.
    \item \textbf{Classification mismatch:} for discriminative tasks (say, digit recognition), the ``nearest minimum'' behavior is not the same thing as a calibrated class decision. A corrupted input can converge to the wrong stored pattern, and low energy does not imply correct label.
    \item \textbf{When not to use:} heavy loading (\(P/N\) large) produces a glassy landscape; scaling is \(O(N^2)\) in connectivity; and the low\hyp{}capacity regime makes classical Hopfield networks a poor fit for high\hyp{}dimensional supervised problems compared with the ERM models in \Cref{chap:logistic} or the deep models in \Cref{chap:transformers}.
\end{itemize}

\begin{tcolorbox}[summarybox, title={Applications lens: memory retrieval and optimization}]
In the classical framing, Hopfield networks are about associative memory: you store patterns, then recover a full pattern from a partial or noisy cue (pattern completion). That is the same idea behind many information-retrieval and recognition stories: the input is a corrupted probe, and the system ``snaps'' to a nearby clean prototype.

There is also an optimization lens. Because \(E(\mathbf{s})\) is a scalar objective over binary states, you can sometimes encode a cost function as an energy and run asynchronous updates as a heuristic descent method. This is one way Hopfield-style dynamics have been discussed for combinatorial optimization tasks (for example, variants of traveling-salesman-style objectives). The warning is the same one we give throughout this book: the algorithm will happily converge to a local minimum. Your real work is in deciding whether the energy you wrote down matches the engineering goal and whether the resulting minima are meaningful.
\end{tcolorbox}

\paragraph{Example: Memory recovery (one flip)}

Store one pattern
\[
\mathbf{\xi} = (-1, -1, 1, -1)^T.
\]
With \(n=4\), the Hebbian weights from \eqref{eq:hebbian_weights} are
\[
\mathbf{W} = \frac{1}{4} \mathbf{\xi} \mathbf{\xi}^\top, \qquad w_{ii} = 0,
\]
which numerically becomes a single symmetric matrix
\[
\mathbf{W} = \frac{1}{4}
\begin{pmatrix}
0 & 1 & -1 & 1 \\
1 & 0 & -1 & 1 \\
-1 & -1 & 0 & -1 \\
1 & 1 & -1 & 0
\end{pmatrix}.
\]
The off-diagonal entries are therefore the scaled products of pattern components (e.g., $w_{12}=w_{21}=0.25$ and $w_{13}=w_{31}=-0.25$).
More generally, every off-diagonal weight is the scaled product of the corresponding pattern entries.

Now start from a one-bit-corrupted probe \(\mathbf{s}^{(0)}=[-1,-1,1,1]^T\), with \(\theta_i=0\). If we update neuron \(4\),
\[
h_4=\sum_j w_{4j}s_j = -0.75 \quad\Rightarrow\quad s_4\leftarrow -1,
\]
so in one asynchronous flip we recover \(\mathbf{s}^{(1)}=\mathbf{\xi}\). The energy drops from \(E=0\) to \(E=-1.5\) under \eqref{eq:energy_function}. The appearance of \(-\mathbf{\xi}\) as a fixed point is expected: the energy only depends on products \(s_i s_j\), so negating all bits leaves every term unchanged.

% QC-BEGIN: hopfield_memory_recovery_4n
% xi -1 -1 1 -1
% W 0 0.25 -0.25 0.25
% W 0.25 0 -0.25 0.25
% W -0.25 -0.25 0 -0.25
% W 0.25 0.25 -0.25 0
% s0 -1 -1 1 1 E 0.0
% step 1 update_i 4 h -0.75 s -1 -1 1 -1 E -1.5
% QC-END: hopfield_memory_recovery_4n

\paragraph{Spurious attractors}

Beyond the intended memories \(\{\pm \mathbf{\xi}^\mu\}\), Hopfield networks can converge to \emph{spurious attractors}: stable states formed by mixtures of stored patterns. These unintended minima become increasingly common as the loading factor \(P/N\) grows (here \(P\) denotes the number of stored patterns); for random patterns the practical capacity is roughly \(0.138\, N\). The possibility of converging to a spurious state, or to the complemented memory rather than the original, explains why Hopfield networks are better viewed as associative memories than as discriminative classifiers.

\paragraph{Historical and practical significance}
I like Hopfield networks because they show a recurring engineering move: constrain a system until you can prove something useful, then use that proof as a debugging lens. Symmetry + no self-loops gives you an energy you can compute; asynchronous updates give you a monotone progress measure.

You will almost never deploy a classical Hopfield net as-is (capacity and spurious minima show up fast). But the idea ``memory as an objective landscape'' survives, and it is the right mental model to carry into later energy-based and content-addressable mechanisms.

\paragraph{Connections to other chapters.} Hopfield networks sit next to SOMs in spirit: both are unsupervised, but the diagnostic tools differ. SOMs give you map geometry (U-matrix/component planes); Hopfield gives you overlap and an energy trace to check whether recall is behaving. Later, attention revisits content-addressable lookup in a differentiable form.

\begin{tcolorbox}[summarybox, title={Key takeaways}]
\begin{itemize}
    \item Hopfield networks store binary patterns as attractors in an energy landscape defined by symmetric weights.
    \item Asynchronous updates monotonically reduce the Lyapunov energy, ensuring convergence to a fixed point.
    \item Capacity is limited and spurious attractors appear as the load \(P/N\) grows; treat Hopfield nets primarily as associative memories.
\end{itemize}

\medskip
\noindent\textbf{Minimum viable mastery.}
\begin{itemize}
    \item Write the energy \(E(s)\), state the symmetry condition \(W=W^\top\) with zero diagonal, and explain why asynchronous updates decrease \(E\).
    \item Distinguish true memories from spurious attractors, and connect failure modes to loading \(P/N\) and correlation among stored patterns.
    \item Use overlap and energy traces as diagnostics when demonstrating recall under corruption.
\end{itemize}

\noindent\textbf{Common pitfalls.}
\begin{itemize}
    \item Using asymmetric weights or nonzero self-connections (breaks the standard energy argument).
    \item Overloading the network and expecting clean recall (spurious minima dominate).
    \item Reporting a single cherry-picked recall trajectory instead of multiple corruption levels and multiple stored sets.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[summarybox, title={Exercises and lab ideas}]
\begin{itemize}
    \item Implement asynchronous recall for bipolar states with symmetric \(W\) and \(w_{ii}=0\). Plot energy vs.\ update step and verify it never increases.
    \item Store \(P\) random patterns with Hebbian weights. Use a fixed random seed and a fixed corruption protocol: pick one pattern \(\mathbf{\xi}^\mu\), flip \(k\) bits uniformly at random, then run asynchronous updates for a fixed budget (or until no flips occur). Sweep \(P/N\) and corruption level \(k\); plot both energy \(E(\mathbf{s}(t))\) and overlap \(m^{(\mu)}(\mathbf{s}(t))\) versus step, and report when spurious attractors become common.
    \item Construct (or search for) a small example where synchronous updates enter a 2-cycle; compare with asynchronous updates from the same initial state.
\end{itemize}

\medskip
\noindent\textbf{If you are skipping ahead.} The key transferable idea is the combination of a state update rule with a scalar quantity that tracks progress (energy, loss, or validation curves). That mindset carries into deep training in \Crefrange{chap:mlp}{chap:cnn}.
\end{tcolorbox}

\medskip
\paragraph{Where we head next.} \Cref{chap:cnn} turns from associative memory to deep feedforward perception, where convolution and pooling form hierarchical features. The optimization workflow is unchanged: the ERM toolkit from \Cref{chap:supervised} and the training mechanics from \Crefrange{chap:mlp}{chap:backprop} remain central. The analogy to keep in mind is the diagnostic habit: in Hopfield networks we watched an energy trace and overlap; in deep CNNs you will watch training/validation curves and slice errors. Recurrence and attention return in \Cref{chap:rnn} and \Cref{chap:transformers}.

\nocite{Hopfield1982, AmitGutfreundSompolinsky1985}
